# [How Knowledge Graph and Attention Help? A Quantitative Analysis into Bag-level Relation Extraction](https://aclanthology.org/2021.acl-long.359/)
- We summarize our contributions as follows: • To the best of our knowledge, our proposed framework is the first work to quantitatively analyze the working mechanism of Knowledge Graph and attention for bag-level RE.

- To quantitatively evaluate the effect of attention and KG on Bag-level RE, we first define two metrics to measure the noise pattern (Section 4.1).

- To evaluate the effects of attention and KG, we design two straightforward Bag-level RE models without the attention module, BRE and BRE+CE.




# [Which Linguist Invented the Lightbulb? Presupposition Verification for Question-Answering](https://aclanthology.org/2021.acl-long.304/)
- Presuppositions and unanswerability.

- We conducted a side-by-side study with 100 unanswerable questions.

- This is the oracle behavior of closed-book QA systems that allow Unanswerable as an answer.




# [R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling](https://aclanthology.org/2021.acl-long.379/)
- • We propose an efficient optimization algorithm to scale up our approach to a linear number of composition steps (Section 2.2).

- We make the following contributions: • Our novel CKY-based recursive Transformer on differentiable trees model is able to learn both representations and tree structure (Section 2.1).

- Our encoder parser operates in a bottom-up fashion akin to CKY parsing, yet runs in linear time with regard to the number of composition steps, thanks to a novel pruned tree induction algorithm.




# [Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation](https://aclanthology.org/2021.acl-long.91/)
- Some of our findings are: (1) models trained with more data rely on source information more and have more sharp token contributions; (2) the training process is non-monotonic with several distinct stages.

- Now we turn to analyzing the training process of an NMT model.

- When analyzing the training process, we find that changes in training are non-monotonic and form several distinct stages (e.g., stages changing direction from decreasing influence of source to increasing).




# [Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?](https://aclanthology.org/2021.acl-long.75/)
- In this paper, we ask: can we develop a semantic parsing approach that handles both natural language variation and compositional generalization?

- Figure 2: We evaluate semantic parsing approaches across a diverse set of evaluations focused on natural language variation, compositional generalization, or both.

- This paper proposed to expand the set of benchmarks used to evaluate compositional generalization in semantic parsing.




# [BERTAC: Enhancing Transformer-based Language Models with Adversarially Pretrained Convolutional Neural Networks](https://aclanthology.org/2021.acl-long.164/)
- Our work uses a GAN-style training scheme only for pretraining CNNs, not for fine-tuning TLMs.

- We also show that BERTAC outperformed the SOTA method of open-domain QA on Quasar-T and SearchQA.

- We tested all nine CNN models for BERTAC in our GLUE and open-domain QA experiments (Section 5).




# [Knowing the No-match: Entity Alignment with Dangling Cases](https://aclanthology.org/2021.acl-long.278/)
- Dangling entity detection.

- It has two jointly optimized modules, i.e., entity alignment and dangling entity detection.

- It consists of two jointly optimized modules for entity alignment and dangling entity detection, respectively.




# [Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval](https://aclanthology.org/2021.acl-long.118/)
- Our contributions are summarized as follows: • We provide in-depth analysis on informationseeking QA datasets, namely on Natural Questions and TyDi QA to identify the remaining headrooms.

- In this work, we investigate what makes information-seeking question answering (QA) more challenging, focusing on the Natural Questions (NQ; Kwiatkowski et al., 2019) and TyDi QA (Clark et al., 2020) datasets.

- • We show that answerability prediction and paragraph retrieval remain challenging even for state-of-the-art models through controlled experiments using four different models.




# [Value-Agnostic Conversational Semantic Parsing](https://aclanthology.org/2021.acl-long.284/)
- We use the same hyperparameters for all of our conversational semantic parsing experiments.

- We showed that abstracting away values while encoding the dialogue history and decoding programs significantly improves conversational semantic parsing accuracy.

- Figure 1: Illustration of the conversational semantic parsing problem that we focus on and the representations that we use.




# [Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation](https://aclanthology.org/2021.acl-long.480/)
- Multimodal Machine Translation (MMT) aims at designing better translation systems by extending conventional text-only translation systems to take into account multimodal information, especially from visual modality Wang et al., 2019).

- First, we revisit the need for visual context in the popular task of multimodal machine translation and find that: (1) under sufficient textual context, the MMT models' improvements over text-only counterparts result from the regularization effect (Section 5.2).

- In this paper we devise two interpretable models that exhibit state-of-the-art performance on the widely adopted MMT datasets -Multi30k and the new video-based dataset -VaTex.




# [Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection](https://aclanthology.org/2021.acl-long.132/)
- First, we present a human-andmodel-in-the-loop process for training online hate detection models.

- We presented a human-and-model-in-the-loop process for training an online hate detection system.

- To address these challenges, we present a human-and-model-in-the-loop process for collecting data and training hate detection models.




# [Joint Verification and Reranking for Open Fact Checking Over Tables](https://aclanthology.org/2021.acl-long.529/)
- In addition to our open-domain performance, our model achieves a new closed-domain stateof-the-art result.

- We introduce the first model for open-domain table fact verification, demonstrating strong performance exceeding the previous closedsetting state of the art.

- When using an oracle to retrieve a reference table, our approach also represents a new closed-domain state of the art.




# [Reliability Testing for Natural Language Processing Systems](https://aclanthology.org/2021.acl-long.321/)
- Hence, we argue for the need for reliability testing (especially worst-case testing) in NLP by contextualizing it among existing work on promoting accountability and improving generalization beyond the training distribution.

- We contribute a reliability testing framework -DOCTOR -that translates safety and fairness concerns around NLP systems into quantitative tests.

- Next, we showed how adversarial attacks can be reframed as worst-case tests.




# [De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention](https://aclanthology.org/2021.acl-long.371/)
- (3) Our method can effectively resolve both intra-dictionary and inter-dictionary biases.

- Generally, the main contributions of this paper are: • We proposed a causal framework, which not only fundamentally formulates the DS-NER process, but also explains the causes of both intra-dictionary bias and inter-dictionary bias.

- This section describes causal invariance regularizer to eliminate the inter-dictionary bias.




# [A Unified Generative Framework for Aspect-Based Sentiment Analysis](https://aclanthology.org/2021.acl-long.188/)
- We implement the BART to generate the target sequence in an end-to-end process based on the unified task formulation.

- However, their inference process is not an end-to-end process.

- However, the pipeline process is not end-to-end.




# [Multimodal Multi-Speaker Merger & Acquisition Financial Modeling: A New Task, Dataset, and Neural Baselines](https://aclanthology.org/2021.acl-long.526/)
- • To the best of our knowledge, no such M&A conference call dataset exists in academia, and our proposed methodology, M3ANet is the first deep learning approach for financial predictions on M&A conference calls.

- We also present a strong baseline model using multimodal multi-speaker inputs from the M&A calls to perform financial forecasting.

- We present a dataset of M&A calls that can be utilized to predict financial risk following M&A calls.




# [Explanations for CommonsenseQA: New Dataset and Models](https://aclanthology.org/2021.acl-long.238/)
- 2 https://github.com/dair-iitd/ ECQA-Dataset as eXplanation Generator (XG), comprises a novel two step fine-tuned property generation model (XGP) to generate common-sense properties and a free-flow explanation generation model (XGF).

- We also curate a free-flow explanation for each QA pair.

- Here we generate the free-flow explanations in a two-step manner.




# [Adversarial Learning for Discourse Rhetorical Structure Parsing](https://aclanthology.org/2021.acl-long.305/)
- In this work, we explore to adversarially train a discriminator to estimate the quality of the entire DRS tree for global optimization.

- For model learning, we have two goals: (i) learning of DRS parsing at each time step for local optimization and (ii) learning an adversarial bot to evaluate the pros and cons of the entire tree for global optimization.

- In this part, we compare with seven previous state-of-the-art (SOTA) parsers on text-level DRS parsing.




# [Improving Factual Consistency of Abstractive Summarization via Question Answering](https://aclanthology.org/2021.acl-long.536/)
- First, we propose an efficient automatic evaluation metric for factual consistency that is a simplification of the recently published QAGS protocol .

- We first proposed an efficient evaluation protocol called QUALS to measure factual consistency.

- In this paper we proposed to improve the factual consistency of abstractive summarization models.




# [TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance](https://aclanthology.org/2021.acl-long.254/)
- To stimulate progress of QA research over such hybrid data, we propose a new dataset, named TAT-QA (Tabular And Textual dataset for Question Answering).

- Hybrid QA Model We adopt HyBrider (Chen et al., 2020b) as our baseline over hybrid data, which tackles tabular and textual data from Wikipedia.

- This well reveals the effectiveness of our method that reasons over both tabular and textual data involving lots 3283 of numerical contents.




# [Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection](https://aclanthology.org/2021.acl-long.125/)
- To sum up, our contributions are: • We are the first to propose a topic-driven approach for dialogue emotion detection.

- A topic-augmented language model based on finetuning has been developed for topic extraction.

- Different from existing approaches, we propose a topic-driven and knowledge-aware model built on a Transformer Encoder-Decoder structure for dialogue emotion detection.




# [Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment](https://aclanthology.org/2021.acl-long.265/)
- In this paper, we introduce denoising word alignment as a new cross-lingual pre-training task.

- In this paper, we introduce a new cross-lingual pre-training task, named as denoising word alignment.

- Our contributions are summarized as follows: • We present a cross-lingual pre-training paradigm that alternately self-labels and predicts word alignments.




# [Explaining Relationships Between Scientific Documents](https://aclanthology.org/2021.acl-long.166/)
- Evaluating explanations of the relationships between scientific documents requires human judges with scientific expertise whose time and effort can be costly.

- In this paper we use citing sentences to operationalize the problem of generating natural language explanations of the relationships between two scientific papers.

- This appears to be a weakness in using citation sentences as proxies for relationship explanations.




# [Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation](https://aclanthology.org/2021.acl-long.409/)
- In this paper, we presented a model of variational semantic memory for few-shot WSD.

- Inspired by these advances, we introduce the first model of semantic memory for WSD in a meta-learning setting.

- We experimentally demonstrate the effectiveness of this approach for few-shot WSD, advancing the state of the art in this task.




# [Subsequence Based Deep Active Learning for Named Entity Recognition](https://aclanthology.org/2021.acl-long.332/)
- For example, in Named Entity Recognition (NER), each sentence is usually considered an instance.

- Improving the efficiency of AL for NER by allowing querying of subsequences over full sentences; 2.

- Finally, we summarise the AL algorithm proposed.




# [Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding](https://aclanthology.org/2021.acl-long.429/)
- We attempted to solve this problem by using the Bregman divergence (Bregman, 1967) to provide a unified interpretation of the SCE and NS loss functions.

- Next, we introduce the Bregman divergence.

- Our code will be available at https://github.com/kamigaito/ acl2021kge 2 Softmax Cross Entropy and Bregman Divergence




# [The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity](https://aclanthology.org/2021.acl-long.544/)
- Are you a robot?").

- Next we attempt to understand how existing systems handle the "are you a robot?" intent.

- Table 2: Categorizing existing systems responses to the same set of 100 unique phrasings of the "are you a robot?" intent.




# [A Neural Transition-based Model for Argumentation Mining](https://aclanthology.org/2021.acl-long.497/)
- We present a neural transition-based model for AM, which can jointly learn ACTC and ARI.

- In this paper, we propose a neural transition-based model for argumentation mining, which can incrementally construct an argumentation graph by predicting a sequence of actions.

- Towards these issues, we present a neural transition-based model for AM, which can classify the types of ACs and identify ARs simultaneously.




# [BASS: Boosting Abstractive Summarization with Unified Semantic Graph](https://aclanthology.org/2021.acl-long.472/)
- We further propose a graph-based encoderdecoder model based on the unified semantic graph.

- • We propose a graph-based encoder-decoder model to improve both the document representation and summary generation process of the Seq2Seq architecture by leveraging the graph structure.

- Our main contributions are summarized as follows: • We present the unified semantic graph which aggregates co-referent phrases distributed in context for better modeling the longdistance relations and global structure in longdocument summarization and MDS.




# [Discovering Dialog Structure Graph for Coherent Dialog Generation](https://aclanthology.org/2021.acl-long.136/)
- In this paper, we conduct unsupervised discovery of discrete dialog structure from chitchat corpora.

- Our contribution includes: (1) we identify the task of unsupervised dialog structure graph discovery in open-domain dialogs.

- In this paper, we propose a novel discrete variational auto-encoder with graph neural network (DVAE-GNN) to discover a two-layer dialog structure from chitchat corpus.




# [Long-Span Summarization via Local Attention and Content Selection](https://aclanthology.org/2021.acl-long.470/)
- Long-span Summarization.

- We study two methods for long-span summarization tasks.

- Furthermore, with a small-scale GPU card, our approach achieves comparable or superior performance to previous state-of-the-art systems.




# [Defense against Synonym Substitution-based Adversarial Attacks via Dirichlet Neighborhood Ensemble](https://aclanthology.org/2021.acl-long.426/)
- By augmenting these adversarial examples with the original training data, the model is robust to such perturbations.

- Such expansions will slightly hurt the performance on the clean data.

- We demonstrated through extensive experimentation that our adversarially trained smooth classifiers consistently outperform all existing empirical and certified defenses by a significant margin on three datasets across different network architectures, establishing state-of-the-art for defenses against adversarial text attacks.




# [A Joint Model for Dropped Pronoun Recovery and Conversational Discourse Parsing in Chinese Conversational Speech](https://aclanthology.org/2021.acl-long.138/)
- This paper presents that dropped pronoun recovery and conversational discourse parsing are two strongly related tasks.

- The task of dropped pronoun recovery (DPR) aims to locate the position of the dropped pronoun and identify its type.

- To overcome these shortcomings, we propose a novel neural model called DiscProReco to perform DPR and CDP jointly.




# [RAW-C: Relatedness of Ambiguous Words-in Context (A New Lexical Resource for English)](https://aclanthology.org/2021.acl-long.550/)
- As depicted in Figure 4, Cosine Distance tended to underestimate how related humans find same-sense uses to be, and overestimate how related humans find different-senses to be.

- Further, we see that Cosine Distance systematically overestimates how related participants find different-sense Homonyms to be.

- First, contextualized representations from both BERT and ELMo capture the distinction between same-sense and different-sense uses of a word, but their ability to distinguish between homonymy and polysemy is marginal at best.




# [Breaking Corpus Bottleneck for Context-Aware Neural Machine Translation with Cross-Task Pre-training](https://aclanthology.org/2021.acl-long.222/)
- Document-level context-aware neural machine translation (NMT) aims to translate sentences in a document under the guidance of document-level context.

- 3 Fine-tuning on Document-Level Parallel Dataset

- On the one hand, sentence-level parallel dataset is a natural resource to use.




# [EMAILSUM: Abstractive Email Thread Summarization](https://aclanthology.org/2021.acl-long.537/)
- In this work, we propose an abstractive email thread summarization dataset, EMAILSUM, that contains 2,549 email threads with human-written short and long summaries.

- Following the branch of dialogue or thread summarization, we introduce a new abstractive Email Thread Summarization (EMAILSUM) dataset.

- We collect both short (< 30 words) and long (< 100 words) abstractive summaries per thread.




# [Parameter-Efficient Transfer Learning with Diff Pruning](https://aclanthology.org/2021.acl-long.378/)
- We propose diff pruning as a simple approach for parameter-efficient transfer learning with pretrained models.

- Direct BERT compression methods also provide a straightforward approach to parameter-efficient transfer learning.

- Instead of modifying the architecture of the model, diff pruning extends the base model through a task-specific difference vector.




# [Aspect-Category-Opinion-Sentiment Quadruple Extraction with Implicit Aspects and Opinions](https://aclanthology.org/2021.acl-long.29/)
- Experiments demonstrate the advantages of the new task in aspect-based sentiment analysis with implicit aspects/opinions.

- We construct two new datasets for this task, with ACOS annotations including implicit aspects and implicit opinions.

- In this work, we introduce a new task named Aspect-Category-Opinion-Sentiment (ACOS) Quadruple Extraction, with the goal to extract all aspect-category-opinion-sentiment quadruples in a review sentence, and provide full support for aspect-level sentiment analysis with implicit aspects and opinions.




# [BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data](https://aclanthology.org/2021.acl-long.14/)
- • An unlikelihood training method with nondialogue inference data was introduced to enhance persona consistency understanding.

- • A BERT-based generative framework, BoB, was proposed for training persona-based dialogue models from limited data.

- Contributions in this work are three-fold: • We disentangled the task of persona-based dialogue generation into two sub-tasks: consistency understanding and dialogue generation.




# [Coreference Reasoning in Machine Reading Comprehension](https://aclanthology.org/2021.acl-long.448/)
- First, we propose a methodology for creating MRC datasets that better reflect the coreference reasoning challenge.

- Our main contributions are as follows: • We show that Quoref does not reflect the natural challenges of coreference reasoning and propose a methodology for creating MRC datasets that better reflect this challenge.

- • We propose an approach to use existing coreference resolution datasets for training MRC models.




# [Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models](https://aclanthology.org/2021.acl-long.144/)
- This study applied causal mediation analysis to discover and interpret the mechanisms behind syntactic agreement in pre-trained neural language models.

- In this paper, we apply causal mediation analysis in order to study the subject-verb agreement mechanisms in language models.

- To investigate whether the same neurons are implicated in subject-verb agreement across structures, we select the top 5% of neurons per layer by NIE and calculate the proportion of these high-NIE neurons that overlap between each pair of structures.




# [Investigating label suggestions for opinion mining in German Covid-19 social media](https://aclanthology.org/2021.acl-long.1/)
- Label suggestions.

- Interactive label suggestions (G3).

- Static label suggestions (G2).




# [POS-Constrained Parallel Decoding for Non-autoregressive Generation](https://aclanthology.org/2021.acl-long.467/)
- The main contributions of this work could be summarized as follows: • For the first time, we experimentally reveal that the implicit assumption of knowledge distillation does not always hold for the tasks (e.g., text summarization, story ending generation, as demonstrated in our experiments).

- In both text summarization (XSUM) and story ending generation (ROCStories) tasks, the two original NAG models CMLM and DisCo outperform the AG model.

- As demonstrated in our experiments (See § 4.5), there are a number of such tasks beyond the assumption like text summarization and story ending generation.




# [Determinantal Beam Search](https://aclanthology.org/2021.acl-long.512/)
- We derive determinantal beam search, a novel generalization of beam search that casts subset selection as the subdeterminant optimization problem.

- Stochastic Beam Search.

- In our experiments, we explore the use of determinantal beam search as a diverse decoding strategy for language generation.




# [Focus Attention: Promoting Faithfulness and Diversity in Summarization](https://aclanthology.org/2021.acl-long.474/)
- Generating Diverse and Faithful Summaries with Focus Sampling.

- In this paper we introduce a Focus Attention MEchanism (or FAME) to transformer-based seq2seq architectures.

- In this section we present our experimental setup to assess the ability of our FAME models to generate faithful summaries and to demonstrate that focus sampling is more effective in generating diverse and faithful summaries than other sampling-based decoding methods.




# [Discontinuous Named Entity Recognition as Maximal Clique Discovery](https://aclanthology.org/2021.acl-long.63/)
- In this paper, we reformulate discontinuous NER as the task of discovering maximal cliques in a segment graph, and propose a novel Mac architecture.

- In this paper, we reformulate discontinuous NER as the task of maximal clique discovery by constructing a segment graph and leveraging the classic B-K backtracking algorithm (Bron and Kerbosch, 1973) to find all the maximum cliques as the entities.

- With the grid tagging scheme, we propose an endto-end neural architecture named Mac.




# [Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding](https://aclanthology.org/2021.acl-long.462/)
- In this paper, we propose Shallow Aggressive Decoding (SAD) to accelerate online inference efficiency of the Transformer for instantaneous GEC.

- 3 Shallow Aggressive Decoding

- To better exploit the Transformer for instantaneous GEC in practice, we propose a novel approach -Shallow Aggressive Decoding (SAD) to improve the model's online inference efficiency.




# [PENS: A Dataset and Generic Framework for Personalized News Headline Generation](https://aclanthology.org/2021.acl-long.7/)
- In this paper, we formulated the problem of personalized news headline generation.

- The problem of personalized news headline generation is formulated as follows.

- In this section, we formulate the problem of personalized news headline generation and differentiate it from personalized news recommendations.




# [Edited Media Understanding Frames: Reasoning About the Intents and Implications of Visual Disinformation](https://aclanthology.org/2021.acl-long.158/)
- In addition, we introduce a new model, PELICAN, improving over competitive languageand-vision transformer baselines.

- For Edited Media Understanding Frames, not all image regions are created equal.

- In this section, we present a new model for Edited Media Understanding Frames, with a goal of kickstarting research on this challenging problem.




# [Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution](https://aclanthology.org/2021.acl-long.539/)
- This paper aims to more fully interpret the stepwise prediction decisions of neural abstractive summarization models.

- Finally, this work has focused chiefly on abstractive summarization models.

- However, for summarization, showing the model partial or ungrammatical inputs in the source may significantly alter the model's behavior.




# [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://aclanthology.org/2021.acl-long.353/)
- Fine-tuning for natural language generation.

- In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation (NLG) tasks, inspired by prompting.

- Inductive bias of prefix-tuning.
