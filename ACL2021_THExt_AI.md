# [How Knowledge Graph and Attention Help? A Quantitative Analysis into Bag-level Relation Extraction](https://aclanthology.org/2021.acl-long.359/)
- To quantitatively evaluate the effect of attention and KG on Bag-level RE, we first define two metrics to measure the noise pattern (Section 4.1).

- To evaluate the effects of attention and KG, we design two straightforward Bag-level RE models without the attention module, BRE and BRE+CE.

- In conclusion, we construct a set of datasets and propose a framework to quantitatively evaluate how attention module and KG work in the bag-level RE.

- We summarize our contributions as follows: • To the best of our knowledge, our proposed framework is the first work to quantitatively analyze the working mechanism of Knowledge Graph and attention for bag-level RE. 1 dumps.wikimedia.org/wikidatawiki/entities/20201109/ • We have conducted extensive experiments to inspire and support us with the above findings.




# [Which Linguist Invented the Lightbulb? Presupposition Verification for Question-Answering](https://aclanthology.org/2021.acl-long.304/)
- Presuppositions and unanswerability.

- • We propose a novel framework for handling presuppositions in QA, breaking down the problem into three parts (see steps above), and evaluate progress on each (S5).

- This is the oracle behavior of closed-book QA systems that allow Unanswerable as an answer.

- This supports our hypothesis that presupposition failure-based answers would be more satisfactory to the users, and suggests that building a QA system that approaches the oracle behavior of our proposed system is a worthwhile pursuit.




# [R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling](https://aclanthology.org/2021.acl-long.379/)
- • We propose an efficient optimization algorithm to scale up our approach to a linear number of composition steps (Section 2.2).

- Our encoder parser operates in a bottom-up fashion akin to CKY parsing, yet runs in linear time with regard to the number of composition steps,

- We make the following contributions: • Our novel CKY-based recursive Transformer on differentiable trees model is able to learn both representations and tree structure (Section 2.1).

- In this paper, we have proposed an efficient CKYbased recursive Transformer to directly model hierarchical structure in linguistic utterances.




# [Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation](https://aclanthology.org/2021.acl-long.91/)
- Some of our findings are: (1) models trained with more data rely on source information more and have more sharp token contributions; (2) the training process is non-monotonic with several distinct stages.

- We can see that, generally, models trained with more data rely on source more heavily.

- When analyzing the training process, we find that changes in training are non-monotonic and form several distinct stages (e.g., stages changing direction from decreasing influence of source to increasing).

- Now we turn to analyzing the training process of an NMT model.




# [Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?](https://aclanthology.org/2021.acl-long.75/)
- In this paper, we ask: can we develop a semantic parsing approach that handles both natural language variation and compositional generalization?

- This paper proposed to expand the set of benchmarks used to evaluate compositional generalization in semantic parsing.

- Notably, designing approaches that can handle both compositional generalization and the natural language variation of non-synthetic datasets is difficult.

- In this section, we survey recent work related to compositional generalization in semantic parsing.




# [BERTAC: Enhancing Transformer-based Language Models with Adversarially Pretrained Convolutional Neural Networks](https://aclanthology.org/2021.acl-long.164/)
- Our work uses a GAN-style training scheme only for pretraining CNNs, not for fine-tuning TLMs.

- We also show that BERTAC outperformed the SOTA method of open-domain QA on Quasar-T and SearchQA.

- We tested our model on GLUE and on opendomain QA.

- The generator was trained in a GAN-style manner using QA datasets.




# [Knowing the No-match: Entity Alignment with Dangling Cases](https://aclanthology.org/2021.acl-long.278/)
- Dangling entity detection.

- It has two jointly optimized modules, i.e., entity alignment and dangling entity detection.

- It consists of two jointly optimized modules for entity alignment and dangling entity detection, respectively.

- We construct a dataset to support the study of the proposed problem setting, and design a multi-learning framework for both entity alignment and dangling entity detection.




# [Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval](https://aclanthology.org/2021.acl-long.118/)
- In this work, we investigate what makes information-seeking question answering (QA) more challenging, focusing on the Natural Questions (NQ; Kwiatkowski et al., 2019) and TyDi QA (Clark et al., 2020) datasets.

- Our contributions are summarized as follows: • We provide in-depth analysis on informationseeking QA datasets, namely on Natural Questions and TyDi QA to identify the remaining headrooms.

- We conduct the same experiments on SQuAD 2.0, to highlight the unique challenges of the information-seeking queries.

- • We show that answerability prediction and paragraph retrieval remain challenging even for state-of-the-art models through controlled experiments using four different models.




# [Value-Agnostic Conversational Semantic Parsing](https://aclanthology.org/2021.acl-long.284/)
- We showed that abstracting away values while encoding the dialogue history and decoding programs significantly improves conversational semantic parsing accuracy.

- We use the same hyperparameters for all of our conversational semantic parsing experiments.

- We propose a new "value-agnostic" approach to contextual semantic parsing driven by type-based representations of the dialogue history and functionbased representations of the generated programs.

- Figure 1: Illustration of the conversational semantic parsing problem that we focus on and the representations that we use.




# [Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation](https://aclanthology.org/2021.acl-long.480/)
- First, we revisit the need for visual context in the popular task of multimodal machine translation and find that: (1) under sufficient textual context, the MMT models' improvements over text-only counterparts result from the regularization effect (Section 5.2).

- Multimodal Machine Translation (MMT) aims at designing better translation systems by extending conventional text-only translation systems to take into account multimodal information, especially from visual modality Wang et al., 2019).

- In this paper we devise two interpretable models that exhibit state-of-the-art performance on the widely adopted MMT datasets -Multi30k and the new video-based dataset -VaTex.

- Inspecting the results, we find that applying these regularization techniques achieves similar gains over the text-only baseline as incorporating multimodal information does.




# [Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection](https://aclanthology.org/2021.acl-long.132/)
- First, we present a human-andmodel-in-the-loop process for training online hate detection models.

- We presented a human-and-model-in-the-loop process for training an online hate detection system.

- To address these challenges, we present a human-and-model-in-the-loop process for collecting data and training hate detection models.

- 3 The platform supports human-andmodel-in-the-loop dataset creation for a variety of NLP tasks.




# [Joint Verification and Reranking for Open Fact Checking Over Tables](https://aclanthology.org/2021.acl-long.529/)
- 3. In addition to our open-domain performance, our model achieves a new closed-domain stateof-the-art result.

- With an accuracy of 75.1%, we obtain the best open-domain results with our model using the joint reranking-and-verification loss and five tables.

- In Section 4, we introduced our model as a joint system for fact verification and evidence reranking.

- Our contributions can be summarized as follows: 1. We introduce the first model for open-domain table fact verification, demonstrating strong performance exceeding the previous closedsetting state of the art.




# [Reliability Testing for Natural Language Processing Systems](https://aclanthology.org/2021.acl-long.321/)
- We contribute a reliability testing framework -DOCTOR -that translates safety and fairness concerns around NLP systems into quantitative tests.

- Hence, we argue for the need for reliability testing (especially worst-case testing) in NLP by contextualizing it among existing work on promoting accountability and improving generalization beyond the training distribution.

- We first give a brief introduction to adversarial attacks in NLP before showing how they can be used for reliability testing.

- We argue that reliability testing, by reframing the concept of adversarial attacks, has the potential to fill this gap.




# [De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention](https://aclanthology.org/2021.acl-long.371/)
- For inter-dictionary bias, we propose a causal invariance regularizer which will make DS-NER models more robust to the perturbation of dictionaries.

- (3) Our method can effectively resolve both intra-dictionary and inter-dictionary biases.

- For inter-dictionary bias, we design a causal invariance regularizer to capture the dictionary-invariant evidence for NER.

- Generally, the main contributions of this paper are: • We proposed a causal framework, which not only fundamentally formulates the DS-NER process, but also explains the causes of both intra-dictionary bias and inter-dictionary bias.




# [A Unified Generative Framework for Aspect-Based Sentiment Analysis](https://aclanthology.org/2021.acl-long.188/)
- In conclusion, all the experiment results confirm that our proposed method, which unifies the training and the inference to an end-to-end generative framework, provides a new SOTA solution for the whole ABSA task.

- However, the pipeline process is not end-to-end.

- However, their inference process is not an end-to-end process.

- We implement the BART to generate the target sequence in an end-to-end process based on the unified task formulation.




# [Multimodal Multi-Speaker Merger & Acquisition Financial Modeling: A New Task, Dataset, and Neural Baselines](https://aclanthology.org/2021.acl-long.526/)
- We present a dataset of M&A calls that can be utilized to predict financial risk following M&A calls.

- We also present a strong baseline model using multimodal multi-speaker inputs from the M&A calls to perform financial forecasting.

- To the best of our knowledge, no such M&A conference call dataset exists in academia, and our proposed methodology, M3ANet is the first deep learning approach for financial predictions on M&A conference calls.

- • We accompany the dataset with neural baseline architectures that use the multimodal multi-speaker input to predict stock volatility and price movement.




# [Explanations for CommonsenseQA: New Dataset and Models](https://aclanthology.org/2021.acl-long.238/)
- 2 https://github.com/dair-iitd/ ECQA-Dataset as eXplanation Generator (XG), comprises a novel two step fine-tuned property generation model (XGP) to generate common-sense properties and a free-flow explanation generation model (XGF).

- We also curate a free-flow explanation for each QA pair.

- Here we generate the free-flow explanations in a two-step manner.

- Query (q, a, c):   A.5 Anecdotal Examples: Free-Flow Explanation Generation Table 14 gives an example of free-flow explanation generation by the two variants of XGF system. (




# [Adversarial Learning for Discourse Rhetorical Structure Parsing](https://aclanthology.org/2021.acl-long.305/)
- For model learning, we have two goals: (i) learning of DRS parsing at each time step for local optimization and (ii) learning an adversarial bot to evaluate the pros and cons of the entire tree for global optimization.

- In this work, we explore to adversarially train a discriminator to estimate the quality of the entire DRS tree for global optimization.

- In this research, we explored a global optimization method based on recent top-down frameworks.

- In this study, we aim to learn from the entire DRS tree to optimize our model from a global perspective.




# [Improving Factual Consistency of Abstractive Summarization via Question Answering](https://aclanthology.org/2021.acl-long.536/)
- In this paper we proposed to improve the factual consistency of abstractive summarization models.

- First, we propose an efficient automatic evaluation metric for factual consistency that is a simplification of the recently published QAGS protocol .

- We first proposed an efficient evaluation protocol called QUALS to measure factual consistency.

- We demonstrate through experiments that our method improves the factual consistency of summarization models measured by both automatic metrics such as QAGS as well as human evaluation.




# [TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance](https://aclanthology.org/2021.acl-long.254/)
- To stimulate progress of QA research over such hybrid data, we propose a new dataset, named TAT-QA (Tabular And Textual dataset for Question Answering).

- We further propose a novel TAGOP model based on TAT-QA.

- Hybrid QA Model We adopt HyBrider (Chen et al., 2020b) as our baseline over hybrid data, which tackles tabular and textual data from Wikipedia.

- We test three types of QA models on TAT-QA, specially addressing tabular, textual, and hybrid data.




# [Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection](https://aclanthology.org/2021.acl-long.125/)
- To sum up, our contributions are: • We are the first to propose a topic-driven approach for dialogue emotion detection.

- A topic-augmented language model based on finetuning has been developed for topic extraction.

- Different from existing approaches, we propose a topic-driven and knowledge-aware model built on a Transformer Encoder-Decoder structure for dialogue emotion detection.

- We have proposed a Topic-Driven and Knowledge-Aware Transformer model that incorporates topic representation and the commonsense knowledge from ATOMIC for emotion detection in dialogues.




# [Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment](https://aclanthology.org/2021.acl-long.265/)
- In this paper, we introduce denoising word alignment as a new cross-lingual pre-training task.

- In this paper, we introduce a new cross-lingual pre-training task, named as denoising word alignment.

- Our contributions are summarized as follows: • We present a cross-lingual pre-training paradigm that alternately self-labels and predicts word alignments.

- • We introduce a pre-training task, denoising word alignment, which predicts word alignments from perturbed translation pairs.




# [Explaining Relationships Between Scientific Documents](https://aclanthology.org/2021.acl-long.166/)
- We answer these questions by performing both automatic and human evaluations.

- In this paper we use citing sentences to operationalize the problem of generating natural language explanations of the relationships between two scientific papers.

- Evaluating explanations of the relationships between scientific documents requires human judges with scientific expertise whose time and effort can be costly.

- We employ a large, publicly available dataset of scientific documents to train a domain-adapted leftto-right language model for use in text generation applications and beyond.




# [Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation](https://aclanthology.org/2021.acl-long.409/)
- Inspired by these advances, we introduce the first model of semantic memory for WSD in a meta-learning setting.

- In this paper, we presented a model of variational semantic memory for few-shot WSD.

- We experimentally demonstrate the effectiveness of this approach for few-shot WSD, advancing the state of the art in this task.

- Furthermore, we propose adaptive β-VSM which learns an adaptive memory update rule from data using a lightweight hypernetwork.




# [Subsequence Based Deep Active Learning for Named Entity Recognition](https://aclanthology.org/2021.acl-long.332/)
- For example, in Named Entity Recognition (NER), each sentence is usually considered an instance.

- Finally, we summarise the AL algorithm proposed.

- On OntoNotes 5.0, Shen et al. (2017) achieve stateof-the-art performance with 25% of the original dataset querying full sentences, while we require only 13% of the dataset querying subsequences.

- We evaluate the efficacy and efficiency of the tested AL strategies in three ways.




# [Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding](https://aclanthology.org/2021.acl-long.429/)
- Our code will be available at https://github.com/kamigaito/ acl2021kge 2 Softmax Cross Entropy and Bregman Divergence

- We attempted to solve this problem by using the Bregman divergence (Bregman, 1967) to provide a unified interpretation of the SCE and NS loss functions.

- Next, we introduce the Bregman divergence.

- (6) By using the Bregman divergence, we can induce the following propositions for NS (θ ).




# [The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity](https://aclanthology.org/2021.acl-long.544/)
- We find that for most utterances, systems fail to confirm their nonhuman identity.

- Next we attempt to understand how existing systems handle the "are you a robot?" intent.

- Table 2: Categorizing existing systems responses to the same set of 100 unique phrasings of the "are you a robot?" intent. Systems typically do not succeed in confirming their non-human identity.

- §5 RQ3. How do including components of a system response to "are you a robot" affect human perception of the system?




# [A Neural Transition-based Model for Argumentation Mining](https://aclanthology.org/2021.acl-long.497/)
- We present a neural transition-based model for AM, which can jointly learn ACTC and ARI.

- In this paper, we propose a neural transition-based model for argumentation mining, which can incrementally construct an argumentation graph by predicting a sequence of actions.

- Towards these issues, we present a neural transition-based model for AM, which can classify the types of ACs and identify ARs simultaneously.

- Thus, in this work, we fill this gap by proposing a neural transition-based model that can identify both tree and non-tree argumentation structures without introducing any prior structural assumptions.




# [BASS: Boosting Abstractive Summarization with Unified Semantic Graph](https://aclanthology.org/2021.acl-long.472/)
- We further propose a graph-based encoderdecoder model based on the unified semantic graph.

- • We propose a graph-based encoder-decoder model to improve both the document representation and summary generation process of the Seq2Seq architecture by leveraging the graph structure.

- We further present a graph-based encoder-decoder model to improve both the document representation and summary generation process by leveraging the graph structure.

- Our main contributions are summarized as follows: • We present the unified semantic graph which aggregates co-referent phrases distributed in context for better modeling the longdistance relations and global structure in longdocument summarization and MDS.




# [Discovering Dialog Structure Graph for Coherent Dialog Generation](https://aclanthology.org/2021.acl-long.136/)
- In this paper, we conduct unsupervised discovery of discrete dialog structure from chitchat corpora.

- In this paper, we propose a novel discrete variational auto-encoder with graph neural network (DVAE-GNN) to discover a two-layer dialog structure from chitchat corpus.

- Experimental results demonstrate that DVAE-GNN can discover meaningful dialog structure, and the use of dialog structure as background knowledge can significantly improve multi-turn dialog coherence.

- Our contribution includes: (1) we identify the task of unsupervised dialog structure graph discovery in open-domain dialogs.




# [Long-Span Summarization via Local Attention and Content Selection](https://aclanthology.org/2021.acl-long.470/)
- Long-span Summarization.

- We study two methods for long-span summarization tasks.

- Thus, we are interested in exploiting pretrained models for long-span summarization tasks.

- We propose that long-span dependencies can be handled by two complementary methods.




# [Defense against Synonym Substitution-based Adversarial Attacks via Dirichlet Neighborhood Ensemble](https://aclanthology.org/2021.acl-long.426/)
- By augmenting these adversarial examples with the original training data, the model is robust to such perturbations.

- Such expansions will slightly hurt the performance on the clean data.

- The values of hyperparameters in Dirichlet Neighborhood Ensemble (DNE) are listed in Table 7.

- We demonstrated through extensive experimentation that our adversarially trained smooth classifiers consistently outperform all existing empirical and certified defenses by a significant margin on three datasets across different network architectures, establishing state-of-the-art for defenses against adversarial text attacks.




# [A Joint Model for Dropped Pronoun Recovery and Conversational Discourse Parsing in Chinese Conversational Speech](https://aclanthology.org/2021.acl-long.138/)
- This paper presents that dropped pronoun recovery and conversational discourse parsing are two strongly related tasks.

- Therefore, we constructed the first Structure Parsing-enhanced Dropped Pronoun Recovery (SPDPR) dataset by annotating the discourse structure information on a popular dropped pronoun recovery dataset (i.e., Chinese SMS).

- Dropped pronoun recovery is a critical technique that can benefit many downstream applications

- As there is no public dataset annotated with both dropped pronouns and conversational discourse structures, we also construct Structure Parsing-enhanced Dropped Pronoun Recovery (SPDPR) corpus, which is the first corpus annotated with both types of information.




# [RAW-C: Relatedness of Ambiguous Words-in Context (A New Lexical Resource for English)](https://aclanthology.org/2021.acl-long.550/)
- Further, we see that Cosine Distance systematically overestimates how related participants find different-sense Homonyms to be.

- As depicted in Figure 4, Cosine Distance tended to underestimate how related humans find same-sense uses to be, and overestimate how related humans find different-senses to be.

- These facts present a challenge for computational models of lexical semantics.

- However, none focus specifically on graded relatedness judgments for ambiguous words, controlling both the inflection and part of speech of the target word in question.




# [Breaking Corpus Bottleneck for Context-Aware Neural Machine Translation with Cross-Task Pre-training](https://aclanthology.org/2021.acl-long.222/)
- Document-level context-aware neural machine translation (NMT) aims to translate sentences in a document under the guidance of document-level context.

- One nice property of our approach is that the fine-tuned models can be used to translate both sentences and documents.

- In order to leverage both large-scale sentence-level parallel dataset and source-side monolingual documents for context-aware NMT, in this paper, we have proposed a novel cross-task pre-training approach, which simultaneously learns to translate a sentence from source language to target language while denoising a document from deliberately noised to original.

- For example,  propose a two-stage training strategy for context-aware NMT by pre-training the model on a sentencelevel parallel dataset.




# [EMAILSUM: Abstractive Email Thread Summarization](https://aclanthology.org/2021.acl-long.537/)
- Following the branch of dialogue or thread summarization, we introduce a new abstractive Email Thread Summarization (EMAILSUM) dataset.

- In this work, we propose an abstractive email thread summarization dataset, EMAILSUM, that contains 2,549 email threads with human-written short and long summaries.

- Email Thread Summarization is not a new task.

- Table 1: An email thread and human-written short and long summaries from our EMAILSUM Dataset.




# [Parameter-Efficient Transfer Learning with Diff Pruning](https://aclanthology.org/2021.acl-long.378/)
- We propose diff pruning as a simple approach for parameter-efficient transfer learning with pretrained models.

- Direct BERT compression methods also provide a straightforward approach to parameter-efficient transfer learning.

- Instead of modifying the architecture of the model, diff pruning extends the base model through a task-specific difference vector.

- Diff pruning formulates task-specific finetuning as learning a diff vector δ τ that is added to the pretrained model parameters θ, which remain fixed.




# [Aspect-Category-Opinion-Sentiment Quadruple Extraction with Implicit Aspects and Opinions](https://aclanthology.org/2021.acl-long.29/)
- Experiments demonstrate the advantages of the new task in aspect-based sentiment analysis with implicit aspects/opinions.

- We construct two new datasets for this task, with ACOS annotations including implicit aspects and implicit opinions.

- It is challenging to effectively model the four subtasks together to construct quadruples containing implicit aspects and implicit opinions.

- In fact, product reviews contain a large amount of implicit aspects and opinions.




# [BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data](https://aclanthology.org/2021.acl-long.14/)
- • An unlikelihood training method with nondialogue inference data was introduced to enhance persona consistency understanding.

- • A BERT-based generative framework, BoB, was proposed for training persona-based dialogue models from limited data.

- Contributions in this work are three-fold: • We disentangled the task of persona-based dialogue generation into two sub-tasks: consistency understanding and dialogue generation.

- To address the challenges of consistency understanding brought by limited data, we leverage large-scale non-dialogue inference data in our model.




# [Coreference Reasoning in Machine Reading Comprehension](https://aclanthology.org/2021.acl-long.448/)
- First, we propose a methodology for creating MRC datasets that better reflect the coreference reasoning challenge.

- Our main contributions are as follows: • We show that Quoref does not reflect the natural challenges of coreference reasoning and propose a methodology for creating MRC datasets that better reflect this challenge.

- We propose an approach to use existing coreference resolution datasets for training MRC models.

- Second, we propose to directly use coreference resolution datasets for training MRC models to improve their coreference reasoning.




# [Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models](https://aclanthology.org/2021.acl-long.144/)
- In this paper, we apply causal mediation analysis in order to study the subject-verb agreement mechanisms in language models.

- This study applied causal mediation analysis to discover and interpret the mechanisms behind syntactic agreement in pre-trained neural language models.

- As an illustration of the targeted syntactic evaluation paradigm, consider the following example, which demonstrates subject-verb agreement across an agreement attractor.

- Thus, while excluding the complementizer can make subject-verb agreement slightly more difficult for LMs (Marvin and




# [Investigating label suggestions for opinion mining in German Covid-19 social media](https://aclanthology.org/2021.acl-long.1/)
- Label suggestions

- Interactive label suggestions (G3).

- We hence conclude that interactively updated label suggestions need to be considered carefully when applied to non-expert annotation scenarios.

- Our goal is to study the effects of interactively updated and static label suggestions in non-expert annotation scenarios.




# [POS-Constrained Parallel Decoding for Non-autoregressive Generation](https://aclanthology.org/2021.acl-long.467/)
- The main contributions of this work could be summarized as follows: • For the first time, we experimentally reveal that the implicit assumption of knowledge distillation does not always hold for the tasks (e.g., text summarization, story ending generation, as demonstrated in our experiments).

- In both text summarization (XSUM) and story ending generation (ROCStories) tasks, the two original NAG models CMLM and DisCo outperform the AG model.

- In more details, the proposed POS-constrained Parallel Decoding (POSPD) trains a POS predictor to obtain POS tags of target sequences.

- As demonstrated in our experiments (See § 4.5), there are a number of such tasks beyond the assumption like text summarization and story ending generation.




# [Determinantal Beam Search](https://aclanthology.org/2021.acl-long.512/)
- We derive determinantal beam search, a novel generalization of beam search that casts subset selection as the subdeterminant optimization problem.

- In our experiments, we explore the use of determinantal beam search as a diverse decoding strategy for language generation.

- Stochastic Beam Search.

- We observe competitive performance compared with diverse beam search.




# [Focus Attention: Promoting Faithfulness and Diversity in Summarization](https://aclanthology.org/2021.acl-long.474/)
- Generating Diverse and Faithful Summaries with Focus Sampling.

- Finally, our newly introduced focus sampling technique is a better alternative to top-k or nucleus sampling to generate diverse set of faithful summaries.

- Table 2 presents results assessing focus sampling (Focus sample,k ), top-k sampling (Div top,k ) and nucleus sampling (Div nucleus ), for their abilities to generate diverse and faithful summaries.

- In this paper we introduce a Focus Attention MEchanism (or FAME) to transformer-based seq2seq architectures.




# [Discontinuous Named Entity Recognition as Maximal Clique Discovery](https://aclanthology.org/2021.acl-long.63/)
- In this paper, we reformulate discontinuous NER as the task of discovering maximal cliques in a segment graph, and propose a novel Mac architecture.

- With the grid tagging scheme, we propose an endto-end neural architecture named Mac.

- In order to overcome the limitation of such prior works, we propose Mac, a Maximal clique discovery based discontinuous NER model.

- In this paper, we reformulate discontinuous NER as the task of maximal clique discovery by constructing a segment graph and leveraging the classic B-K backtracking algorithm (Bron and Kerbosch, 1973) to find all the maximum cliques as the entities.




# [Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding](https://aclanthology.org/2021.acl-long.462/)
- In this paper, we propose Shallow Aggressive Decoding (SAD) to accelerate online inference efficiency of the Transformer for instantaneous GEC.

- To better exploit the Transformer for instantaneous GEC in practice, we propose a novel approach -Shallow Aggressive Decoding (SAD) to improve the model's online inference efficiency.

- 3 Shallow Aggressive Decoding

- Our final approach not only advances the stateof-the-art in English GEC benchmarks with an almost 10× online inference speedup but also is easily adapted to other languages.




# [PENS: A Dataset and Generic Framework for Personalized News Headline Generation](https://aclanthology.org/2021.acl-long.7/)
- In this paper, we formulated the problem of personalized news headline generation.

- The problem of personalized news headline generation is formulated as follows.

- To the best of our knowledge, there are no exclusive methods for personalized news headline generation.

- In this section, we formulate the problem of personalized news headline generation and differentiate it from personalized news recommendations.




# [Edited Media Understanding Frames: Reasoning About the Intents and Implications of Visual Disinformation](https://aclanthology.org/2021.acl-long.158/)
- In addition, we introduce a new model, PELICAN, improving over competitive languageand-vision transformer baselines.

- In this section, we present a new model for Edited Media Understanding Frames, with a goal of kickstarting research on this challenging problem.

- For Edited Media Understanding Frames, not all image regions are created equal.

- Table 1: Questions for each of the frames in Edited Media Understanding Frames.




# [Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution](https://aclanthology.org/2021.acl-long.539/)
- This paper aims to more fully interpret the stepwise prediction decisions of neural abstractive summarization models.

- Finally, this work has focused chiefly on abstractive summarization models.

- However, for summarization, showing the model partial or ungrammatical inputs in the source may significantly alter the model's behavior.

- Our conclusion is that the pre-trained language model has likely memorized certain articles and their summaries.




# [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://aclanthology.org/2021.acl-long.353/)
- Fine-tuning for natural language generation.

- In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation (NLG) tasks, inspired by prompting.

- We propose prefix-tuning as an alternative to full fine-tuning for conditional generation tasks.

- In low-data settings, prefix-tuning outperforms finetuning on both tasks ( §6.3).




# [StructuralLM: Structural Pre-training for Form Understanding](https://aclanthology.org/2021.acl-long.493/)
- In this paper, we propose StructuralLM to jointly exploit cell and layout information from scanned documents.

- The motivation behind StructuralLM is to jointly exploit cell and layout information across scanned document images.

- It is built upon an extension of the Transformer encoder, and jointly exploit cell and layout information from scanned documents.

- Empirical results show that our StructuralLM outperforms strong baselines and achieves new state-of-the-art results in the downstream tasks.




# [KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation](https://aclanthology.org/2021.acl-long.44/)
- In this paper, we propose Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based model capable of reasoning about and generating commonsense descriptions from cross modality inputs of images and texts.

- We propose the pretraining task of Knowledge-Based Commonsense Generation, which improves the reasoning ability of KM-BART by leveraging a large language model pretrained on external commonsense knowledge graphs.

- In this section, we describe our methodology for Visual Commonsense Generation.

- Our contributions in this work are three-folded: 1. We extend the BART model to process multimodal data of images and texts, and enable multimodal reasoning by introducing taskrelevant tokens.




# [Crafting Adversarial Examples for Neural Machine Translation](https://aclanthology.org/2021.acl-long.153/)
- We introduce a new definition of NMT adversarial example basing on the round-trip translation.

- • We propose a novel black-box word level NMT attack method that could effectively attack the mainstream NMT models, and exhibit high transferability when attacking popular online translators.

- Based on our new definition and metrics, we propose a promising black-box attack method called Word Saliency speedup Local Search (WSLS) that could effectively attack the mainstream NMT architectures, e.g. RNN and Transformer.

- Neural Machine Translation.




# [Intent Classification and Slot Filling for Privacy Policies](https://aclanthology.org/2021.acl-long.340/)
- We present PolicyIE, an intent classification and slot filling benchmark on privacy policies with two alternative neural approaches as baselines.

- We perform experiments using sequence tagging and sequence-to-sequence (Seq2Seq) learning models to jointly model intent classification and slot filling.

- Taking this as a motivation, we investigate the scope of Seq2Seq learning for joint intent classification and slot filling for privacy policy sentences.

- Intent Classification and Slot Filling Voice assistants and chat-bots frame the task of natural language understanding via classifying intents and filling slots given user utterances.




# [Bad Seeds: Evaluating Lexical Methods for Bias Measurement](https://aclanthology.org/2021.acl-long.148/)
- (2) We explore which features of seeds can cause instability, including both social biases and linguistic dimensions in our analysis.

- 1 Analysis: We provide a systematic framework for understanding the different sources of instability in seed sets that can affect bias measurements.

- Prior work examining seeds has shown that the frequency and part of speech of seeds can affect the resulting bias measurements.

- We use a mixture of literature survey, qualitative analysis of seed terms, and analytic methods to explore the use of seed sets for bias measurement through two overarching research questions.




# [Math Word Problem Solving with Explicit Numerical Values](https://aclanthology.org/2021.acl-long.455/)
- • We propose a numerical properties prediction mechanism to utilize numerical properties.

- The results show that our model achieved better performance than existing state-of-theart methods.

- With the category and comparison information, the model can better identify the interactive relationship between the numerals, and thus generate better results.

- The main contributions of this paper can be summarized as follows: • We explicitly incorporate numerical value information into math word problem solving tasks.




# [Learning Dense Representations of Phrases at Scale](https://aclanthology.org/2021.acl-long.518/)
- First, we aim to learn strong phrase representations from the supervision of reading comprehension tasks.

- In this study, we show that we can learn dense representations of phrases at the Wikipedia scale, which are readily retrievable for open-domain QA and other knowledge-intensive NLP tasks.

- We also introduce query-side fine-tuning that adapts our model to different types of queries.

- Ablation of phrase representations.




# [Word Sense Disambiguation: Towards Interactive Context Exploitation from Both Word and Sense Perspectives](https://aclanthology.org/2021.acl-long.406/)
- Sense Aware Context Exploitation in Supervised WSD

- Experiments on English and crosslingual all-words WSD datasets verify the effectiveness of our approach, surpassing previous state-of-the-art by large margins.

- In this paper, we propose an interactive context exploitation method from both word and sense perspectives in a supervised similarity-based WSD architecture.

- It also shows that the proposed method has an overwhelming advantage of learning few-shot and zero-shot WSD ability.




# [Can Sequence-to-Sequence Models Crack Substitution Ciphers?](https://aclanthology.org/2021.acl-long.561/)
- The contributions of our work are: • We propose an end-to-end multilingual decipherment model that can solve 1:1 substi-tution ciphers without explicit plaintext language identification, which we demonstrate on ciphers of 14 different languages.

- In this work, we present an end-to-end decipherment model that is capable of solving simple substitution ciphers without the need for explicit language identification.

- Our method, by contrast, can solve substitution ciphers from different languages without explicit language identification.

- • We conduct extensive testing of the proposed method in different realistic decipherment conditions; different cipher lengths, no-space ciphers, and ciphers with noise, and demonstrate that our model is robust to these conditions.




# [Learning Faithful Representations of Causal Graphs](https://aclanthology.org/2021.acl-long.69/)
- Our key contributions are: • We define a faithfulness property for word embeddings over a causal graph, that captures geometric properties of the causal graph, beyond the direct link prediction by ensuring global proximity preservation.

- We show that the faithfulness of text embeddings to a causal graph is important for causal inferencealigned downstream tasks.

- Since human validated causal graphs can be used directly to answer questions of the type "What causes X?", we demonstrate the utility of learning faithful representations by using our distance-based features to solve the Yahoo! causal question-answering (QA) task.

- Embeddings that violate the faithfulness property, can lead to spurious correlations based on co-location in the embedding space.




# [SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues](https://aclanthology.org/2021.acl-long.54/)
- Overall, we make the following contributions: (i) We propose to model and infer social relations and individual's attributes jointly with SocAoG for the consistency of attributes and social relations among a group.

- MCMC is proposed to parse the relation graph incrementally, enabling the dynamic inference upon any incoming utterance.

- The paper proposes a SocAoG model with α-βγ processes for the consistent inference of social relations in dialogues.

- Experiments show that our model outperforms state-of-the-art methods; case studies and ablation studies are provided for analysis.




# [Metaphor Generation with Conceptual Mappings](https://aclanthology.org/2021.acl-long.524/)
- We propose a novel framework for metaphor generation informed by conceptual metaphor theory.

- In summary, we have shown two methods for incorporating knowledge of conceptual metaphor theory in metaphor generation.

- • A thorough evaluation using both automatic and human evaluations (Section 5).

- Our contributions are: • Two metaphor generation models grounded in CMT: 1) An unsupervised lexical model relying on frame embeddings learned from Framenet (CM-Lex, Section 3.1) and 2) a BART (Lewis et al., 2020) model encoding source/target domain information through fine-tuning (CM-BART, Section 3.2).




# [Refining Sample Embeddings with Relation Prototypes to Enhance Continual Relation Extraction](https://aclanthology.org/2021.acl-long.20/)
- The comparison experiments show that our model significantly outperforms current state-of-the-art CRE models.

- (3) Our extensive experiments upon two RE benchmark datasets justify our model's remarkable superiority over the state-of-the-art CRE models and less dependence on memory size.

- Our contributions in this paper are summarized as follows: (1) We propose a novel CRE model which achieves enhanced performance through refining sample embeddings with relation prototypes and is effective in avoiding catastrophic forgetting.

- We introduce the following state-of-the-art CRE baselines to be compared with our model in our experiments.




# [Select, Extract and Generate: Neural Keyphrase Generation with Layer-wise Coverage Attention](https://aclanthology.org/2021.acl-long.111/)
- The objective of the sentence-selector is to predict the salient sentences in a document, as described in Task 1.

- This paper presents SEG-Net, a keyphrase generation model that identifies the salient sentences in a target document to utilize maximal information for keyphrase prediction.

- Our proposed model, SEG-Net jointly learns to extract and generate present and absent keyphrases from the salient sentences in a target document.

- 2. A layer-wise coverage attention.




# [HERALD: An Annotation Efficient Method to Detect User Disengagement in Social Conversations](https://aclanthology.org/2021.acl-long.283/)
- Our experiments show that HERALD achieves 86% accuracy in user disengagement detection in two dialog corpora.

- We propose a two-stage pipeline HER-ALD to automatically label and denoise training data and, at the same time, build a user disengagement detector.

- Unlike the existing work, we leverage weak supervision to improve annotation efficiency for detecting user disengagement in social conversations.

- Complaints are an evident sign of user disengagement.




# [A Sequence-to-Sequence Approach to Dialogue State Tracking](https://aclanthology.org/2021.acl-long.135/)
- (3) Scalable, the model can deal with categorical and non-categorical slots and unseen schemas.

- We have proposed a new approach to dialogue state tracking.

- Our approach Seq2Seq-DU formalizes dialogue state tracking as a sequence to sequence problem using BERT and pointer generation.

- There has been a large amount of work on task-oriented dialogue, especially dialogue state tracking and natural language understanding (eg., Chen et al., 2017))




# [Lexical Semantic Change Discovery](https://aclanthology.org/2021.acl-long.543/)
- Increasing the threshold on the predicted words improves the F 0.5 for both the type-based and token-based approach.

- In this paper, we compare both families of approaches for change discovery.

- We define the task of lexical semantic change discovery as follows.

- We then tune the threshold to find the best-performing type-and token-based approach   (Schlechtweg et al., 2018).




# [LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding](https://aclanthology.org/2021.acl-long.201/)
- Meanwhile, a spatial-aware self-attention mechanism is integrated into the Transformer architecture.

- We also introduce a spatial-aware self-attention mechanism to the model architecture for better modeling the document layout.

- • In addition to the masked visual-language model, we add text-image alignment and textimage matching as the new pre-training strategies to enforce the alignment among different modalities.

- In this paper, we present a multi-modal pre-training approach for visually-rich document understanding tasks, aka LayoutLMv2.




# [Fast and Accurate Neural Machine Translation with Translation Memory](https://aclanthology.org/2021.acl-long.246/)
- In this paper, we present a fast and accurate approach for TM-based NMT which can be applied to general translation tasks besides TM-specialized tasks.

- This paper presents a simple TM-based NMT model that employs a single bilingual sentence as its TM and thus is fast in training and inference.

- Experiments on TM-specialized tasks demonstrate its superiority over strong baselines in terms of running time and BLEU.

- In a summary, based on the above extensive experimental results, our proposed models substantially surpass several baselines on TM-specialized tasks and general tasks, in terms of BLEU and running time.




# [Leveraging Type Descriptions for Zero-shot Named Entity Recognition and Classification](https://aclanthology.org/2021.acl-long.120/)
- This paper explored the task of zero-shot NERC with entity type descriptions to transfer knowledge from observed to unseen classes.

- Regarding the mentioned causes for the low zero-shot NERC

- This paper is the first to study zero-shot NERC, by leveraging entity type descriptions.

- We addressed the zero-shot NERC specific challenge that the not-an-entity class is not well defined by proposing a multiclass architecture that uses class-aware encoding to model the negative class.




# [Measuring and Increasing Context Usage in Context-Aware Machine Translation](https://aclanthology.org/2021.acl-long.505/)
- Context-aware Machine Translation

- We introduce a new, architecture-agnostic, metric to measure how context-aware machine translation models are using context and propose a simple regularization technique to increase context usage by these models.

- Taking inspiration from the above, we propose Conditional Cross-Mutual Information (CXMI), a new measure of the influence of context on a model's predictions.

- We release a software package to encourage the use of this metric in future context-aware machine translation research.




# [Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model](https://aclanthology.org/2021.acl-long.141/)
- • We propose a new way to generate weak labels for ultra-fine entity typing.

- First, we obtain weak ultra-fine entity typing labels from a BERT masked language model.

- In this work, we propose a new approach to automatically generate ultra-fine entity typing labels.

- Many different approaches have been proposed to improve fine-grained entity typing performance.




# [MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation Understanding](https://aclanthology.org/2021.acl-long.285/)
- (3) Our proposed MPC-BERT achieves new state-ofthe-art performance on all three downstream tasks at two benchmarks.

- Experimental results on three downstream tasks show that MPC-BERT outperforms previous methods by large margins and achieves new state-of-the-art performance on two benchmarks.

- In this paper, we present MPC-BERT, a pre-trained language model with five self-supervised tasks for MPC understanding.

- Our goal is to build a pre-trained language model for universal MPC understanding.




# [Dual Slot Selector via Local Reliability Verification for Dialogue State Tracking](https://aclanthology.org/2021.acl-long.12/)
- Nevertheless, existing approaches generally predict the dialogue state at every turn from scratch.

- Joint accuracy refers to the accuracy of the dialogue state in each turn.

- • Empirical results show that our model achieves state-of-the-art performance with significant improvements.

- The Dual Slot Selector is a two-stage judging process.




# [I like fish , especially dolphins : * Addressing Contradictions in Dialogue Modeling](https://aclanthology.org/2021.acl-long.134/)
- We introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and humanbot contradictory dialogues.

- We formalize dialogue contradiction detection as a supervised classification task.

- We further propose a structured utterance-based approach where utterances are paired before being fed into Transformer NLI models to tackle the dialogue contradiction detection task.

- We hope future work on dialogue contradiction detection could explore pretraining models on more dialogue-focused corpora.




# [Explainable Prediction of Text Complexity: The Missing Preliminaries for Text Simplification](https://aclanthology.org/2021.acl-long.88/)
- We demonstrate that by simply applying explainable complexity prediction as a preliminary step, the out-of-sample text simplification performance of the state-of-the-art, black-box models can be improved by a large margin.

- Using complexity prediction as a preliminary step reduces the error of the state-of-the-art text simplification models by a large margin.

- All these qualitative and quantitative results suggest that the state-of-the-art black-box models tend to oversimplify and distort the meanings of outof-sample input that is already simple.

- Evidently, the lack of transparency and explainability has limited the application of these end-to-end black-box models in reality, especially to out-of-sample data, context, and domains.




# [Contrastive Learning for Many-to-many Multilingual Neural Machine Translation](https://aclanthology.org/2021.acl-long.21/)
- Therefore, mRASP2 has a great potential to serve many-to-many translations, including both English-centric and non-English directions.

- In this paper, we propose a multilingual COntrastive Learning framework for Translation (mCOLT or mRASP2) to reduce the representation gap of different languages, as shown in Figure 1.

- In this work, we take a step towards a unified many-to-many multilingual NMT with only English-centric parallel corpora and additional monolingual corpora.

- Simple yet effective, mRASP2 achieves consistent translation performance improvements for both English-centric and non-English directions on a wide range of benchmarks.




# [Language Grounding Through Neuro-Symbolic Interaction in a 3D World](https://aclanthology.org/2021.acl-long.159/)
- We learn the dynamics model through interaction.

- After pretraining our physical dynamics model, we integrate it with a Transformer Language Model (LM).

- We factorize an embodied agent into an explicit model of world dynamics, and a model of language form.

- In this paper, we investigate an alternate strategy for learning physical commonsense through interaction, and then transferring that into language.




# [BinaryBERT: Pushing the Limit of BERT Quantization](https://aclanthology.org/2021.acl-long.334/)
- Therefore, BinaryBERT retains the good performance of the ternary model, and can be further refined on the new architecture.

- We thus propose a ternary weight splitting that splits a trained ternary BERT to initialize BinaryBERT, followed by fine-tuning for further refinement.

- Given the challenging loss landscape of binary BERT, we propose ternary weight splitting (TWS) that exploits the flatness of ternary loss landscape as the optimization proxy of the binary model.

- Motivated by the above empirical observations, we propose ternary weight splitting, which takes the ternary model as a proxy to bridge the gap between the binary and full-precision models.




# [From Discourse to Narrative: Knowledge Projection for Event Relation Extraction](https://aclanthology.org/2021.acl-long.60/)
- Specifically, we design Multi-tier Knowledge Projection Network (MKPNet), which can leverage multi-tier discourse knowledge effectively for event relation extraction.

- Specifically, we propose Multi-tier Knowledge Projection Network (MKPNet) which can effectively leverage multi-tier discourse knowledge for implicit event relation extraction.

- In this paper, we propose a knowledge projection paradigm for event relation extraction and Multitier Knowledge Projection Network (MKPNet) is designed to leverage multi-tier discourse knowledge.

- The main contributions of this paper are: • We propose a new knowledge projection paradigm, which can effectively leverage the commonalities between discourses and narratives for event relation extraction.




# [UnitedQA: A Hybrid Approach for Open Domain Question Answering](https://aclanthology.org/2021.acl-long.240/)
- The significant improvement brought by our proposed hybrid approach indicates the benefit of combining extractive and generative readers for open-domain QA.

- In this study, we propose a hybrid model for opendomain QA, called UnitedQA, which combines the strengths of extractive and generative readers.

- We hypothesize that extractive and generative readers adopt different answer inference strategies, thus a hybrid extractive/generative reader can be a better option for open-domain QA tasks.

- Then, we compare the prediction errors made by extractive and generative models, respectively.




# [READONCE Transformers: Reusable Representations of Text for Transformers](https://aclanthology.org/2021.acl-long.554/)
- This work introduced READONCE Transformers, a novel approach for using large scale transformerbased language models to both build and consume reusable document representations.

- , we also evaluate the ability of READONCE Transformers to handle long documents on the NarrativeQA dataset.

- Further, can we extend text-to-text transformer architectures to consume such representa-tions in conjunction with text?

- These simulations provide strong evidence of the ability of READONCE Transformers to handle long documents more effectively than standard transformer based models.




# [How is BERT surprised? Layerwise detection of linguistic anomalies](https://aclanthology.org/2021.acl-long.325/)
- We use Gaussian models to characterize outof-domain embeddings at intermediate layers of Transformer language models.

- It has been proposed that there are different types of linguistic anomalies.

- We apply our model to test sentences drawn from BLiMP and 7 psycholinguistics studies, exhibiting morphosyntactic, semantic, and commonsense anomalies.

- In this paper, we introduce a new tool to probe for surprisal at intermediate layers of BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019), formulating the problem as density estimation.




# [Semantic Representation for Dialogue Modeling](https://aclanthology.org/2021.acl-long.342/)
- We investigated the feasibility of using AMRs for dialogue modeling, describing an algorithm to construct dialogue-level AMRs automatically and exploiting two ways to incorporate AMRs into neural dialogue systems.

- We consider two main ways of making use of dialogue-level AMRs.

- Figure 2 illustrates our method for constructing a dialogue-level AMR graph from multiple utterancelevel AMRs.

- We focus on creating conversation-level AMRs to facilitate information exchange more effectively for dialogue modeling.




# [What is Your Article Based On? Inferring Fine-grained Provenance](https://aclanthology.org/2021.acl-long.458/)
- Therefore, we propose a rank-aware multi-head cross-attention to relieve this problem.

- Therefore, we propose to learn a query generator, which is different with previous works.

- Setup To conduct an isolated evaluation of the ILP based inference

- Therefore, we propose to develop a query generator to generate the possible metadata of the target source article as new search keywords, so that the search engine is more likely to recall source articles.




# [Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment](https://aclanthology.org/2021.acl-long.67/)
- We present a direct and effective framework for BLI with unsupervised bitext mining and word alignment, which sets a new state of the art on the task.

- In this paper, we show it is possible to produce much higher quality lexicons without these restrictions by introducing new methods that combine (1) unsupervised bitext mining and (2) unsupervised word alignment.

- We show that simply pipelining recent algorithms for unsupervised bitext mining (Tran et al., 2020) and unsupervised word alignment (Sabet et al., 2020) significantly improves bilingual lexicon induction (BLI) quality, and that further gains are possible by learning to filter the resulting lexical entries.

- We show that retrieval-based bitext mining and contextual word alignment achieves even better performance.




# [LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking](https://aclanthology.org/2021.acl-long.64/)
- We introduced LNN-EL, a neuro-symbolic approach for entity linking on short text.

- • We propose, to the best of our knowledge, the first neuro-symbolic method for entity linking (coined "LNN-EL") that provides a principled approach to learning EL rules.

- Entity Linking Models.

- Entity Linking.




# [From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via Synchronous Semantic Decoding](https://aclanthology.org/2021.acl-long.397/)
- We propose an unsupervised semantic parsing method -Synchronous Semantic Decoding, which leverages paraphrasing and grammar-constrained decoding to simultaneously resolve the semantic gap and the structure gap.

- The semantic gap and the structure gap are simultaneously resolved by jointly leveraging paraphrasing and grammar-constrained decoding.

- The main contributions of this paper are: • We propose an unsupervised semantic parsing method -Synchronous Semantic De-coding , which can simultaneously resolve the semantic gap and the structure gap by jointly leveraging paraphrasing and grammar-constrained semantic decoding.

- In this paper, we propose an unsupervised semantic parsing method -Synchronous Semantic Decoding (SSD), which can simultaneously resolve the structure gap and the semantic gap by jointly leveraging paraphrasing and grammarconstrained decoding.




# [Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders](https://aclanthology.org/2021.acl-long.204/)
- We propose a Stacked Acoustic-and-Textual Encoding (SATE) method to cascade the ASR encoder and the MT encoder.

- This demonstrates the superiority of stacked acoustic and textual encoding for the speech translation task.

- This inspires us to propose a Stacked Acoustic-and-Textual Encoding method, which is straightforward to incorporate the pre-trained models into ST.

- SATE initializes with the pre-trained ASR and MT encoders, which stacks acoustic and textual encoding.




# [A Unified Generative Framework for Various NER Subtasks](https://aclanthology.org/2021.acl-long.451/)
- Our contribution can be summarized as follows: • We propose a novel and simple generative solution to solve the flat NER, nested NER, and discontinuous NER subtasks in a unified framework, in which NER subtasks are formulated as an entity span sequence generation problem.

- To better utilize the pre-trained BART, we propose three kinds of entity representations to linearize entities into entity pointer index sequences.

- the larger probability that it can be recalled for the flat NER and discontinuous NER.

- Since flat, continuous and discontinuous entities can all be represented as entity pointer index sequences, this formulation can tackle all the three kinds of NER subtasks in a unified way.




# [Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter](https://aclanthology.org/2021.acl-long.454/)
- The main architecture of the proposed Lexicon Enhanced BERT is shown in Figure 2.

- • BERT. Directly fine-tuning a pre-trained Chinese BERT on Chinese sequence labeling tasks.

- Extensive experiments show that the proposed LEBERT achieves state-of-theart performance on ten datasets of three Chinese sequence labeling tasks.

- Inspired by the work about BERT Adapter (Houlsby et al., 2019;Bapna and Firat, 2019;Wang et al., 2020), we propose Lexicon Enhanced BERT (LEBERT) to integrate lexicon information between Transformer layers of BERT directly.




# [COSY: COunterfactual SYntax for Cross-Lingual Understanding](https://aclanthology.org/2021.acl-long.48/)
- Contributions: 1) we develop a syntax-aware network that incorporates transferable syntax in language models; 2) we propose a novel counterfactual training method that addresses the technical challenge of emphasizing syntax; and 3) extensive experiments on three benchmarks demonstrate the effectiveness of our method for cross-lingual tasks.

- Specifically, COSY implicitly forces the networks to learn to encode the input not only based on semantic features but also based on syntactic features through syntax-aware networks and a counterfactual training method.

- In this section, we evaluate our COSY method for cross-lingual understanding under both zero-shot and few-shot settings.

- Comparison with the State of the Art.




# [MECT: Multi-Metadata Embedding based Cross-Transformer for Chinese Named Entity Recognition](https://aclanthology.org/2021.acl-long.121/)
- By combining the radical information, we propose a Multi-metadata Embedding based Cross-Transformer (MECT).

- The main contributions of the proposed method include: • The use of multi-metadata feature embedding of Chinese characters in Chinese NER.

- • A novel two-stream model that combines the radicals, characters and words of Chinese characters to improve the performance of the proposed MECT method.

- The proposed method uses multi-metadata embedding that fuses the information of radicals, characters and words through a Cross-Transformer network.




# [CDRNN: Discovering Complex Dynamics in Human Language Processing](https://aclanthology.org/2021.acl-long.288/)
- Continuous-time deconvolutional regression (CDR) is a recently proposed technique to address delayed effects in measures of human cognition

- In so doing, CDRNN provides detailed estimates of human language processing dynamics that are difficult to obtain using other measures.

- This study proposed and evaluated CDRNN, a deep neural extension of continuous-time deconvolutional regression that relaxes implausible simplifying assumptions made by widely used regression techniques in psycholinguistics.

- Predictors may thus coordinate in a non-linear, non-additive, and time-varying manner.




# [AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models](https://aclanthology.org/2021.acl-long.400/)
- B Search Space of Architecture Hyper-parameters.

- We propose an effective and efficient method Au-toTinyBERT to search for the optimal architecture hyper-parameters of efficient PLMs.

- We demonstrate the effectiveness of one-shot learning by comparing the performance of one-shot model and stand-alone trained model on the given architectures.

- over, we introduce HAT (Wang et al., 2020a), as a baseline of one-shot learning.




# [Lexicon Learning for Few-Shot Neural Sequence Modeling](https://aclanthology.org/2021.acl-long.382/)
- We have described a lexical translation mechanism for representing token-level translation rules in neural sequence models.

- This paper describes a neural sequence model that obtains improved generalization via a learned lexicon of token translation rules.

- Specifically, we augment decoder output layers with a lexical translation mechanism which generalizes neural copy mechanisms (e.g. See et al., 2017) and enables models to generate token-level translations purely attentionally.

- While the lexical translation mechanism is quite general, we focus here on its ability to improve few-shot learning in sequence-to-sequence models.




# [Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization](https://aclanthology.org/2021.acl-long.117/)
- Besides, combining all three annotations, our summarizer can achieve new state-of-the-art performance on the SAMSum dataset.

- Extensive experimental results show that our method can obtain consistent and remarkable improvements over strong baselines on both datasets and achieves new stateof-the-art performance on the SAMSum dataset.

- We investigate to use DialoGPT as unsupervised annotators for dialogue summarization, including keywords extraction, redundancy detection and topic segmentation.

- To alleviate the above problem, we explore the pre-trained language model as an unsupervised annotator to automatically provide annotations for the dialogue.




# [LeeBERT: Learned Early Exit for BERT with Cross-Level Optimization](https://aclanthology.org/2021.acl-long.231/)
- • We propose a novel cross-level optimization (CLO) algorithm to learn the loss term weights better.

- Built upon previous literature , we propose a novel cross-level optimization (CLO) algorithm to solve the bilevel optimization better.

- In this work, we propose a novel training mechanism called Learned Early Exiting for BERT (Lee-BERT).

- A Derivation of our cross-level optimization algorithm.




# [Align Voting Behavior with Public Statements for Legislator Representation Learning](https://aclanthology.org/2021.acl-long.99/)
- In practice, we build a heterogeneous graph to bridge the voting behavior and public statements of legislators.

- (2) Hashtag usage prediction.

- Given representation of legislators and legislation, the roll call vote prediction comes out to be a classification task.

- Experiments demonstrate that our framework can learn effective legislative representation and yield improvements for the roll call vote prediction task.




# [Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker](https://aclanthology.org/2021.acl-long.274/)
- We summarize our contributions as follows: • We construct a heterogeneous graph interaction network for document-level EE.

- To tackle the aforementioned two challenges, in this paper, we propose a Heterogeneous Graphbased Interaction Model with a Tracker (GIT) for document-level EE.

- GIT uses a heterogeneous graph interaction network to model global interactions among sentences and entity mentions.

- To tackle the challenges, we introduce Heterogeneous Graph-based Interaction Model with a Tracker (GIT).




# [TicketTalk: Toward human-level performance with end-to-end, transaction-based dialog systems](https://aclanthology.org/2021.acl-long.55/)
- In this section, we show how our end-to-end approach to transaction-based dialog systems produces verbal responses and predicts API calls with near human-level quality and accuracy.

- We have described an end-to-end dialog system approach that shows promising potential for transaction-based dialog applications.

- In this work we promote an end-to-end approach to single-domain, transaction-based dialog systems and describe how we overcome both data and grounding challenges described above.

- Dataset size and pre-training are key factors in creating models for end-to-end dialog systems.




# [Learning to Ask Conversational Questions by Optimizing Levenshtein Distance](https://aclanthology.org/2021.acl-long.438/)
- We present an alternative solution, a Reinforcement Iterative Sequence Editing (RISE) framework for the optimization of MLD.

- In this paper, we have proposed a minimum Levenshtein distance (MLD) based Reinforcement Iterative Sequence Editing (RISE) framework for Conversational Question Simplification (CQS).

- To train RISE, we have devised an Iterative Reinforce Training (IRT) algorithm with a novel Dynamic Programming based Sampling (DPS) process.

- To this end, we devise an Iterative Reinforce Training (IRT) algorithm that allows RISE to do some exploration itself.




# [Improving Zero-Shot Translation by Disentangling Positional Information](https://aclanthology.org/2021.acl-long.101/)
- Our contributions are as follow: • We show that the positional correspondence to input tokens hinders zero-shot translation.

- With this simple modification, we achieve improvements up to 18.5 BLEU points on zero-shot translation.

- Our approach substantially improves zero-shot translation quality, as summarized in Table 3.

- Here we extend the challenge of zero-shot translation by integrating a new language.




# [Assessing Emoji Use in Modern Text Processing Tools](https://aclanthology.org/2021.acl-long.110/)
- In this study, we assessed how well prominent NLP tools cope with text containing emoji characters.

- We consider a set of popular NLP tools and empirically assess to what extent they support emojis across a set of standard tasks, encompassing tokenization, part-of-speech tagging, dependency parsing, and sentiment analysis.

- Owoputi et al. (2013) proposed an improved part-of-speech tagging model for online conversational text based on word clusters.

- In many real-world settings, applications and services are expected to operate on text containing emojis, and thus it is important to investigate these capabilities.




# [A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition](https://aclanthology.org/2021.acl-long.372/)
- In this work, we also aim to design a competitive model for both overlapped and discontinuous NER.

- The results show that our model is highly competitive to the state-of-the-art models for overlapped or discontinuous NER.

- Muis and Lu (2016) present a hypergraph model that is capable of handling both overlapped and discontinuous entities.

- With the Succession relation, we can recognize discontinuous entities.




# [Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning](https://aclanthology.org/2021.acl-long.142/)
- To improve the robustness of the models when no external contexts are available, we propose Cooperative Learning.

- With the external contexts, our models with CL outperform previous state-of-the-art approaches on most of the datasets.

- 2. We propose Cooperative Learning to jointly improve the accuracy of both input views in a unified model.

- Cooperative Learning targets at using the retrieval-based input view to help improve the accuracy of the model when there are no external contexts available.




# [Nested Named Entity Recognition via Explicitly Excluding the Influence of the Best Path](https://aclanthology.org/2021.acl-long.275/)
- In this paper, we proposed a simple and effective method for nested named entity recognition by explicitly excluding the influence of the best path through selecting and removing chunks at each level to build different potential functions.

- We empirically demonstrate that extracting the innermost entities first results in better performance.

- We conduct experiments on three nested named entity recognition datasets in English, i.e., ACE2004 (Doddington et al., 2004), ACE2005 (Walker et al., 2006) and GENIA (Kim et al., 2003).

- Besides, we found the innermost-first encoding scheme works better than the conventional outermost-first encoding scheme.




# [On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation](https://aclanthology.org/2021.acl-long.172/)
- Adapter-based tuning is more robust to overfitting.

- Adapter-based tuning alone without TAPT even outperforms fine-tuning with TAPT.

- Adapter-based tuning is more stable across a wider range of learning rates.

- • Adapter-based tuning demonstrates higher stability and better generalization ability.




# [A Semantic-based Method for Unsupervised Commonsense Question Answering](https://aclanthology.org/2021.acl-long.237/)
- Our contributions in this paper are summarized as follows: • We propose a semantic-based question answering model (SEQA) for robust commonsense question answering in an unsupervised setting.

- Hence we propose a new method for unsupervised commonsense question answering, which achieves better results and performs more robustly.

- We present a semantic-based question answering method, SEQA, which can answer commonsense questions more accurately and robustly in an unsupervised setting.

- Previous work has explored pre-trained language models (LMs) for unsupervised commonsense question answering.




# [Transferable Dialogue Systems and User Simulators](https://aclanthology.org/2021.acl-long.13/)
- In this section, we demonstrate the capability of transfer learning of the proposed framework under two low-resource setups: Domain Adaptation and Single-to-Multiple Domain Transfer.

- We propose a novel joint learning framework of training both the DS and the US for complex multidomain dialogues.

- Our contributions can be summarised as follows: • Novel contributions in joint optimisation of a fully text-to-text dialogue system with a matched user simulator on complex, multidomain human-human dialogues.

- More importantly, it is shown that the proposed framework is highly effective for transfer learning, which is a novel contribution relative to previous work.




# [Towards Argument Mining for Social Good: A Survey](https://aclanthology.org/2021.acl-long.107/)
- 2 We conclude the survey by defining the conceptual coordinates and the practical challenges of (semi-)automatic moderation, a highly integrative application of AM for Social Good which represents a natural testbed for the integrated definition of quality discussed above.

- We present an interdisciplinary formulation of the notion of argument quality, which is more apt to work with heterogeneous data and platforms, such as discussion forums and social media.

- Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

- Furthermore, there is a growing research interest in other aspects of AM, such as argument quality.




# [CogAlign: Learning to Align Textual Neural Representations to Cognitive Language Processing Signals](https://aclanthology.org/2021.acl-long.291/)
- • We propose a text-aware attention mechanism that extracts useful cognitive information via a compatibility matrix.

- Further experiments exhibit that CogAlign is able to transfer cognitive information from Zuco to other datasets without cognitive processing signals.

- In a nutshell, our contributions are listed as follows: • We present CogAlign that learns to align neural representations of natural language to cognitive processing signals at both word and sentence level.

- In addition, Co-gAlign with both cognitive processing signals obtains new state-of-the-art performance in all NLP tasks.




# [IrEne: Interpretable Energy Prediction for Transformers](https://aclanthology.org/2021.acl-long.167/)
- The result is that IrEne can predict not only the inference energy consumption of the entire model, but also of its components, making the energy prediction highly interpretable.

- This work focused on inference energy predictions of Transformers on a target hardware device.

- Finding energy bottlenecks: We use IrEne to analyze the energy bottlenecks in Transformer models.

- First, we frame the problem of interpretable energy prediction over a model tree abstraction.




# [Argument Pair Extraction via Attention-guided Multi-Layer Multi-Cross Encoding](https://aclanthology.org/2021.acl-long.496/)
- To overcome this limitation, we propose an attention-guided multi-layer multi-cross (MLMC) encoding mechanism.

- Figure 2 shows our proposed attention-guided multi-layer multi-cross (MLMC) encoding based model.

- In this paper, we adopt the table-filling approach for modeling the sentence-level correlation between two passages, and propose the attention-guided multi-layer multi-cross (MLMC) encoding scheme for the argument pair extraction (APE) task.

- In addition, we also design an auxiliary attention loss to guide each argument to refer to its paired arguments.




# [Personalized Transformer for Explainable Recommendation](https://aclanthology.org/2021.acl-long.383/)
- In summary, our key contributions are: • We propose PETER that makes recommendation and generates explanation simultaneously based on user and item IDs for explainable recommendation.

- Meanwhile, we demonstrate that conducting recommendation task on the same model is also feasible, so we name it PETER, which stands for PErsonalized Transformer for Explainable Recommendation.

- We propose a simple and effective solution to address the personalized generation problem of Transformer, unleashing its language modeling power to generate explanations for recommender systems.

- It can also make recommendations.




# [Are Pre-trained Convolutions Better than Pre-trained Transformers?](https://aclanthology.org/2021.acl-long.335/)
- What are the benefits of pre-trained convolutions over Transformers?

- Concretely, this paper seeks to empirically validate whether pre-trained convolutions are competitive with pre-trained Transformers across a range of tasks.

- Hence, the benefits achieved by pre-training are not exclusive to Transformer models.

- Are only Transformers able to capitalize on the benefits of pre-training?




# [DynaSent: A Dynamic Benchmark for Sentiment Analysis](https://aclanthology.org/2021.acl-long.186/)
- ing version 1 of the DynaSent dataset for Englishlanguage ternary (positive/negative/neutral) sentiment analysis.

- We presented DynaSent, as the first stage in an ongoing effort to create a dynamic benchmark for sentiment analysis.

- Thus, with only two rounds collected, DynaSent is already a substantial new resource for sentiment analysis.

- In Round 2, we leverage Dynabench to begin creating a new dynamic sentiment benchmark.




# [Dependency-driven Relation Extraction with Attentive Graph Convolutional Networks](https://aclanthology.org/2021.acl-long.344/)
- In this paper, we propose a dependency-driven neural approach for RE, where attentive graph neural network (A-GCN) is proposed to distinguish the important contextual information for this task.

- State-of-the-art performance is observed on both datasets.

- In this paper, we propose A-GCN to leverage dependency information for relation extraction, where an attention mechanism is applied to dependency connections to applying weighting on both connections and types so as to better distin-guish the important dependency information and leverage them accordingly.

- Compared with the standard GCN, A-GCN enhances it from two aspects: (1) using an attention mechanism to weigh different dependency connections and (2) introducing dependency types to the process to encode more detailed dependency information.




# [Check It Again: Progressive Visual Question Answering via Visual Entailment](https://aclanthology.org/2021.acl-long.317/)
- In this paper, we propose a select-and-rerank (SAR) progressive framework based on Visual Entailment.

- To summarize, our contributions are as follows: 1. We propose a select-and-rerank progressive framework to tackle the language priors problem, and empirically investigate a range of design choices for each module of this framework.

- Our method establishes a new state-of-the-art accuracy of 66.73% with an improvement of 7.55% on the previous best.

- From the results on VQA-CP v2 shown in Table 1, we can observe that: (1) Top20-SAR+LMH establishes a new state-of-the-art accuracy of 66.73% on VQA-CP v2, beating the previous bestperforming method CL by 7.55%.




# [Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization](https://aclanthology.org/2021.acl-long.510/)
- Moreover, our experiments show that the phase transition phenomenon is task and model dependent.

- Structured Lottery Tickets.

- Based on the observation, we further propose a tickets sharing strategy to improve multi-task fine-tuning.

- We summarize our contributions as follows: • Our result is the first to identify the phase transition phenomenon in pruning large neural language models.




# [Implicit Representations of Meaning in Neural Language Models](https://aclanthology.org/2021.acl-long.143/)
- encodings of entities and situations must begin with a formal framework for representing them.

- Even when trained only on language data, NLMs encode simple representations of meaning.

- Formal models of situations (built, like (a )-(b ), from logical representations of entities and their attributes) are central to linguistic theories of meaning.

- This paper investigates the extent to which neural language models build meaning representations of the world, and introduces a method to probe and modify the underlying information state.




# [Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features](https://aclanthology.org/2021.acl-long.58/)
- In this paper, we investigate how to design knowledge grounded dialogue systems that are less prone to including hallucinations or subjective information.

- Knowledge-Grounded Dialogue

- We demonstrate that this controllable dialogue system is able to produce responses that are perceived by humans to be more objective and faithful to document-based evidence.

- There are existing knowledge-grounded dialogue datasets (e.g. (Ghazvininejad et al., 2018;Dinan et al., 2019;Qin et al., 2019)) that could be appropriate training resources for such an informative dialogue agent.




# [Exploiting Language Relatedness for Low Web-Resource Language Model Adaptation: An Indic Languages Study](https://aclanthology.org/2021.acl-long.105/)
- RelateLM exploits relatedness along two dimensions -script relatedness through transliteration, and sentence structure relatedness through pseudo translation.

- In this paper, we make the following contributions: • We address the problem of adding a Low Web-Resource Language (LRL) to an existing pretrained LM, especially when monolingual corpora in the LRL is limited.

- Together, our experiments establish that using a related language as pivot, along with data augmentation through transliteration and bilingual dictionary-based pseudo translation, can be an effective way of adapting an LM for LRLs, and that this is more effective than direct training or pivoting through English.

- We propose RelateLM, which explores relatedness between the LRL and a Related Prominent Language (RPL) already present in the LM.




# [StereoRel: Relational Triple Extraction from a Stereoscopic Perspective](https://aclanthology.org/2021.acl-long.375/)
- Furthermore, we propose a novel model for relational triple extraction, which can simultaneously handle the above issues, named StereoRel.

- Correspondingly, the proposed method leverages three decoders to extract relational triples in a unified model.

- For relational triple extraction, from the stereoscopic perspective, there are the following two aspects worthy of discussion.

- This work has the following main contributions: • We provide a revealing insight into relational triple extraction from a stereoscopic perspective, where the occurrence of several challenging issues and shortcomings of existing methods are rationalized.




# [Stance Detection in COVID-19 Tweets](https://aclanthology.org/2021.acl-long.127/)
- In this work, we have constructed a COVID-19-Stance dataset that can be used to further the research on stance detection, especially in the context of COVID-19 pandemic.

- In summary, the contributions of this work are as follows: • We construct a COVID-19-Stance dataset that consists of 6,133 tweets covering user's stance towards four targets relevant to COVID-19 health mandates.

- The recency of the COVID-19 pandemic means there was no established stance detection dataset for this broader topic, when we began our research.

- We provide a comprehensive set of baseline results for the newly constructed COVID-19-Stance dataset, including results with established supervised baselines for stance detection tasks, and also baselines that employ approaches for handling small amounts of labeled data, including self-training and domain adaptation approaches.




# [MLBiNet: A Cross-Sentence Collective Event Detection Network](https://aclanthology.org/2021.acl-long.373/)
- Thus, we devise a bidirectional decoder to model event interdependency within a sentence.

- Firstly, a bidirectional decoder is proposed to explicitly model the sentence-level event inter-dependency, and event relevant information within a sentence is aggregated by an information aggregation module.

- In this paper, we propose a novel Multi-Layer Bidirectional Network (MLBiNet) for ED task.

- And we propose a multi-layer bidirectional network called MLBiNet shown in Figure 1 to deal with the challenges in detecting multiple events collectively.




# [DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations](https://aclanthology.org/2021.acl-long.72/)
- In this paper, we proposed a self-supervised objective for learning universal sentence embeddings.

- Together, these results demonstrate the effectiveness and feasibility of replacing hand-labelled data with carefully designed self-supervised objectives for learning universal sentence embeddings.

- Our primary contributions are: • We propose a self-supervised sentence-level objective that can be used alongside MLM to pretrain transformer-based language models, inducing generalized embeddings for sentence-and paragraph-length text without any labelled data (subsection 5.1).

- Therefore, closing the performance gap between unsupervised and supervised universal sentence embedding methods is an important goal.




# [Neural Bi-Lexicalized PCFG Induction](https://aclanthology.org/2021.acl-long.209/)
- To model bilexical dependencies and meanwhile reduce complexities, we draw inspiration from the canonical polyadic decomposition (CPD) (Kolda and Bader, 2009) and propose a latent-variable based neural parameterization of L-PCFGs.

- Bilexical dependencies, which have been shown useful in unsupervised dependency parsing (Han et al., 2017;Yang et al., 2020), are thus ignored.

- Zhu et al. (2020) propose neural L-PCFGs for unsupervised joint parsing.

- Our main goal is to find a parameterization that removes the implausible independence assumptions of Zhu et al. (2020) while decreases the complexities of the original L-PCFGs.




# [Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision](https://aclanthology.org/2021.acl-long.390/)
- MetaAdaptRank significantly improves the few-shot ranking accuracy of Neu-IR models across all benchmarks.

- This paper presents MetaAdaptRank, a domain adaption method for few-shot Neu-IR with contrastive weak data synthesis and meta-reweighted data selection.

- Specifically, CTSyncSup directly improves the few-shot ranking accuracy of BERT rankers by 3% on all benchmarks.

- We also empirically indicate that both contrastive weak data synthesis and meta-reweighted data selection contribute to MetaAdaptRank's effectiveness.




# [Improving Formality Style Transfer with Context-Aware Rule Injection](https://aclanthology.org/2021.acl-long.124/)
- In this work, we proposed the Context-Aware Rule Injection(CARI), an innovative method for formality style transfer (FST) by injecting multiple rules into an end-to-end BERT-based encoder and decoder model.

- In this work, we propose Context-Aware Rule Injection (CARI), an end-to-end BERT-based encoder and decoder model that is able to learn to select optimal rules based on context.

- Rule-based Formality Style Transfer In the past few years

- Our contributions are as follows: 1. We propose a new method, CARI, to integrate rules for pre-trained language models.




# [What Ingredients Make for an Effective Crowdsourcing Protocol for Difficult NLU Data Collection Tasks?](https://aclanthology.org/2021.acl-long.98/)
- We also find that the JUSTIFICATION intervention is ineffective as a stand-alone method for increasing NLU data quality.

- However, we find that training workers using an iterative feedback and requalification protocol is an effective strategy for collecting high-quality QA data.

- Our results suggest that asking workers to write justifications is not a helpful stand-alone strategy for improving NLU dataset difficulty, at least in the absence of explicit incentives for workers to write high-quality justifications.

- For the EXPERT and CROWD protocols, we train work-ers using an iterative process of collecting data, sending feedback, and qualifying high performing workers to subsequent rounds.




# [PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction](https://aclanthology.org/2021.acl-long.486/)
- This experiment proves that the Global Correspondence component is effective and greatly outperforms the heuristic nearest neighbor principle in the subject-object alignment task.

- In this paper, we presented a brand-new perspective and introduced a novel joint relational extraction framework based on Potential Relation and Global Correspondence, which greatly alleviates the problems of redundant relation judgement, poor generalization of span-based extraction and inefficient subject-object alignment.

- The main contributions of this paper are as follows: 1. We tackle the relational triple extraction task from a novel perspective which decomposes the task into three subtasks: Relation Judgement, Entity Extraction and Subject-object Alignment, and previous works are compared on the basis of the proposed paradigm as shown in Table 1.

- To better comprehend the task and advance the state of the art, we propose a novel perspective to decompose the task into three subtasks: i) Relation Judgement which aims to identify relations in a sentence, ii) Entity Extraction which aims to extract all subjects and objects in the sentence and iii) Subject-object Alignment which aims to align the subject-object pair into a triple.




# [MASK-ALIGN: Self-Supervised Neural Word Alignment](https://aclanthology.org/2021.acl-long.369/)
- We also introduce an attention variant called leaky attention to reduce the high attention weights on specific tokens such as periods.

- We propose to explicitly model the NULL token with an attention variant, namely leaky attention.

- Experiments show that MASK-ALIGN achieves new stateof-the-art results without using the guided alignment loss.

- We have presented a self-supervised neural alignment model MASK-ALIGN.




# [A Gradually Soft Multi-Task and Data-Augmented Approach to Medical Question Understanding](https://aclanthology.org/2021.acl-long.119/)
- We propose a novel multi-task learning approach for medical question understanding.

- We consider the multi-task learning of medical question summarization and medical RQE.

- In this paper, we introduce a novel, gradually soft multi-task and data-augmented approach to medical question understanding.

- Finally, we describe our gradually soft parameter-sharing scheme.




# [Benchmarking Scalable Methods for Streaming Cross Document Entity Coreference](https://aclanthology.org/2021.acl-long.364/)
- Feature-Based Encoders Existing models for streaming cross-document coreference exclusively make use of feature-based mention encoders.

- Our results show that the relative performance of different mention encoders and clustering algorithms varies across different domains.

- We also consider a hybrid encoder which combines feature-based and neural mention encoders.

- Following previous work, we adopt a two-step approach to performing streaming cross-document coreference.




# [Cross-modal Memory Networks for Radiology Report Generation](https://aclanthology.org/2021.acl-long.459/)
- In this paper, we propose an effective yet simple approach to radiology report generation enhanced by cross-modal memory networks (CMN), which is designed to facilitate the interactions across modalities (i.e., images and texts).

- In this paper, we propose to generate radiology reports with cross-modal memory networks, where a memory matrix is employed to record the alignment and interaction between images and texts, with memory querying and responding performed to obtain the shared information across modalities.

- Experimental results on two benchmark datasets demonstrate the effectiveness of our model, which achieves the state-of-the-art performance.

- ory vector) records particular cross-modal information connecting images and texts.




# [Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT Knowledge Distillation](https://aclanthology.org/2021.acl-long.228/)
- • We propose an efficient KD paradigm based on the empirical findings.

- Based on the second finding, we further propose an efficient paradigm to distill HSK.

- Based on this finding, we propose a new paradigm to improve the training efficiency in BERT KD, which does not require loading the teacher model during training.

- We study the three dimensions separately and compare a variety of strategies to extract the crucial knowledge.




# [Exploring Discourse Structures for Argument Impact Classification](https://aclanthology.org/2021.acl-long.306/)
- This indicates that a sequence of discourse re-lations is one of the essential factors for identifying the persuasive power of an argument.

- In this paper, we explicitly investigate how discourse structures influence the impact and the persuasiveness of an argument claim.

- Our contributions can be highlighted as follows: 1. To the best of our knowledge, we are the first to explicitly analyze the effect of discourse among contexts and an argument on the persuasiveness.

- Discourse relations, such as Restatement and Instantiation, among arguments reveal logical structures of a debate conversation.




# [Factoring Statutory Reasoning as Language Understanding Challenges](https://aclanthology.org/2021.acl-long.213/)
- Our contribution enables finer-grained scoring and debugging of models for statutory reasoning, which facilitates incremental progress and identification of performance bottlenecks.

- Here, taking inspiration from Prolog programs, we introduce a novel paradigm, by breaking statutory reasoning down into a sequence of tasks.

- Holzenberger et al. (2020) introduced SARA, a benchmark for the task of statutory reasoning, as well as two different approaches to solving this problem.

- Solutions to tackle statutory reasoning may range from high-structure, high-human involvement expert systems, to less structured, largely selfsupervised language models.




# [Diverse Pretrained Context Encodings Improve Document Translation](https://aclanthology.org/2021.acl-long.104/)
- Our architecture is designed to incorporate multiple sources of external embeddings into a pretrained sequence-to-sequence transformer model.

- Our key architectural innovation in this paper is an architecture for two-staged training that enables jointly conditioning on multiple context types, including both the source and target language context.

- Earlier work in document machine translation exploits the context by taking a concatenated string of adjacent source sentences as the input of neural sequence-to-sequence models ( Scherrer, 2017).

- This work is closely related to two lines of research: document-level neural machine translation and representation learning via language modeling.




# [Counterfactual Inference for Text Classification Debiasing](https://aclanthology.org/2021.acl-long.422/)
- We have designed a counterfactual framework for text classification debiasing.

- Inspired by this, we propose a novel modelagnostic paradigm (CORSAIR), which adopts factual learning before mitigating the negative influence of the dataset biases in inference (i.e., after training), without the need of employing data manipulations or designing balancing mechanisms.

- Inspired by the success of counterfactual inference in mitigating biases in computer vision (Niu et al., 2021;Wang et al., 2020;Tang et al., 2020;Yang et al., 2020;Goyal et al., 2017), we propose a counterfactual-inference-based text-classification debiasing framework (CORSAIR), which is able to make unbiased decisions with biased observations.

- The results demonstrate our proposed framework's effectiveness, generalizability and fairness, proving that CORSAIR, when employed on four different types of base models, is significantly helpful to mitigate the two types of dataset biases.




# [NeuralWOZ: Learning to Collect Task-Oriented Dialogue via Model-Based Simulation](https://aclanthology.org/2021.acl-long.287/)
- We propose NeuralWOZ, a novel dialogue collection framework, and we show our method achieves state-of-the-art performance on zero-shot domain transfer task.

- To support development of scalable dialogue systems, we propose NeuralWOZ, a model-based dialogue collection framework.

- Our method achieves new state-of-the-art of zeroshot domain transfer learning for dialogue state tracking on the MultiWOZ 2.1 dataset (Table 1).

- For the zero-shot domain transfer task, we exclude dialogues which contains target domain from  (Wolf et al., 2020).




# [Breaking Down Walls of Text: How Can NLP Benefit Consumer Privacy?](https://aclanthology.org/2021.acl-long.319/)
- Automated processing of privacy policies opens the door to a number of scenarios where language technologies can be developed to support users in the context of different tasks.

- Today privacy relies on the 'Notice and Choice' framework, which assumes that people actually read the text of privacy policies.

- Readability Analysis (Massey et al., 2013;Meiselwitz, 2013) Characterize the ease of understanding or comprehension of privacy policies.

- Thus, an opportunity exists for language technologies to bridge this gap by processing privacy policies to meet the needs of Internet and mobile users.




# [Bridge-Based Active Domain Adaptation for Aspect Term Extraction](https://aclanthology.org/2021.acl-long.27/)
- In this paper, we propose a novel active domain adaptation method.

- In this paper, we propose a novel active domain adaptation method for aspect term extraction.

- Type-III denotes the proposed active domain adaptation strategy.

- This proves the effectiveness of our proposed active domain adaptation strategy.




# [FORECASTQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data](https://aclanthology.org/2021.acl-long.357/)
- FORECASTQA is a question answering task whose goal is to test a machine's forecasting ability.

- In support of the proposed FORECASTQA formulation, we construct a dataset of 10,392 yes-no and multiple-choice questions.

- To utilize this form of data for forecasting, we proposed a question-answering task that requires forecasting skills to solve FORECASTQA, and provided the accompanying dataset.

- Specifically, we formulate the forecasting problem as a multiple-choice Question Answering (QA) task, where the input is a news corpus, questions, choices and timestamps associated with each question, and the output is one of the given choices per question.




# [Selecting Informative Contexts Improves Language Model Fine-tuning](https://aclanthology.org/2021.acl-long.87/)
- The instability of language model fine-tuning has previously been investigated by others.

- Several methods have recently been proposed to improve language model fine-tuning performance.

- Algorithm 1 summarizes IGF with a secondary learner for language model fine-tuning.

- This suggests that IGF could be used as a more energy efficient alternative to standard language model fine-tuning.




# [Towards User-Driven Neural Machine Translation](https://aclanthology.org/2021.acl-long.310/)
- • We propose a novel framework for user-driven NMT based on cache module and contrastive learning, which is able to model user traits in zero-shot scenarios.

- Different from them, user-driven NMT can generate personalized translations for these unseen users in a zero-shot manner.

- Furthermore, we contribute UDT-Corpus, which is the first Chinese-English parallel corpus annotated with user behavior.

- We propose user-driven NMT task, which aims to leverage user behavior to generate personalized translations.




# [VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words](https://aclanthology.org/2021.acl-long.389/)
- In this work, we propose VisualSparta, a simple yet effective text-to-image retrieval model that outperforms all existing query-agnostic retrieval models in both accuracy and speed.

- In conclusion, this paper presents VisualSparta, an accurate and efficient text-to-image retrieval model that shows the state-of-the-art scalable performance in both MSCOCO and Flickr30K.

- Since VisualSparta can be fit into an inverted-index architecture

- In this section, we present VisualSparta retriever, a fragment-level transformer-based model for efficient text-image matching.




# [Using Meta-Knowledge Mined from Identifiers to Improve Intent Recognition in Conversational Systems](https://aclanthology.org/2021.acl-long.545/)
- This paper focuses on the algorithms to use the meta-knowledge and on evaluating their impact on the accuracy of intent recognition.

- But, more important, the results seem to support our claim that meta-knowledge embedded in the output layer of our neuro-symbolic algorithms can improve intent recognition performance in practical systems.

- The results of the experiments indicate that the intent proto-taxonomies embedded by those developers can indeed be used by many workspaces to improve accuracy in intent recognition, notably in OOS detection.

- We proposed 3 new neuro-symbolic algo-  rithms and tested them using datasets built using data from intent identifiers of conversational systems.




# [Diversifying Dialog Generation via Adaptive Label Smoothing](https://aclanthology.org/2021.acl-long.272/)
- 2. We introduce a light-weight bi-directional decoder that can produce context-aware supervision signals for non-target words.

- We address the low-diversity issue of neural dialogue models by introducing an adaptive label smoothing approach, AdaLabel.

- To address the above issue, we propose an Adaptive Label smoothing (AdaLabel) method that can dynamically estimate a soft target distribution at each time step for different contexts.

- We demonstrate the hard target, label smoothing, and Adaptive Label Smoothing approach when learning to predict the next word ("human").




# [Accelerating BERT Inference for Sequence Labeling via Early-Exit](https://aclanthology.org/2021.acl-long.16/)
- Thus, we proposed a TOKen-level Early-Exit (TOKEE) that allows part of tokens that get confident predictions to exit earlier.

- Sentence-Level Early-Exit (SENTEE) is a simple extension for sequential labeling tasks based on existing early-exit approaches.

- Thus, to further accelerate the inference for sequence labeling tasks, we propose a token-level early-exit (TOKEE) method that allows simple tokens with confident predictions to exit early.

- First, we proposed the SENTence-level Early-Exit (SEN-TEE), which is a simple extension of existing earlyexit methods.




# [Novel Slot Detection: A Benchmark for Discovering Unknown Slot Types in the Task-Oriented Dialogue System](https://aclanthology.org/2021.acl-long.270/)
- Our contributions are three-fold: (1) We introduce a Novel Slot Detection (NSD) task in the task-oriented dialogue system.

- In this paper, we first introduce a new and important task, Novel Slot Detection (NSD), in the task-oriented dialogue system (Section 2.2).

- (2) We construct two public NSD datasets and establish a benchmark for future work.

- In this paper, we defined a new task, Novel Slot Detection(NSD), then provide two public datasets and establish a benchmark for it.




# [Privacy at Scale: Introducing the PrivaSeer Corpus of Web Privacy Policies](https://aclanthology.org/2021.acl-long.532/)
- Subsequently, we pretrain PrivBERT, a transformerbased language model, using the corpus and evaluate it on data practice classification and question answering tasks.

- We evaluated PrivBERT on the data practice classification and the question answering tasks and achieved state of the art results.

- We then analyse the lengths and top level distribution of the privacy policies in the corpus and use topic modelling to explore the component topics.

- Figure 2 shows the percentage of privacy policies in the corpus that contain each topic.




# [GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation](https://aclanthology.org/2021.acl-long.370/)
- We propose a General Word-Level Autocomple-tioN (GWLAN) task for computer-aided translation (CAT).

- Second, almost no public benchmarks are available for the autocompletion task of CAT.

- Our contributions are two-fold: • We propose the task of general word-level autocompletion for CAT, and construct the first public benchmark to facilitate research in this topic.

- This motivates us to propose a general word-level autocompletion task for CAT.




# [LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations](https://aclanthology.org/2021.acl-long.198/)
- To address the above limitations, we propose a Line Graph Enhanced Text-to-SQL model (LGESQL), which explicitly considers the topological structure of edges.

- Our proposed line graph enhanced text-to-SQL (LGESQL) model achieves state-of-the-art results in all configurations at the time of writing.

- Our main contributions are summarized as follows: • We propose to model the 1-hop edge features with a line graph in text-to-SQL.

- Additionally, we propose an auxiliary task called graph pruning.




# [Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation](https://aclanthology.org/2021.acl-long.266/)
- Contributions Our main contributions are: • We show the effectiveness of rejuvenating lowfrequency information by pretraining NAT models from raw data.

- Inspired by this finding, we propose reverse KD to recall more alignments for low-frequency target words ( §2.3).

- By observing these outputs, we found a large amount of translation errors on low-frequency words, most of which are domain-specific terminologies.

- Experimental results show that the proposed method consistently improve translation performance over the standard NAT models across languages and advanced NAT architectures.




# [Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text](https://aclanthology.org/2021.acl-long.498/)
- A third contribution is a novel evaluation method for text simplification.

- Our main contribution is the Keep it Simple (KiS) procedure, a novel unsupervised method for text simplification.

- We optimize a three-component reward: fluency, salience and simplicity.

- We present results experimentally validating the KiS procedure for text simplification.




# [A Bidirectional Transformer Based Alignment Model for Unsupervised Word Alignment](https://aclanthology.org/2021.acl-long.24/)
- We propose a bidirectional Transformer based alignment (BTBA) model for unsupervised learning of the word alignment task.

- We present a bidirectional Transformer based alignment (BTBA) model for unsupervised learning of the word alignment task.

- This paper presents a novel BTBA model for unsupervised learning of the word alignment task.

- We further fine-tune the target-to-source attention in the BTBA model to obtain better alignments using a full context based optimization method and selfsupervised training.




# [Multi-stage Pre-training over Simplified Multimodal Pre-training Models](https://aclanthology.org/2021.acl-long.199/)
- Specifically, we propose a new Multi-stage Pretraining (MSP) method.

- We perform the above three-stage pre-training on a simplified model of LXMERT (LXMERT-S).

- The pre-training process is divided into three stages based on different granularities of text-image correspondence from token, phrase to sentence.

- In this paper, we design a staged pre-training from word-level to phrase-level to sentence-level, which is related to the size of information units.




# [Learning Syntactic Dense Embedding with Correlation Graph for Automatic Readability Assessment](https://aclanthology.org/2021.acl-long.235/)
- (2) We verify that the correlation relationships among linguistic features could be utilized to learn syntactic dense embeddings.

- We prove that complementing semantic dense embeddings with syntactic dense embeddings learned with correlation graph of linguistic features can produce better-informed representations for readability assessment.

- Table 1 shows three pairs of linguistic features for Chinese readability assessment.

- (3) We propose a Dual-channel neural network model (i.e., Dual-Model) to combine the syntactic dense embeddings and the BERT semantic dense embeddings for readability predictions.




# [DYPLOC: Dynamic Planning of Content Using Mixed Language Models for Text Generation](https://aclanthology.org/2021.acl-long.501/)
- them for long-form opinion text generation poses distinct challenges.

- Opinion Text Generation.

- • We construct two opinion text generation datasets with content plans that capture prominent entities and concepts.

- Our opinion text generation framework takes as input a set of content items.




# [GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling](https://aclanthology.org/2021.acl-long.15/)
- A global intent-slot graph interaction layer is further introduced to perform sentence-level intent-slot interaction.

- In our work, we apply a global-locally graph interaction network to model the slot dependency and interaction between the multiple intents and slots.

- In this paper, we investigated a non-autoregressive model for joint multiple intent detection and slot filling.

- Then, we illustrate the local slot-aware and global intent-slot graph interaction network, respectively.




# [Poisoning Knowledge Graph Embeddings via Relation Inference Patterns](https://aclanthology.org/2021.acl-long.147/)
- We propose data poisoning attacks against KGE models based on inference patterns like symmetry, inversion and composition.

- We study the problem of generating data poisoning attacks on KGE models.

- We study the adversarial vulnerabilities of KGE models through data poisoning attacks.

- Poisoning Attacks on KGE models: We study poisoning attacks for the task of link prediction using KGE models.




# [Integrating Semantics and Neighborhood Information with Graph-Driven Generative Models for Document Retrieval](https://aclanthology.org/2021.acl-long.174/)
- Specifically, we applied a graph-induced Gaussian prior to model the two types of information in a unified framework.

- In this section, we present a more effective framework to unify the semantic and neighborhood information for the task of document hashing.

- To fully exploit the two types of information, in this paper, we propose a hashing method that unifies the semantics and neighborhood information with the graph-driven generative models.

- Extensive experimental results on three public datasets demonstrate that the proposed method can outperform state-of-the-art methods, indicating the effectiveness of the proposed framework in unifying the semantic and neighborhood information for document hashing.




# [Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion](https://aclanthology.org/2021.acl-long.534/)
- Our contributions can be summarized as follows: • We summarize three principles of KGC: inferential ability, assumptions and patterns, and construct a rule-guided dataset.

- We highlighted three principles for KGC datasets: inferential ability, assumptions, and patterns, and contribute a large-scale dataset InferWiki.

- Knowledge Graph Completion (KGC) aims to predict missing links in KG by inferring new knowledge from existing ones.

- Different from existing datasets, InferWiki aims to include positive, negative, and unknown testing triples, to evaluate the model under two types of assumptions: open-world assumption and closedworld assumption.




# [Self-Supervised Multimodal Opinion Summarization](https://aclanthology.org/2021.acl-long.33/)
- We proposed the first self-supervised multimodal opinion summarization framework.

- This study proposes a self-supervised multimodal opinion summarization framework called MultimodalSum by extending the existing selfsupervised opinion summarization framework, as shown in Figure 1.

- To resolve the heterogeneity of multimodal data, we also proposed a multimodal training pipeline.

- The goal of the self-supervised multimodal opinion summarization is to generate a pseudo sum-mary from multimodal data.




# [E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning](https://aclanthology.org/2021.acl-long.42/)
- In this paper, we propose a new end-to-end paradigm for pixel-level vision-language pretraining, to jointly learn visual representation, and semantic alignments between image and text.

- We further incorporate the tasks of object detection and image captioning into pre-training with a unified Transformer encoder-decoder architecture for enhancing visual learning.

- We make the following major contributions in this paper: • We propose the first end-to-end vision-language pre-trained model for both V+L understanding and generation, namely E2E-VLP, which can achieve comparable or superior performance with faster online inference speedup.

- To address the limitations, we propose a new endto-end paradigm for pixel-level vision-language pre-training, namely E2E-VLP, by enhancing with fine-grained visual learning.




# [Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks](https://aclanthology.org/2021.acl-long.456/)
- Different from previous works, we design a quasi dual learning method between symbolic grounded equation generation and problem's part-of-speech generation to enhance the understanding ability by easing the difficulty of generating problems from symbolic equations.

- Finally, we also propose a novel duality exploiting task that exploits the quasi duality between symbolic grounded equation generation and the problem's part-of-speech generation to enhance the understanding ability of our solver.

- Third, we propose program consistency checker to compute the semantic loss between the predicted program and ground-truth equation to ensure reasonable equation mapping.

- Therefore, we propose a program consistency checker to check the symbolic program consistency and regularize the model by computing semantic loss between the predicted symbolic program and ground-truth equation to ensure the reasonable symbolic equation mapping.




# [Improving Paraphrase Detection with the Adversarial Paraphrasing Task](https://aclanthology.org/2021.acl-long.552/)
- In the remainder of this paper, we apply the adversarial paradigm to the problem of paraphrase detection, and demonstrate the following novel contributions: • We use the adversarial paradigm to create a new benchmark examining whether paraphrase detection models are assessing the meaning equivalence of sentences rather than being over-reliant on word-level measures.

- We call this the Adversarial Paraphrasing Task (APT).

- Do human-generated adversarial paraphrases improve paraphrase detection?

- Do machine-generated adversarial paraphrases improve paraphrase detection?




# [Syntax-Enhanced Pre-trained Model](https://aclanthology.org/2021.acl-long.420/)
- (2) We propose a syntax-aware attention layer and a pre-training task for infusing syntactic information into the pre-trained model.

- To inject syntactic information, we introduce a syntax-aware attention layer and a newly designed pre-training task are proposed.

- In this paper, we seek to enhance pre-trained models with syntax of text.

- To address this, we conduct a large-scale study on injecting automatically produced syntax of text in both the pre-training and fine-tuning stages.




# [Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?](https://aclanthology.org/2021.acl-long.224/)
- , the performance gap between the two paradigms has gradually decreased.

- The ASR model was trained with the goal of achieving state-of-the-art performance.

- Although BLEU scores are not strictly comparable across languages, we can safely consider all our models as state-of-the-art.

- Systems' behavior is analysed from different perspectives, by exploiting high-quality post-edits and annotations by professionals.




# [Control Image Captioning Spatially and Temporally](https://aclanthology.org/2021.acl-long.157/)
- We propose a novel caption generation model with contrastive constraints and attention guidance called LoopCAG to control the captioning process spatially and temporally.

- 3) We intensively study the controllability and explainability of trace-controlled image captioning.

- Our contribution can be summarized as: 1) We propose a novel model LoopCAG, which learns the caption tokens' spatial grounding through attention guidance and temporal localization between trace input and the caption sentences through contrastive constraints in an end-to-end loop manner among the three modalities(vision, language, and traces).

- Composing the above together, We propose a novel trace-controlled image captioning model called LoopCAG and demonstrate its superior capability on captioning quality and flexible controllability.




# [PLOTCODER: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context](https://aclanthology.org/2021.acl-long.169/)
- In this section, we present PLOTCODER, a hierarchical model architecture for synthesizing visualization code from natural language and code context.

- Code context encoder.

- We evaluate PLOTCODER's ability to synthesize visualization programs using Jupyter notebooks of homework assignments or exam solutions.

- We describe PLOTCODER, a model architecture that includes an encoder that links the natural language description and code context, and a hierarchical program decoder that synthesizes plotted data from the code context and dataframe items.




# [Learning to Explain: Generating Stable Explanations Fast](https://aclanthology.org/2021.acl-long.415/)
- Moreover, our L2E approach produces explanations between 5 and 7.5 × 10 4 times faster than the six baselines, making it suitable for long documents and very large black-box models.

- In this paper, we present a learning to explain (L2E) approach that efficiently learns the commonalities of the explanation process across different examples.

- We have presented a Learning to Explain (L2E) approach to learn the commonalities of the explanation generation processes across different examples.

- We start by investigating the faithfulness of an explanation model to the black-box model f θ θ θ .




# [Cross-Lingual Abstractive Summarization with Limited Parallel Resources](https://aclanthology.org/2021.acl-long.538/)
- We name our model Multi-task Cross-Lingual Abstractive Summarization (MCLAS) under limited resources.

- Therefore, in this paper, we will develop a new model for cross-lingual abstractive summarization under limited supervision.

- In this paper, we propose a novel multi-task learning framework MCLAS to achieve cross-lingual abstractive summarization with limited parallel resources.

- To our best knowledge, cross-lingual summarization under low-resource settings has not been well investigated and explored.




# [DynaEval: Unifying Turn and Dialogue Level Evaluation](https://aclanthology.org/2021.acl-long.441/)
- (3) Empirical results show that DynaEval outperforms the stateof-the-art dialogue coherence model and strongly correlates with human judgements at both turn and dialogue level.

- S-DiCoh (Mesgar et al., 2020) is a recent state-of-the-art dialogue coherence model.

- DynaEval serves as a unified framework for both turn and dialogue level evaluation in open-domain dialogue.

- We propose Dy-naEval to provide meaningful dialogue-level representation with explicit modeling of the interactive dynamics among interlocutors, for a unified turn and dialogue level quality assessment.




# [CTFN: Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network](https://aclanthology.org/2021.acl-long.412/)
- Importantly, the cyclic consistency constraint is presented to improve the translation performance.

- Based on CTFN, a hierarchical architecture is further established to exploit multiple bi-direction translations, leading to double multimodal fusing embeddings compared with traditional translation methods.

- On the basis of CTFN, a hierarchical architecture is established to exploit multiple bi-direction translations, leading to double multimodal fusing embeddings (Figure 4).

- Specifically, the cyclic consistency constraint is proposed to improve the translation performance, allowing us directly to discard decoder and only embrace encoder of Transformer.




# [Controversy and Conformity: from Generalized to Personalized Aggressiveness Detection](https://aclanthology.org/2021.acl-long.460/)
- In this paper, we present novel methods of personalized aggressive content detection based on the representation of user opinion about aggressive texts.

- The gain provided by our personalized methods is greater for more controversial documents.

- The gain is greater for more controversial documents.

- We also checked contribution of aggressive texts for the consecutive most controversial documents included in the personal user embeddings, Fig. 3.




# [Structural Guidance for Transformer Language Models](https://aclanthology.org/2021.acl-long.289/)
- We show evidence that generative structural supervision indeed induces more robust and human-like linguistic generalization in Transformer language models and explore the different trade-offs involved in the presented methods.

- Our work explores two forms of syntactic supervision as structural guidance for Transformer language models.

- Here we explore two major classes of structural guidance for Transformer language models based on joint modeling of language and constituency parses.

- This work hypothesizes that the Transformer language model may benefit from explicit generative structural supervision to systematically generalize syntactic knowledge.




# [Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition](https://aclanthology.org/2021.acl-long.432/)
- In addition, the supervised learning with a very small scale of expert annotations can boost the performance significantly.

- In addition, we introduce the unsupervised and supervised settings for crowdsourcing learning which are directly borrowed from the domain adaptation.

- By this case study, we introduced unsupervised and supervised crowdsourcing learning, where the former is a widelystudied setting while the latter has been seldom investigated.

- (2) We propose a novel method for crowdsourcing learning.




# [Better than Average: Paired Evaluation of NLP Systems](https://aclanthology.org/2021.acl-long.179/)
- The choice of aggregation mechanism matters in real evaluation setups, and we therefore recommend BT as a robust aggregation mechanism.

- By re-evaluating 296 real NLP evaluation setups across four tasks and 18 evaluation metrics, different aggregation mechanisms yield different conclusions as to which systems are SotA in about 30% of the setups (Sec. 5).

- To ease the adoption of more robust aggregation mechanisms, we release Pairformance, 1 a practical tool for performing full analyses of evaluation scores with mean, median, BT, and two variants of BT (Elo and TrueSkill).

- We release Pairformance, a tool for performing full diagnostic analyses based on an evaluation dataframe made of the evaluation scores of systems and baselines.




# [Factuality Assessment as Modal Dependency Parsing](https://aclanthology.org/2021.acl-long.122/)
- • We develop a joint modal dependency parsing model that extracts events, conceivers and parses a document into its modal dependency structure.

- We evaluate our joint model against the pipeline model.

- In addition, we evaluate the joint model against the pipeline model, and show the advantage of the joint model in overall end-to-end modal dependency parsing performance.

- In this paper, we proposed a novel approach to factuality assessment by casting it as a modal dependency parsing problem.




# [ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information](https://aclanthology.org/2021.acl-long.161/)
- In this work, we propose ChineseBERT, a model that incorporates the glyph and pinyin information of Chinese characters into the process of largescale pretraining.

- The glyph embedding is based on different fonts of a Chinese character, being able to capture character semantics from the visual surface character forms.

- We hypothesize glyph and pinyin embeddings also serve as strong regularization over text semantics, which means that the proposed ChineseBERT model is able to perform better with less training data.

- Due to the additional consideration of glyph and pinyin, the proposed cannot be directly initialized using a vanilla BERT model, as the model structures are different.




# [A DQN-based Approach to Finding Precise Evidences for Fact Verification](https://aclanthology.org/2021.acl-long.83/)
- Comparison on retrieval of precise evidences.

- Thus, a post-processing strategy is needed to tackle the label bias on Q-values.

- Inspired by the strong exploration ability of the Deep Q-learning Network (DQN) (Mnih et al., 2015), we develop a DQN-based approach to retrieval of precise evidences.

- Existing methods for FV do not target the retrieval of precise evidences.




# [Reservoir Transformers](https://aclanthology.org/2021.acl-long.331/)
- Our contributions are as follows: • We introduce a area under the convergence curve metric for measuring performanceefficiency trade-offs, and show that replacing regular transformer layers with reservoir layers leads to improvements.

- We introduce "reservoir transformers", wherein fixed random reservoir layers are interspersed with regular updateable transformer layers.

- This work explores inserting random non-linear transformations, or what we call reservoir layers, into transformer networks.

- • We show that the addition of reservoir layers leads to improved test set generalization on a variety of tasks in a variety of settings.




# [ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning](https://aclanthology.org/2021.acl-long.260/)
- In this paper, we present ERICA, a general framework for PLMs to improve entity and relation understanding via contrastive learning.

- The experimental results show that ERICA improves the performance of typical PLMs (BERT and RoBERTa) and outperforms baselines, especially under lowresource settings, which demonstrates that ERICA effectively improves PLMs' entity and relation understanding and captures the in-text relational facts.

- Specifically, we define two novel pre-training tasks: (1) the entity discrimination task to distinguish which tail entity can be inferred by the given head entity and relation.

- We demonstrate the effectiveness of our method on several language understanding tasks, including relation extraction, entity typing and question answering.




# [Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search](https://aclanthology.org/2021.acl-long.508/)
- unless for the proposed Drop-and-Restore

- We thus propose to extend PoWER-BERT with a novel Drop-and-Restore process ( §3.3), which eliminates this inherent limitation.

- Second, we design Drop-and-Restore process that makes PoWER-BERT applicable beyond classification, which enables PoWER-BERT to be applicable to a wider range of NLP tasks such as span-based question answering.

- In addition to our main contribution above, we thus propose to extend the PoWER-BERT so that it is applicable to token-level classification, such as span-based question-answering.




# [TEXT2EVENT: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction](https://aclanthology.org/2021.acl-long.217/)
- 2. We design an effective sequence-to-structure architecture, which is enhanced with a constrained decoding algorithm for event knowledge injection during inference and a curriculum learning algorithm for efficient model learning.

- In this paper, we propose TEXT2EVENT, a sequence-to-structure generation paradigm for event extraction.

- Concretely, we propose an effective sequence-to-structure network for event extraction, which is further enhanced by a constrained decoding algorithm for event knowledge injection during inference and a curriculum learning algorithm for efficient model learning.

- In summary, the contributions are as follows: 1. We propose a new paradigm for event extraction --sequence-to-structure generation, which can directly extract events from the text in an end-to-end manner.




# [Learning Relation Alignment for Calibrated Cross-modal Retrieval](https://aclanthology.org/2021.acl-long.43/)
- Finally, we propose a metric named Intra-modal Self-attention Distance (ISD) to quantify the relation consistency.

- , we propose a metric called Intra-modal Self-attention Distance with annotation (ISDa) to quantify their semantic gap at the relation level.

- Accordingly, we propose a new regularized training method called Inter-modal Alignment on Intra-modal Selfattentions (IAIS) to calibrate two intra-modal attention distributions mutually via inter-modal alignment, which helps learn better contextualized representations for image-text pairs.

- Furthermore, we present a regularized training method IAIS to calibrate intra-modal selfattentions mutually by minimizing the ISD metric.




# [Self-Attention Networks Can Process Bounded Hierarchical Languages](https://aclanthology.org/2021.acl-long.292/)
- These findings agree with our theoretical characterization that selfattention networks have a memory advantage over recurrent ones.

- In this paper, we theoretically and experimentally demonstrate that self-attention networks can process bounded hierarchical languages Dyck k,D , even with a memory advantage over recurrent networks, despite performing distributed processing of sequences without explicit recursive elements.

- So, what can self-attention networks tell us about natural language and recursion?

- In particular, we prove that self-attention networks can both recognize and generate Dyck k,D , with two conceptually simple yet different constructions (Figure 1).




# [Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task](https://aclanthology.org/2021.acl-long.328/)
- 2. A parameter sharing and initialization strategy are proposed to encourage information sharing between tasks.

- 3. Cross-attentive regularization and online knowledge distillation are proposed to reduce the model representation difference between different modalities and enhance the knowledge transfer from the MT task to the ST task.

- Finally, an online knowledge distillation learning is introduced for MTL in order to enhance knowledge transfer from the MT to the ST task.

- A novel crossattentive regularization is proposed to reduce the distance between encoder outputs from different input modalities.




# [A Pre-training Strategy for Zero-Resource Response Selection in Knowledge-Grounded Conversations](https://aclanthology.org/2021.acl-long.343/)
- Our contributions are summarized as follows: • To the best of our knowledge, this is the first exploration of knowledge-grounded response selection under the zero-resource setting.

- In particular, we propose decomposing the training of the knowledge-grounded response selection into three tasks and joint train all tasks in a unified pre-trained language model.

- We first describe a standard knowledge-grounded response selection task such as Wizard-of-Wikipedia.

- In this work, we consider building a knowledge-grounded response matching model with BERT.




# [Point, Disambiguate and Copy: Incorporating Bilingual Dictionaries for Neural Machine Translation](https://aclanthology.org/2021.acl-long.307/)
- Copier couples Pointer and Disambiguator based on a hierarchical copy mechanism seamlessly integrated with Transformer, thereby building a sophisticated endto-end architecture.

- (3) The above two steps are then systematically integrated based on a hierarchical copy mechanism.

- To address the above problems, we propose a novel neural architecture consisting of three novel components: Pointer, Disambiguator, and Copier, to effectively incorporate bilingual dictionaries into NMT models in an end-to-end manner.

- Finally, Copier connects the outputs of Pointer and Disambiguator via a hierarchical copy operation.




# [Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction](https://aclanthology.org/2021.acl-long.367/)
- • We propose a dual-channel span pruning strategy by incorporating explicit supervision from the ATE and OTE tasks to ease the high computational cost caused by span enumeration and maximize the chances of pairing valid target and opinion candidates together.

- Hence, we propose to use a dual-channel pruning strategy which results in two separate pruned pools of aspects and opinions.

- We employ the ABSA subtasks of ATE and OTE to guide our dual-channel span pruning strategy through the scores of the predicted opinion and target span.

- In this work, we propose a span-level approach -Span-ASTE to learn the interactions between target spans and opinion spans for the ASTE task.




# [Hierarchical Context-aware Network for Dense Video Event Captioning](https://aclanthology.org/2021.acl-long.156/)
- Compared with these works, we are the first to implement a novel video-level hierarchical context-aware network for dense video event captioning.

- Our contributions can be summarized as: 1) We propose a hierarchical context-aware model for dense video event captioning to capture video-level context.

- In this paper, we propose a novel hierarchical context-aware model for dense video event captioning (HCN) to capture both the local and global context simultaneously.

- In this paper, we propose a novel hierarchical context-aware network to encode both the local and global context of long videos.




# [A Survey of Race, Racism, and Anti-Racism in NLP](https://aclanthology.org/2021.acl-long.149/)
- However, questions of race and racial bias have been minimally explored in NLP literature.

- The papers we surveyed suggest that research on race in NLP has used a very limited range of data sets, which fails to account for the multidimensionality of race and simplifications inherent in classification.

- In this work, we conduct a comprehensive survey of how NLP literature and research practices engage with race.

- We suggest that readers use our work as one starting point for bringing inclusion and racial justice into NLP.




# [Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases](https://aclanthology.org/2021.acl-long.146/)
- These findings strongly question previous conclusions that current MLMs could serve as reliable factual knowledge bases.

- To this end, this paper conducts a thorough study on whether MLMs could be reliable factual knowledge bases.

- Our findings strongly question the conclusions of previous literatures, and demonstrate that current MLMs can not serve as reliable knowledge bases when using prompt-based retrieval paradigm.

- All the above findings demonstrate that current MLMs are not reliable in factual knowledge extraction.




# [SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis](https://aclanthology.org/2021.acl-long.175/)
- Future work could also focus on optimal weighting between semantics and style.

- Our analysis leads us to two caption evaluation metrics that capture separate dimensions of caption quality and a fused metric.

- In order to provide caption-level insight as well, we combine SPURTS, SPARCS, and our grammar outlier penalty into one metric -SeMantic and linguistic UndeRstanding Fusion (SMURF) -which rewards captions based on semantics and fluency.

- Both BERT and RoBERTa have achieved state-of-the-art results in various language understanding tasks.




# [An In-depth Study on Internal Structure of Chinese Words](https://aclanthology.org/2021.acl-long.452/)
- Third, we propose word-internal structure as a new task, and present benchmark results using a popular dependency parser.

- Utilizing word-internal structure.

- Annotating word-internal structure.

- This paper presents a thorough study on internal structures of Chinese words.




# [AGGGEN: Ordering and Aggregating while Generating](https://aclanthology.org/2021.acl-long.113/)
- Recent neural data-to-text systems generate text "end-to-end" (E2E) by learning an implicit mapping between input representations (e.g. RDF triples) and target texts.

- We demonstrate this for two data-to-text generation tasks: the E2E NLG (Novikova et al., 2017) and the WebNLG Challenge (Gardent et al., 2017a).

- Our contributions are as follows: • We present a novel interpretable architecture for jointly learning to plan and generate based on modelling ordering and aggregation by aligning facts in the target text to input representations with an HMM and Transformer encoder-decoder.

- The model is trained end-to-end and all intermediate steps are learned in a unified framework.




# [BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?](https://aclanthology.org/2021.acl-long.280/)
- In this paper, we have presented an extensive analysis of the ability of language models to identify analogies.

- On the other hand, when carefully tuned, some language models are able to achieve state-of-the-art results.

- The aim of this paper is to analyze the ability of pre-trained LMs to recognize analogies.

- To directly answer the question posed in the title, our conclusion is that language models can identify analogies to a certain extent, but not all language models are able to achieve a meaningful improvement over word embeddings (whose limitations in analogy tasks are well documented).




# [Including Signed Languages in Natural Language Processing](https://aclanthology.org/2021.acl-long.570/)
- We urge the inclusion of signed languages in NLP.

- However, they only capture one aspect of signed languages.

- We survey common SLP tasks and limitations of current methods by drawing on linguistic theories of signed languages.

- The limitations in the design of current SLP models often stem from the lack of exploring the linguistic possibilities of signed languages.




# [TWAG: A Topic-guided Wikipedia Abstract Generator](https://aclanthology.org/2021.acl-long.356/)
- We propose a novel two-stage Topic-guided Wikipedia Abstract Generation model (TWAG).

- In conclusion, the contributions of our work are as follows: • We propose TWAG, a two-stage neural abstractive Wikipedia abstract generation model utilizing the topic information in Wikipedia, which is capable of generating comprehensive abstracts.

- In this paper, we propose a novel topic-guided abstractive summarization model TWAG for generating Wikipedia abstracts.

- However, these models are not suitable for Wikipedia abstract generation.




# [Probabilistic, Structure-Aware Algorithms for Improved Variety, Accuracy, and Coverage of AMR Alignments](https://aclanthology.org/2021.acl-long.257/)
- Contributions are as follows: • A novel all-inclusive formulation of AMR alignment in terms of mappings between spans and connected subgraphs, including spans aligned to multiple subgraphs; mappings between spans and inter-subgraph edges; and characterization of reentrancies.

- TAMR (Tuned Abstract Meaning Representation; Liu et al., 2018) uses the JAMR alignment rules, along with two others, to produce a set of candidate alignments for the sentence.

- Research with the Abstract Meaning Representation (AMR; Banarescu et al., 2013), a broadcoverage semantic annotation framework in which sentences are paired with directed acyclic graphs, must contend with the lack of gold-standard alignments between words and semantic units in the English data.

- This formulation lends itself to unsupervised learning of alignment models.




# [Joint Models for Answer Verification in Question Answering Systems](https://aclanthology.org/2021.acl-long.252/)
- The results show that our models can outperform the state of the art.

- We have proposed new joint models for AS2.

- ASR establishes the new state of the art on WikiQA with an MAP of 92.80 vs. 92.00.

- This is essentially the state-of-the-art AS2 model based on the TANDA approach applied to RoBERTa pre-trained transformer.




# [Beyond Offline Mapping: Learning Cross-lingual Word Embeddings through Context Anchoring](https://aclanthology.org/2021.acl-long.506/)
- We achieve this by combining cross-lingual anchoring with self-learning and iterative restarts.

- For that purpose, we use an extension of skip-gram that leverages translated context words as anchor points.

- So as to understand the role of self-learning and the iterative restarts in our approach, we perform an ablation study and report our results in Table 6.

- This suggests that both the self-learning and the iterative restarts are helpful to make the method more robust to a weak initialization, and have a minor impact otherwise.




# [CoRI: Collective Relation Integration with Data Augmentation for Open Information Extraction](https://aclanthology.org/2021.acl-long.363/)
- To address the issue, we propose a collective approach CoRI, which achieves collective relation integration via two stages: candidate generation and collective inference.

- In this paper, we proposed CoRI, a collective inference approach to relation integration.

- To alleviate the incoherent prediction issue of local approaches, we propose Collective Relation Integration (CoRI) that exploits the dependency of predictions between adjacent entity pairs to enforce global coherence.

- Definition 1 (Relation Integration).




# [Multi-hop Graph Convolutional Network with High-order Chebyshev Approximation for Text Reasoning](https://aclanthology.org/2021.acl-long.513/)
- This study proposes a multi-hop graph convolutional network on high-order dynamic Chebyshev approximation (HDGCN) for text reasoning.

- To improve the efficiency and performance of multi-hop graph reasoning in spectral graph convolution, we proposed a new graph convolutional network with high-order dynamic Chebyshev approximation (HDGCN).

- The main contributions of this paper are listed below: • To improve the efficiency and performance of multi-hop reasoning in spectral graph convolution, we propose a novel graph convolutional network with high-order dynamic Chebyshev Approximation (HDGCN).

- This way have the hidden pairwise interactions to improve the multi-hop graph reasoning in high-order Chebyshev polynomials.




# [Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks](https://aclanthology.org/2021.acl-long.47/)
- We propose a parameter-efficient method for multi-task fine-tuning.

- In summary, we make the following contributions: (1) We propose a parameter-efficient method for multitask fine-tuning based on hypernetworks and adapter layers.

- (3) We provide empirical results on GLUE demonstrating the effectiveness of the proposed method on multi-task learning.

- Extensive experiments show that our method obtains strong improvement over multi-task learning on the GLUE benchmark, and substantially improves the in-domain task generalization.




# [Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates](https://aclanthology.org/2021.acl-long.515/)
- Our main contributions are as follows: • We provide empirical evidence to show that the model update regression occurs across text classification tasks in NLP; • We formulate the regression-free model updates into a constrained optimization problem, and further reduce into a relaxed form which can be approximately optimized through knowledge distillation training method; • We also explore the model ensemble as another method to reduce regression, and analyzed its efficacy; • We analyze the source of the regressions in NLP tasks through linguistic behavioural testing, compare reduction in both distillation and ensemble methods.

- Through linguistic behavioral testing we showed that distillation can reduce the regression across a wider range of linguistic phenomena than ensemble method.

- Here we include model ensemble as an alternative approach to reduce regression, with further analysis on how ensemble reduces regression in Section 5.1.

- Experiments on the GLUE benchmark showed that ensemble can be effective in reducing the regression when updating to homogeneous models.




# [Best of Both Worlds: Making High Accuracy Non-incremental Transformer-based Disfluency Detection Incremental](https://aclanthology.org/2021.acl-long.286/)
- Here we describe the different strategies we used to modify the training and live decoding methods of non-incremental models to detect speech disfluencies word-by-word incrementally.

- The results on ASR transcripts are also state-of-the-art.

- Our system is competitive at reparadnum word detection and achieves state-of-the-art results in edit term detection.

- Here we design and evaluating models that work with online, incremental speech recognition output to detect disfluencies with varying levels of granularity.




# [To POS Tag or Not to POS Tag: The Impact of POS Tags on Morphological Learning in Low-Resource Settings](https://aclanthology.org/2021.acl-long.78/)
- For both tasks the impact made by the presence or absence of POS tags is minimal.

- The overall impact of POS tags is not significant.

- We conclude that the presence or absence of POS tags does not have a significant impact on two morphological learning tasks: segmentation and glossing, or reinflection.

- This paper describes experiments that were run on corpora differing only in the presence or absence of POS tags.




# [Label-Specific Dual Graph Neural Network for Multi-Label Text Classification](https://aclanthology.org/2021.acl-long.298/)
- The contributions of this paper are as follows: • We propose a novel label-specific dual graph neural network (LDGN), which incorporates category information to extract label-specific components from documents, and explores the interactions among these components.

- The outstanding results confirm the effectiveness of label-specific semantic interaction learning with dual graph neural network, which include global statistical patterns and local dynamic relations.

- Thus, our goal is to find a way to explore the complete and adaptive interactions among label-specific semantic components more accurately.

- Thus, our goal is to find a way to explore the complete and adaptive interactions among labelspecific semantic components more accurately.




# [UnNatural Language Inference](https://aclanthology.org/2021.acl-long.569/)
- We find, based on a suite of permutation metrics, that they are not.

- We show that state-of-the-art models do not rely on sentence structure the way we think they should: NLI models (Transformer-based models, RNNs, and ConvNets) are largely insensitive to permutations of word order that corrupt the original syntax.

- In (a), we investigate the state-of-the-art pre-trained models such as RoBERTa-Large (Liu et al., 2019), BART-Large (Lewis et al., 2020) and DistilBERT .

- We find in our experiments that the state-of-theart Transformer-based NLI models (as well as pre-Transformer class of models) do not perform like any of the above hypothetical models.




# [A Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy](https://aclanthology.org/2021.acl-long.34/)
- In this paper, we propose a novel training-free and reference-free summarization evaluation metric consisting of a relevance score and a redundancy score.

- Our metric is composed of a centrality-weighted relevance score and a self-referenced redundancy score.

- To solve the above limitations, based on SU-PERT, we propose a novel training-free and reference-free metric for both multiple and single document summarization evaluation.

- Our final evaluation score of a summary consists of an averaged centrality-weighted relevance score and a self-referenced redundancy score.




# [Towards Table-to-Text Generation with Numerical Reasoning](https://aclanthology.org/2021.acl-long.115/)
- • We introduce a new dataset for table-totext generation focusing on numerical reasoning.

- We proposed numericNLG, a new dataset for tableto-text generation using a table and its corresponding description from scientific papers, focusing on numerical-reasoning texts.

- Recent research on the table-to-text generation task is starting to generate text with more reasoning.

- A copy mechanism has been widely explored to improve faithfulness in various ways.




# [Fine-grained Information Extraction from Biomedical Literature based on Knowledge-enriched Abstract Meaning Representation](https://aclanthology.org/2021.acl-long.489/)
- We develop a new ontology for entities and events with a large corpus from COVID-19 research papers, which is specifically annotated by medical professionals and can serve as a new benchmark for the biomedical IE community.

- To tackle these two challenges, we propose a novel framework for biomedical IE that integrates Abstract Meaning Representation (AMR) (Banarescu et al., 2013) and external knowledge graphs.

- We will also continue exploring the use of richer information from an external knowledge base to further improve the model's performance.

- In this paper, we propose a novel biomedical Information Extraction framework to effectively tackle two unique challenges for scientific domain IE: complex sentence structure and unexplained concepts.




# [Neural Stylistic Response Generation with Disentangled Latent Variables](https://aclanthology.org/2021.acl-long.339/)
- Our contributions are listed below: • We propose a unified framework to simultaneously improve style intensity and maintain content relevance for neural stylistic response generation.

- Experimental results show that our proposed approach improves style intensity and maintains content relevance.

- We propose a uniform framework to simultaneously improve the style intensity and maintain the content relevance for neural stylistic response generation.

- We thus disentangle the content and style by diluting sentence-level information in the style representation.




# [BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised Named Entity Recognition](https://aclanthology.org/2021.acl-long.482/)
- We propose the conditional hidden Markov model (CHMM) to infer true NER labels from multi-source weak annotations.

- The conditional hidden Markov model is an HMM variant for multi-source label denoising.

- Our contributions include: • A multi-source label aggregator CHMM with token-wise transition and emission probabilities for aggregating multiple sets of NER labels from different weak labeling sources.

- It integrates a label aggregator-CHMM and a supervised model-BERT-NER together into an alternate-training procedure.




# [Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning](https://aclanthology.org/2021.acl-long.93/)
- This result strengthens our claim that competition between learned linguistic constraints can obscure underlying linguistic knowledge in model behavior.

- The present study provided evidence that model behavior can be meaningfully described, and understood, with reference to competing constraints.

- Our findings suggest that some linguistic knowledge may never surface in model behavior, though further work is needed on this point.

- Other linguistic processes influence pronouns in Spanish and Italian, and we showed that competition between multiple distinct constraints affects model behavior.




# [Syntopical Graphs for Computational Argumentation Tasks](https://aclanthology.org/2021.acl-long.126/)
- 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection.

- We decompose the problem of viewpoint reconstruction into the subtasks of stance detection and aspect detection, and evaluate the benefits of syn-topical graphs -which are a collection-level approach -on both tasks.

- The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph.

- The newly created graph is then used for stance and aspect detection, to reconstruct viewpoints.




# [ADEPT: An Adjective-Dependent Plausibility Task](https://aclanthology.org/2021.acl-long.553/)
- We present a new large-scale corpus and task, ADEPT, for assessing semantic plausibility.

- We evaluate several transformer-based models on ADEPT.

- We introduce a novel plausibility task: Using automated mechanisms to extract, filter and construct natural sentences, we create ADEPT-a large human-labeled semantic plausibility task consisting of 16 thousand pairs of sentences that differ only by one adjective added to a noun, and designed to resist the statistical correlations that might underpin modern distributional lexical semantics.

- We also plan to develop new models on ADEPT and transfer them to other semantic plausibility tasks.




# [A Dataset and Baselines for Multilingual Reply Suggestion](https://aclanthology.org/2021.acl-long.97/)
- In summary, we present MRS, a multilingual reply suggestion dataset.

- We present MRS, a multilingual dataset for reply suggestion.

- MRS is also a useful benchmark for future research in reply suggestion and cross-lingual generalization.

- Previous reply suggestion models were only studied in the English monolingual setting.




# [Language Embeddings for Typology and Cross-lingual Transfer Learning](https://aclanthology.org/2021.acl-long.560/)
- We train our model on the English MultiNLI  dataset, and directly evaluate the trained model on the other languages without language-specific fine-tuning, in a zero-shot cross-lingual setting.

- In this paper, following the finding that structural similarity is critical in multilingual language models (K et al., 2020), we generate language embeddings from a denoising autoencoder objective and demonstrate that they can be effectively used in cross-lingual zero-shot learning.

- We showed that the trained language embeddings represent typological information, and can also benefit the downstream tasks in a zero-shot learning setting.

- In addition, to address the question of whether the learned language embeddings can help in downstream language tasks, we plug-in the language embeddings to cross-lingual dependency parsing and natural language inference (XNLI,  in a zero-shot learning setting, obtaining performance improvements.




# [From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text](https://aclanthology.org/2021.acl-long.245/)
- Below, we summarize our main contributions: 1. We propose a state-of-the-art translation model that generates Hindi-English CS text starting from monolingual Hindi text.

- Also, ours is the first work, to our knowledge, to repurpose an unsupervised neural machine translation model to translate monolingual sentences into CS text.

- GLUECoS (Khanuja et al., 2020) is an evaluation benchmark spanning six natural language tasks for code-switched English-Hindi and English-Spanish data.

- 3. We use sentences generated from our model to train language models for Hindi-English CS text and show significant improvements in perplexity compared to other approaches.




# [Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP](https://aclanthology.org/2021.acl-long.345/)
- In this paper we introduce AmbER sets, a benchmark for evaluating the entity disambiguation capabilities of retrievers across multiple NLP tasks.

- We introduce AmbER sets for evaluating entity disambiguation capabilities of retrievers and analyze the role of entity popularity in disambiguation.

- In this work, we create AmbER sets for three tasks: fact checking, slot filling, and question answering (Table 2).

- Despite the importance of entity disambiguation, it remains an understudied problem for open-domain NLP.




# [ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic](https://aclanthology.org/2021.acl-long.551/)
- Our models are publicly available.

- Our models establish new stateof-the-art (SOTA) on the majority of tasks, across all cluster tasks.

- For evaluation, we also introduce a novel ARabic natural Language Understanding Evaluation benchmark (ARLUE).

- We presented our efforts to develop two powerful Transformer-based language models for Arabic.




# [Generating Relevant and Coherent Dialogue Responses using Self-separated Conditional Variational AutoEncoders](https://aclanthology.org/2021.acl-long.437/)
- To address these drawbacks, we propose a novel model, namely Self-Separated Conditional Variational Autoencoder (SepaCVAE).

- Thus this approach sacrifices too much relevance and coherence for diversity and informativeness.

- In a word, the evaluation results illustrate the effectiveness of SepaCVAE in terms of improving the relevance and coherence of responses.

- Open-domain dialogue generation is a challenging task in natural language processing.




# [Supporting Land Reuse of Former Open Pit Mining Sites using Text Classification and Active Learning](https://aclanthology.org/2021.acl-long.320/)
- We used active learning for multi-label text classification to extract restrictions and topics from unstructured text in legacy documents and visualized the results using a GIS.

- In this work, we have presented and evaluated a system which automates information requests related to the post-management of former open pit mines by leveraging unstructured and geospatial data.

- We evaluate multi-label active learning performed by three human annotators, who each train a sentence classification model for classifying restrictions and topics, resulting in two runs per person.

- We address this issue by demonstrating and evaluating a workflow consisting of optical character recognition (OCR), text classification and active learning, whose results are then visualized by a Geographic Information System (GIS).




# [H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences](https://aclanthology.org/2021.acl-long.294/)
- We have proposed a new Transformer attention using the inductive bias inspired by the H-Matrix.

- In our case, it also serves to highlight the effectiveness of the inductive bias inspired by the H-Matrix method, as well as the capability of our hierarchical attention to handle long sequences.

- We propose a hierarchical attention that has linear complexity in run time and memory, and only utilizes dense linear algebra operations optimized for GPUs or TPUs.

- We hypothesize that the inductive bias embodied by the proposed hierarchical structure for the attention matrix is effective in capturing the hierarchical structure in the sequences typically seen in many natural language processing and computer vision tasks.




# [SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining](https://aclanthology.org/2021.acl-long.457/)
- In this paper, we introduce SMedBERT, a KE-PLM pre-trained over large-scale medical corpora and medical KGs.

- To the best of our knowledge, SMedBERT is the first PLM with structured semantics knowledge injected in the medical domain.

- (1) Rich semantic information from neighboring structures of linked-entities, such as entity types and relations, are highly useful for medical text understanding.

- (2) Mention-neighbor hybrid attention aims to infuse the structured semantics knowledge into encoder layers, which includes type attention, node attention and gated position infusion module.




# [Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact Verification](https://aclanthology.org/2021.acl-long.128/)
- We have presented a novel topic-aware evidence reasoning and stance-aware aggregation model for fact verification.

- The main contributions are listed as follows: • We propose a novel topic-aware evidence reasoning and stance-aware aggregation approach, which is, to our best knowledge, the first attempt of jointly exploiting semantic interaction and topical consistency to learn latent evidence representation for fact verification.

- To address these problems, in this paper, we propose a novel neural structure reasoning model for fact verification, named TARSA (Topic-Aware Evidence Reasoning and Stance-Aware Aggregation Model).

- Therefore, two kinds of topical relationship are considered: 1) topical coherence among multiple pieces of evidence (T C ee ); 2) topical consistency between the claim and each evidence (T C ce ).




# [How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models](https://aclanthology.org/2021.acl-long.243/)
- 1) We systematically compare monolingual with multilingual pretrained language models for 9 typologically diverse languages on 5 structurally different tasks.

- 4) Our results suggest that monolingually adapted tokenizers can robustly improve monolingual performance of multilingual models.

- diverse languages and across a variety of downstream tasks

- Further, we have disentangled the impact of pretrained corpora size from the influence of the tokenizers on the downstream task performance.




# [What Context Features Can Transformer Language Models Use?](https://aclanthology.org/2021.acl-long.70/)
- We have investigated the extent to which transformer models can use structural and lexical information in long-range contexts for English language modeling.

- This transformation removes significant information in both mid-and long-range conditions (55% and 69% Figure 3: Effect of word identity on usable information.

- Longer contexts, even of a kind previously found to be informative, did not provide additional usable information.

- Notably, the shuf. within trigrams (14% and 41%) and the shuf. trigrams within sent. (16% and 35%) ablations both remove relatively little usable information in both the mid-and long-range conditions.




# [Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification](https://aclanthology.org/2021.acl-long.40/)
- Finally, we showed that attention-based explanations with TaSc outperform other interpretability techniques.

- Motivated by this, we aim to improve the effectiveness of neural models in providing more faithful attention-based explanations for text classification, by introducing noncontextualised information in the model.

- However, our main aim is not to improve predictive performance but the faithfulness of attention-based explanations, which we illustrate below.

- This demonstrates the efficacy of TaSc in providing more faithful attention-based explanations than strong baselines without TaSc (Nguyen, 2018;Atanasova et al., 2020).




# [Generation-Augmented Retrieval for Open-Domain Question Answering](https://aclanthology.org/2021.acl-long.316/)
- 3 Generation-Augmented Retrieval

- (1) We propose Generation-Augmented Retrieval (GAR), which augments queries with heuristically discovered relevant contexts through text generation without external supervision or time-consuming downstream feedback.

- In this work, we propose Generation-Augmented Retrieval and demonstrate that the relevant contexts generated by PLMs without external supervision can significantly enrich query semantics and improve retrieval accuracy.

- In this paper, we propose Generation-Augmented Retrieval (GAR), which augments a query through text generation of a pre-trained language model (PLM).




# [Discriminative Reranking for Neural Machine Translation](https://aclanthology.org/2021.acl-long.563/)
- First, we notice that all methods improve over the beam search output with gains ranging from 1.0 to 4.1 BLEU.

- In this paper, we explore whether training large transformer models using the reranking objective can further improve performance.

- Indeed, recent generative reranking approaches applied to NMT, such as Noisy-Channel Decoding (NCD,  which leverages a pre-trained language model and a backward model, show strong improvements over beam search outputs, as demonstrated in recent WMT evaluations .

- We can see that in all cases, discriminative reranking yields better translations, with gains between 0.2 and 2.3 BLEU points depending on the language direction.




# [Compare to The Knowledge: Graph Neural Fake News Detection with External Knowledge](https://aclanthology.org/2021.acl-long.62/)
- Based on the above directed heterogeneous document graph G, we develop a heterogeneous graph attention network for learning the news representation as well as the contextual entity representations.

- Based on the directed heterogeneous document graph, we develop a heterogeneous graph attention network to learn topic-enriched news representations and contextual entity representations.

- Based on the graph, we develop a heterogeneous graph attention network to learn the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news document.

- Based on the graph, we develop a heterogeneous graph attention network for learning topic-enriched news representation as well as contextual entity representations that encode the semantics of the content of the news document.




# [Lightweight Cross-Lingual Sentence Representation Learning](https://aclanthology.org/2021.acl-long.226/)
- Concerning the training tasks, we propose a novel cross-lingual language model, which combines SMLM and XTR.

- We perform cross-lingual sentence representation learning by a lightweight dual-transformer framework.

- In this paper, we presented a lightweight dualtransformer based cross-lingual sentence representation learning method.

- By introducing the above-stated training tasks, we establish a computationally-lite framework for training cross-lingual sentence models.




# [PLOME: Pre-training with Misspelled Knowledge for Chinese Spelling Correction](https://aclanthology.org/2021.acl-long.233/)
- We propose PLOME, a pre-trained masked language model with misspelled knowledge for CSC.

- In this paper, we propose PLOME, a Pre-trained masked Language mOdel with Misspelled knowl-edgE, for Chinese spelling correction.

- We summarize our contributions as follows: (1) PLOME is the first task-specific language model designed for Chinese spelling correction.

- To the best of our knowledge, PLOME is the first task-specific language model for CSC, which jointly learns semantics and misspelled knowledge




# [The Possible, the Plausible, and the Desirable: Event-Based Modality Detection for Language Processing](https://aclanthology.org/2021.acl-long.77/)
- We propose an event-based modality detection task which is based on solid theoretical foundations yet is adapted to fit the needs of NLP practitioners.

- Next, we aim to jointly predict the modal triggers and their modified events.

- 1229 of the modal triggers are modal verbs.

- First, we change the modal sense labels to be intuitive and self-explanatory.




# [PairRE: Knowledge Graph Embeddings via Paired Relation Vectors](https://aclanthology.org/2021.acl-long.336/)
- We also find the paired relation vectors enable an adaptive adjustment of the margin in loss function, which alleviates the modeling problem for complex relations.

- To overcome the problem of modeling 1-to-N/Nto-1/N-to-N complex relations and enrich the capabilities for different relation patterns, we propose a model with paired vectors for each relation.

- Here we present PairRE, an embedding method that is capable of encoding complex relations and multiple relation patterns simultaneously.

- The recent state-ofthe-art RotatE shows promising results to encode symmetry/antisymmetry, inverse and composition relations.




# [Tree-Structured Topic Modeling with Nonparametric Neural Variational Inference](https://aclanthology.org/2021.acl-long.182/)
- The results indicate that our model is able to learn a reasonable tree-structured topic hierarchy with low redundancy.

- To address these limitations, we propose a novel nonparametric neural method to generate tree-structured topic hierarchies, namely nonparametric Tree-Structured Neural Topic Model (nTSNTM) 1 .

- In (Griffiths et al., 2004), a tree-structured topic model called hLDA was first proposed by introducing a nested Chinese restaurant process (nCRP).

- Firstly, the breaking fractions do not obey the Beta distributions adopted in the stick-breaking process (SBP).




# [Psycholinguistic Tripartite Graph Network for Personality Detection](https://aclanthology.org/2021.acl-long.326/)
- • We propose a novel tripartite graph network, TrigNet, with a flow GAT to reduce the computational cost in graph learning.

- In this work, we proposed a novel psycholinguistic knowledge-based tripartite graph network, TrigNet, for personality detection.

- Figure 2 presents the overall architecture of the proposed TrigNet, which consists of a tripartite graph network and a BERT-based graph initializer.

- Inspired by Wang et al. (2020a), we propose a flow GAT for the tripartite graph.




# [Locate and Label: A Two-stage Identifier for Nested Named Entity Recognition](https://aclanthology.org/2021.acl-long.216/)
- Inspired by these, we propose a two-stage entity identifier and treat NER as a joint task of boundary regression and span classification to address the weaknesses mentioned above.

- Our model outperforms the stateof-the-art models consistently on three nested NER datasets.

- Our main contributions are as follow: • Inspired by the two-stage detector popular in object detection, we propose a novel twostage identifier for NER of locating entities first and labeling them later.

- We denote them as span proposals and contextual spans, respectively.




# [REDDITBIAS: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models](https://aclanthology.org/2021.acl-long.151/)
- The contributions of this work are threefold: 1) we construct REDDITBIAS, a resource for multi-dimensional bias evaluation and mitigation dedicated to conversational AI.

- We presented REDDITBIAS, a comprehensive resource for bias evaluation and debiasing of conversational LMs.

- Consisting of manuallyannotated biased comments from Reddit, REDDIT-BIAS is the first real-world resource dedicated to multi-dimensional analysis (gender, race, religion, queerness) of biases in dialog models.

- In this work, we aim to close all these gaps by introducing REDDITBIAS, the first 'real-world' data set for measuring and mitigating biases in dialog models, together with an evaluation framework that couples bias measures with downstream evaluation on dialog tasks.




# [Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting](https://aclanthology.org/2021.acl-long.465/)
- We explored the task of difficulty-controllable question generation, with question difficulty redefined as the inference steps required to answer it.

- In summary, our contributions are as follows: • To the best of our knowledge, this is the first work of difficulty-controllable question generation, with question difficulty defined as the inference steps to answer it; • We propose a novel framework that achieves DCQG through step-by-step rewriting under the guidance of an extracted reasoning chain; • We build a dataset that can facilitate training of rewriting questions into more complex ones, paired with constructed context graphs and the underlying reasoning chain of the question.

- In this paper, we propose a highly-controllable QG framework that progressively increases difficulties of the generated questions through step-bystep rewriting.

- for transfer learning in question generation.




# [Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution](https://aclanthology.org/2021.acl-long.377/)
- In this work, we present invisible textual backdoors that are activated by a learnable combination of word substitution, in the hope of drawing attention to the security threats faced by NLP models.

- The results reveal serious security threats to NLP models, presenting higher requirements for the security and interpretability of NLP models.

- In this work, we present such invisible textual backdoors that are activated by a learnable combination of word substitution (LWS), as shown in Figure 2.

- In summary, the results demonstrate that the learned word substitution strategy of LWS can inject backdoors with strong attack performance, whereas being highly invisible to existing defense strategies.




# [KACE: Generating Knowledge-Aware Contrastive Explanations for Natural Language Inference](https://aclanthology.org/2021.acl-long.196/)
- The contributions of this paper are as follows: • We introduce a novel knowledge-aware contrastive explanation generation framework (KACE) for natural language inference tasks.

- Moreover, we train an NLI model enhanced with contrastive explanations and achieve the new stateof-art performance on SNLI.

- In this paper, we consider generating contrastive explanations in NLI.

- In this paper, we use knowledgeaware pre-trained language model to generate contrastive explanation.




# [LEXFIT: Lexical Fine-Tuning of Pretrained Language Models](https://aclanthology.org/2021.acl-long.410/)
- We proposed LEXFIT, a lexical fine-tuning procedure which transforms pretrained LMs such as BERT into effective decontextualized word encoders through dual-encoder architectures.

- Lexical Fine-Tuning Objectives.

- We can expose this knowledge by rewiring their parameters through lexical fine-tuning, and turn the LMs into universal (decontextualized) word encoders.

- Our hypothesis is that the pretrained LMs can be turned into effective static decontextualized word encoders via additional inexpensive lexical finetuning (i.e., LEXFIT-ing) on lexical pairs from an external resource.




# [GTM: A Generative Triple-Wise Model for Conversational Question Generation](https://aclanthology.org/2021.acl-long.271/)
- We propose a generative triple-wise model for generating appropriate questions in open-domain conversations, named GTM.

- There are one-to-many mappings in both PQ and QA pairs.

- GTM models the entire background in a triple and one-to-many mappings in PQ and QA pairs simultaneously with latent variables in three hierarchies.

- Third, higher distinct values illustrate that one-to-many mappings in PQ and QA pairs make the generated responses more diverse.




# [Prevent the Language Model from being Overconfident in Neural Machine Translation](https://aclanthology.org/2021.acl-long.268/)
- Accordingly, we propose a Margin-based Token-level Objective (MTO) to maximize the Margin.

- Then we propose Margin-based Token-level and Sentence-level objectives to maximize the Margin.

- Furthermore, to prevent the LM from being overconfident, we propose two effective optimization objectives to maximize the Margin, i.e., the Margin-based Token-level Objective (MTO) and the Margin-based Sentence-level Objective (MSO).

- Therefore, based on the MTO, we further propose a Margin-based Sentence-level Objective (MSO) by adding a dynamic weight function to alleviate the negative effect of these "dirty data".




# [Structural Pre-training for Dialogue Comprehension](https://aclanthology.org/2021.acl-long.399/)
- In this work, we present SPIDER (Structural Pre-traIned DialoguE Reader), a structural language modeling method to capture dialogue exclusive features.

- In this paper, we focus on the task-related adaptation of the pre-trained language models and propose SPIDER (Structural Pre-traIned DialoguE Reader), a structural language modeling method to capture dialogue exclusive features.

- To simulate the dialogue-like features, we propose two pre-training objectives in addition to the original LM objectives: 1) utterance order restoration, which predicts the order of the permuted utterances in dialogue context; 2) sentence backbone regularization, which regularizes the model to improve the factual correctness of summarized subject-verbobject triplets.

- Then, we will introduce our designed language modeling objectives for dialogue scenarios, including utterance order restoration (UOR) and sentence backbone regularization (SBR).




# [Reasoning over Entity-Action-Location Graph for Procedural Text Understanding](https://aclanthology.org/2021.acl-long.396/)
- Thus, it is highly valuable for both state and location tracking of entities.

- In this work, we propose a novel approach REAL for procedural text understanding.

- Finally, the prediction module leverages the graph-based representations to predict the state and location.

- We evaluate the proposed approach on two benchmark datasets for procedural text understanding, ProPara  and Recipes .




# [Cross-language Sentence Selection via Data Augmentation and Rationale Training](https://aclanthology.org/2021.acl-long.300/)
- We (i) propose a data augmentation and negative sampling scheme to create a synthetic training set of cross-lingual query-sentence pairs with binary relevance judgements, and (ii) demonstrate the effectiveness of a Supervised Embedding-based Cross-Lingual Relevance (SECLR) model trained on this data for low-resource sentence selection tasks on text and speech.

- We use a simple data augmentation and negative sampling scheme to generate a labeled dataset of relevant and irrelevant pairs of queries and sentences from these noisy parallel corpora.

- In this work, we presented a supervised crosslingual embedding-based query relevance model, SECLR, for cross-language sentence selection and also applied a rationale training objective to further increase model performance.

- Since this work targets cross-language sentence selection in a low-resource setting, we perform a training data ablation study to understand how training data size affects effectiveness.




# [Unified Dual-view Cognitive Model for Interpretable Claim Verification](https://aclanthology.org/2021.acl-long.5/)
- To weaken the bias of individual cognition view and strengthen the consistent shared evidence between global and local evidence, we project inconsistent loss to suppress the divergence.

- To address the deficiencies, we propose a unified Dual-view model based on Collective and Individual Cognition (CICD) for interpretable claim verification, which focuses on discovering global evidence and local key evidence, respectively, and then strengthens the consistent shared evidence between the both.

- In this paper, we proposed a unified dual-view model based on the perspectives of collective and individual cognition for interpretable claim verification, which constructed collective cognition view-based encoder-decoder module to generate global evidence and designed individual cognition view-based selected interaction module to explore local key evidence segments.

- To alleviate the bias of individual cognition-view evidence fragments and strengthen the consistent shared evidence between global and local evidence, we introduce an inconsistency loss to penalize the disagreement between the both evidence.




# [A Neural Transition-based Joint Model for Disease Named Entity Recognition and Normalization](https://aclanthology.org/2021.acl-long.219/)
- We transform the end-to-end disease recognition and normalization task as an action sequence prediction task.

- We define the end-to-end disease recognition and normalization task as follows.

- In this work, we propose a neural transition-based joint model for disease named entity recognition and normalization.

- Based on the introduced transition system, the endto-end disease recognition and normalization task becomes a new sequence to sequence task, i.e., the action sequence prediction task.




# [Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering](https://aclanthology.org/2021.acl-long.481/)
- First, we propose Motion-Appearance Synergistic Networks (MASN) for video question answering based on three modules, the motion module, the appearance module, and the motionappearance fusion module.

- In this paper, we proposed a Motion-Appearance Synergistic Networks to fuse and create a synergy between motion and appearance features.

- Finally, the Motion-Appearance Fusion module modulates the amount of motion and appearance information utilized and integrates them based on question context.

- Synergistic Networks (MASN) for video question answering which consist of three kinds of modules: the motion module, the appearance module, and the motion-appearance fusion module.




# [XLPT-AMR: Cross-Lingual Pre-Training via Multi-Task Learning for Zero-Shot AMR Parsing and Text Generation](https://aclanthology.org/2021.acl-long.73/)
- • We propose an effective cross-lingual pretraining approach for zero-shot AMR parsing and AMR-to-text generation.

- Performance of English AMR parsing and AMR-to-Text generation.

- In this paper we proposed a cross-lingual pretraining approach via multi-task learning for zeroshot AMR parsing and AMR-to-text generation.

- Then 899 on the trilingual parallel dataset, we propose crosslingual pre-training via multi-task learning.




# [MultiMET: A Multimodal Dataset for Metaphor Understanding](https://aclanthology.org/2021.acl-long.249/)
- It also offers a set of baseline results of various tasks and shows the importance of combining multimodal cues for metaphor understanding.

- To overcome the above limitations, we propose a novel multimodal metaphor dataset (MultiMET) consisting of text-image pairs (text and its corresponding image counterparts) manually annotated for metaphor understanding.

- This paper presents the creation of a novel resource, a large-scale multimodal metaphor dataset, MultiMET, with manual fine-gained annotation for metaphor understanding and research.

- • We propose three tasks to evaluate finegrained multimodal metaphor understanding abilities, including metaphor detection, sentiment analysis, and intent detection in multimodal metaphor.




# [Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators](https://aclanthology.org/2021.acl-long.418/)
- First, we introduce a new fine-tuning strategy that only focuses on the parameters of auxiliary tensors, so the number of fine-tuning parameters can be largely reduced.

- Inspired by this, we designed a novel fine-tuning strategy that only needs to finetune the parameters in auxiliary tensors.

- If this could be achieved, we can derive a lighter network meanwhile reduce the parameters to be fine-tuned.

- So far, most of pre-trained language models (PLM) are developed based on stacked Transformer layers (Vaswani et al., 2017).




# [Data Augmentation with Adversarial Training for Cross-Lingual NLI](https://aclanthology.org/2021.acl-long.401/)
- Data Augmentation.

- In this paper, we propose a novel data augmentation scheme to synthesize controllable and much less noisy data for cross-lingual NLI.

- Cross-lingual Inference Classification.

- To address this, this paper proposes a novel data augmentation strategy with label rectification to build synthetic examples, outperforming even models trained with larger amounts of ground-truth data.




# [Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical Core-Fringe Approach](https://aclanthology.org/2021.acl-long.282/)
- Hierarchical Positive-Unlabeled Learning.

- Second, to leverage the graph and support fine-grained domains without relying on domain-specific corpora, we propose hierarchical core-fringe learning, which learns the domain relevance of core and fringe terms jointly in a semi-supervised manner contextualized in the hierarchy of the domain.

- In this section, we aim to design learning methods to learn the fine-grained domain relevance of core and fringe terms jointly.

- In this way, core and fringe terms help each other, and the domain relevance is learned jointly.




# [Importance-based Neuron Allocation for Multilingual Neural Machine Translation](https://aclanthology.org/2021.acl-long.445/)
- These language-specific neurons are important for preserving the language-specific knowledge.

- Based on this, we divide them into general and language-specific ones and make general neurons participate in the translation of all the languages while language-specific neurons focus on some specific languages.

- The experiments prove that our method can get superior translation results with better general and language-specific knowledge.

- As shown in Table 3, when ρ = 90% the model gets the best translation result and reach best trade-off between general and language-specific neurons.




# [Engage the Public: Poll Question Generation for Social Media Posts](https://aclanthology.org/2021.acl-long.3/)
- We have presented a novel task to generate social media poll questions.

- The topic representations are then incorporated into a sequence-to-sequence (S2S) architecture to decode poll questions word by word.

- Human evaluation further demonstrates our models' capability to generate poll questions relevant to the source post, fluent in language, and particularly engaging to draw user attentions for discussions.

- Extensive experiments on a large-scale dataset newly collected from Weibo have demonstrated the effectiveness of our proposed model.




# [Probing Toxic Content in Large Pre-Trained Language Models](https://aclanthology.org/2021.acl-long.329/)
- In this paper, we present a methodology to probe toxic content in pre-trained language models using commonsense patterns.

- • We perform a large-scale extensible study on toxic content in PTLMs without relying on datasets which are specific to such a task.

- • We develop a large dataset based on structured patterns that can later be used for the evaluation of toxic language classification and harmful content within PTLMs.

- Finally, we compare counts of potentially incoherent associations produced by various PTLMs in English, French and Arabic.




# [Multi-View Cross-Lingual Structured Prediction with Minimum Supervision](https://aclanthology.org/2021.acl-long.207/)
- We propose a novel multi-view framework to selectively transfer knowledge from multiple sources by utilizing a small amount of labeled dataset.

- The contributions of this work are: 1. We propose to leverage a small number of target labeled data to better aggregate multiple source models.

- We propose a novel multi-view framework to achieve a good trade-off between the two views.

- Why the Multi-View Framework Works?




# [Automated Generation of Storytelling Vocabulary from Photographs for Use in AAC](https://aclanthology.org/2021.acl-long.108/)
- Figure 1: An AAC app design demonstrating how context-related vocabulary generated by our method might be presented for use in subsequent conversations.

- The similar performance across photos with different levels of contextual information (RQ2) suggests that our method is robust to variations in the input photograph.

- The design space for generating AAC storytelling vocabulary directly from photographs is vast and under explored.

- 2. How does the level of contextual information in the input photo affect performance?




# [Towards Propagation Uncertainty: Edge-enhanced Bayesian Graph Convolutional Networks for Rumor Detection](https://aclanthology.org/2021.acl-long.297/)
- • We design a new edge-wise consistency training framework to optimize the model with unlabeled latent relations.

- EBGCN significantly outperforms baselines on both rumor detection and early rumor detection tasks.

- • Experiments on three real-world benchmark datasets demonstrate the effectiveness of our model on both rumor detection and early rumor detection tasks 1 .

- Besides, we design an edge-wise consistency training framework incorporating unsupervised relation learning to enforce the consistency on latent relations.




# [Mid-Air Hand Gestures for Post-Editing of Machine Translation](https://aclanthology.org/2021.acl-long.527/)
- In addition, the gesture pointing (where a participant points with the index finger to place the cursor on the item) was highly preferred for single item selection.

- Overall, we hope that future research will pick up the insights from the first and second study and help advance the state-of-the-art in PE.

- Here, we present the first study that investigates the usefulness of mid-air hand gestures in combination with the keyboard (GK) for text editing in PE of MT.

- Note that in contrast to the mouse, the group selection using both index fingers allows the user to manipulate both ends of the selection continuously instead of having one side fixed.




# [A Sweet Rabbit Hole by DARCY: Using Honeypots to Detect Universal Trigger's Adversarial Attacks](https://aclanthology.org/2021.acl-long.296/)
- This paper proposes DARCY, an algorithm that greedily injects multiple trapdoors, i.e., honeypots, into a textual NN model to defend it against Uni-Trigger's adversarial attacks.

- • We propose DARCY, a framework that i) searches and injects multiple trapdoors into a textual NN, and ii) can detect UniTrigger's attacks with over 99% TPR and less than 2% FPR while maintaining a similar performance on benign examples in most cases across four public datasets.

- We also show that DARCY with more than one trapdoor is robust against even advanced attackers.

- Robustness to Varying Attacks.




# [Lower Perplexity is Not Always Human-Like](https://aclanthology.org/2021.acl-long.405/)
- We found that syntactic category is the most influential factor for modeling gaze duration, at least in this experiment.

- (2) The effect of surprisals for modeling human reading behavior was calculated using a linear mixedeffects regression

- Notably, surprisal was effective for gaze duration modeling in all the Japanese LMs.

- It is hoped that this study motivates the creation of a large-scale corpus of human reading behaviors in diverse languages.




# [Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering](https://aclanthology.org/2021.acl-long.564/)
- Active Learning.

- Active Learning with Global Reasoning.

- Our work tests the utility of multiple recent active learning methods on the open-ended understanding task of VQA.

- This paper asks a simple question -why does the modern neural active learning toolkit fail when applied to complex, open ended tasks?




# [Evaluation of Thematic Coherence in Microblogs](https://aclanthology.org/2021.acl-long.530/)
- The main contributions of this paper are: • We define the task of assessing thematic coherence in microblogs and use it as the basis for creating microblog clusters (Sec. 3).

- The aim of our work is to identify reliable metrics for measuring thematic coherence in clusters of microblog posts.

- • We provide guidelines for the annotation of thematic coherence in microblog clusters and construct a dataset of clusters annotated for thematic coherence spanning two different domains (political tweets and COVID-19 related tweets).

- Here we present (a) the creation of a corpus of topic clusters of tweets C and (b) the annotation process for thematic coherence.




# [Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers](https://aclanthology.org/2021.acl-long.566/)
- In this paper, we propose the first large-scale meta-evaluation of MT in which we manually annotated 769 research papers published from 2010 to 2020.

- The accumulation of these pitfalls and the concerning trends we observed lead us to propose a guideline for automatic MT evaluation.

- After demonstrating how the accumulation of these pitfalls leads to dubious evaluation, we propose a general guideline for automatic evaluation in MT and a simple scoring method to meta-evaluate an MT paper.

- We believe that a clear, simple, and well-promoted guideline must be defined for automatic MT evaluation.




# [Rewriter-Evaluator Architecture for Neural Machine Translation](https://aclanthology.org/2021.acl-long.443/)
- We present prioritized gradient descent (PGD) to train the proposed architecture.

- To address this problem, we introduce a novel architecture, Rewriter-Evaluator.

- We also propose prioritized gradient descent (PGD) that facilitates training the rewriter and the evaluator both jointly and efficiently.

- Firstly, Rewriter-Evaluator significantly improves the translation quality of NMT models.




# [On Compositional Generalization of Neural Machine Translation](https://aclanthology.org/2021.acl-long.368/)
- Compositional Generalization.

- In this paper, we study compositional generalization in the context of machine translation.

- Our work is in line but we discuss robustness from the perspective of compositional generalization.

- In contrast to these studies, we quantitatively measure compositionality of NMT under compound translation error rate.




# [Question Answering Over Temporal Knowledge Graphs](https://aclanthology.org/2021.acl-long.520/)
- In this paper we propose CRONQUESTIONS, a new dataset for Temporal KGQA.

- We also propose a new method, CRONKGQA, that is able to leverage Temporal KG Embeddings to perform TKGQA.

- Temporal Knowledge Graphs (Temporal KGs) are multi-relational graph where each edge is associated with a time duration.

- In response, we propose CRONKGQA, an enhancement of EmbedKGQA, which outperforms baselines across all question types.




# [KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers](https://aclanthology.org/2021.acl-long.176/)
- KaggleDBQA provides two resources to facilitate real-world applications of text-to-SQL parsing.

- In this paper, we propose a few-shot evaluation to inspire future research of practical text-to-SQL parsers.

- Each database has associated plain-text documentation that can assist text-to-SQL parsing.

- We encourage adopting this regime for established text-to-SQL benchmarks.




# [InfoSurgeon: Cross-Media Fine-grained Information Consistency Checking for Fake News Detection](https://aclanthology.org/2021.acl-long.133/)
- In this paper, we propose a new task: finegrained, knowledge element-level cross-media information consistency checking.

- Fake News Detection.

- 2019. Learning hierarchical discourse-level structure for fake news detection.

- 3 Fake News Detection




# [Modularized Interaction Network for Named Entity Recognition](https://aclanthology.org/2021.acl-long.17/)
- In this paper, we have proposed a novel Modularized Interaction Network (MIN) model for the NER task.

- The proposed MIN model utilizes both segment-level information and word-level dependencies, and incorporates an interaction mechanism to support information sharing between boundary detection and type prediction to enhance the performance for the NER task.

- In summary, the main contributions of this paper include: • We propose a novel Modularized Interaction Network (MIN) model which utilizes both the segment-level information from segmentbased models and word-level dependencies from sequence labeling-based models in order to enhance the performance of the NER task.

- In this paper, we propose a Modularized Interaction Network (MIN) model which consists of the NER Module, Boundary Module, Type Module and Interaction Mechanism for the NER task.




# [Annotating Online Misogyny](https://aclanthology.org/2021.acl-long.247/)
- In this work, we have documented the construction of a dataset for training systems for automatic detection of online misogyny.

- 2. Model: We present a taxonomy and annotation codebook grounded in previous research on automatic detection of misogyny as well as social science terminology.

- 3. Dataset: We present a new, annotated corpus of Danish social media posts, Bajer, 1 annotated for misogyny, including analysis of class balance, word frequencies, Inter-Annotator Agreement (IAA), annotation errors, and classification baseline.

- This paper investigates the research question: How might we design a comprehensive annotation process which results in high quality data for automatically detecting misogyny?




# [Structurizing Misinformation Stories via Rationalizing Fact-Checks](https://aclanthology.org/2021.acl-long.51/)
- We identify ten types of misinformation stories, a preview of which are shown in Figure 1.

- With a large corpus of fact-checks, these phrases would accumulate and reveal prevalent types of misinformation stories.

- In this paper, we identify ten prevalent misinformation types with rationalized models on fact-checks and analyze their evolution over the last ten years and between notable events.

- Structure of misinformation stories.




# [COVID-Fact: Fact Extraction and Verification of Real-World Claims on COVID-19 Pandemic](https://aclanthology.org/2021.acl-long.165/)
- We release a dataset of 4,086 claims concerning the COVID-19 pandemic, together with supporting and refuting evidence.

- We propose a novel semi-automatic method to build a fact-checking dataset for COVID-19 (COVID-Fact) with the goal of facilitating all the above tasks.

- Usefulness of COVID-Fact for Zero-Shot Scientific Fact-checking.

- We provide a detailed evaluation of the COVID-Fact task and hope that our dataset serves as a challenging testbed for end-to-end fact-checking around COVID-19.




# [CCMatrix: Mining Billions of High-Quality Parallel Sentences on the Web](https://aclanthology.org/2021.acl-long.507/)
- Our contributions are: • development of a new highly efficient and parallelized processing pipeline to confront the substantial computational challenge; • unprecedented size: 10.8 billion mined parallel sentences in 90 different languages; • all these resources are freely available; • we demonstrate the quality of our mined data on a variety of machine translation benchmarks, such as TED, WMT, and WAT, achieving highly competitive results.

- We show that margin-based mining in a joint multilingual sentence embedding space can be scaled to monolingual texts of more than 71 billion unique sentences in 90 languages, including several low resource languages.

- We leverage massively multilingual sentence embeddings and a margin-based criterion to mine parallel sentences.

- Therefore, we use margin-based mining (Artetxe and Schwenk, 2018a).




# [TGEA: An Error-Annotated Dataset and Benchmark Tasks for Text Generation from Pretrained Language Models](https://aclanthology.org/2021.acl-long.469/)
- • TGEA, to the best of our knowledge, is the first dataset built on machine-generated texts from state-of-the-art pretrained language models with rich annotations.

- we use the state-of-the-art BERT-GEC model (Kaneko et al., 2020) as the baseline for this task, which is an encoder-decoder model using representations learned by PLMs as additional inputs.

- (2) Erroneous and associated span detection.

- We also experiment zero-shot generation on the test set.




# [SemFace: Pre-training Encoder and Decoder with a Semantic Interface for Neural Machine Translation](https://aclanthology.org/2021.acl-long.348/)
- We propose two types of semantic interfaces, namely CL-SemFace and VQ-SemFace.

- The key point is to use a semantic interface to connect the pre-trained encoder and decoder.

- We propose SemFace, a better pre-training method for neural machine translation.

- The semantic interface acts as a bridge to connect the encoder and decoder during pre-training.




# [Mitigating Bias in Session-based Cyberbullying Detection: A Non-Compromising Approach](https://aclanthology.org/2021.acl-long.168/)
- To address these challenges, we propose a context-aware and model-agnostic debiasing training framework for cyberbullying detection.

- In this work, we examined unintended biases in datasets for session-based cyberbullying detection.

- This paper aims to mitigate the unintended bias in cyberbullying detection in social media sessions.

- To alleviate these unintended biases, we propose an effective debiasing strategy by leveraging techniques in RL.




# [Continuous Language Generative Flow](https://aclanthology.org/2021.acl-long.355/)
- We have proposed a language generative flow model with non-autoregressive and autoregressive variants.

- In addition to improving language generation quality, we also use the proposed autoregressive flow model for data augmentation.

- Then, following it, we propose two variants of our continuous language generative flow model.

- Overall, we have two contributions: (1) we propose two continuous language generative flow model variants that have better density estimation abilities than an LSTM baseline model, and can perform non-autoregressive and autoregressive generation respectively; (2) Our language flow model largely improves QG, NMT, and data augmentation for QA tasks.




# [CoSQA: 20,000+ Web Queries for Code Search and Question Answering](https://aclanthology.org/2021.acl-long.442/)
- We also propose a novel code contrastive learning method, named CoCLR, to incorporate artificially generated instances into training.

- Furthermore, to better leverage the CoSQA dataset for querycode matching, we propose a code contrastive learning method (CoCLR) to produce more artificially generated instances for training.

- (2) By integrating the code contrastive learning method, siamese network with CodeBERT further achieves significant performance gain on both tasks.

- We demonstrate that CoSQA is an ideal dataset for code question answering and code search.




# [Hate Speech Detection based on Sentiment Knowledge Sharing](https://aclanthology.org/2021.acl-long.556/)
- To overcome the weaknesses of previous works, we propose a hate speech detection framework based on sentiment knowledge sharing (SKS) 1 .

- SKS is our proposed model which detects hate speech based on sentiment knowledge sharing.

- Hate speech detection and sentiment analysis are highly correlated, so that sentiment knowledge sharing can improve the performance of hate speech detection.

- We show that sentiment knowledge sharing improves system performance over the baselines and advances hate speech detection.




# [Cascaded Head-colliding Attention](https://aclanthology.org/2021.acl-long.45/)
- CODA explicit models of the interactions among attention heads through a hierarchical variational distribution.

- We propose cascaded head-colliding attention (CODA, Figure 1b).

- (b) Right: PGM diagram of a 3-layer cascaded head-colliding attention (CODA).

- Vanilla vs. head-colliding attention




# [Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation](https://aclanthology.org/2021.acl-long.387/)
- MedWriter introduces a novel hierarchical retrieval mechanism working with a hierarchical language decoder to automatically learn the dynamic report and sentence templates from the data for generating accurate and professional medical reports.

- On top of the retrieval modules, we design a new multi-query attention mechanism to fuse the retrieved information for medical report generation.

- To sum up, our contributions are: • To the best of our knowledge, we are the first to model the memory retrieval mechanism in both report and sentence levels.

- HRGR-Agent (Li et al., 2018) incorporates retrieved sentences in a reinforcement learning framework for medical report generation.




# [Introducing Orthogonal Constraint in Structural Probes](https://aclanthology.org/2021.acl-long.36/)
- Orthogonal Structural Probes are less vulnerable to memorization.

- We have expanded structural probing to new types of auxiliary tasks and introduced a new setting, Orthogonal Structural Probe, in which probes can be optimized jointly.

- We introduce orthogonality to structural probes.

- Our Orthogonal Structural Probes are trained jointly for multiple objectives (Section 3.3).




# [Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning](https://aclanthology.org/2021.acl-long.568/)
- Finally, we connect intrinsic dimensional with low dimensional task representations and compression-based generalization bounds to provide intrinsic-dimension-based generalization bounds independent of the full parameter count, further justifying why these methods generalize so well in practice across tasks.

- • Lastly, we show that compression based generalization bounds can be applied to our intrinsic dimension framework to provide generalization bounds for large pre-trained models independent of the pre-trained model parameter count.

- A lower intrinsic dimension is strongly correlated with better evaluation performance.

- In conclusion, we proposed viewing the various phenomena surrounding fine-tuning and pretraining through the lens of intrinsic dimensionality.




# [Evidence-based Factual Error Correction](https://aclanthology.org/2021.acl-long.256/)
- In this paper, we propose Factual Error Correction, as an explainable alternative for fact verification.

- A challenge for factual error correction is the lack of datasets consisting of claims paired with their corrections.

- Exposing the evidence source and 1 https://github.com/j6mes/ 2021-acl-factual-error-correction

- Information Retrieval Figure 1: Factual Error Correction uses evidence to make corrections to claims, in contrast to fact verification, which instead classifies the veracity of the claim.




# [Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy](https://aclanthology.org/2021.acl-long.281/)
- This paper has presented a systematic study of word meaning representation in context.

- We have created a new resource to investigate how vector models represent word meanings in context.

- We use this resource to perform a systematic evaluation of contextualized word meaning representations.

- The results suggest that the best monolingual models based on Transformers (Vaswani et al., 2017) can identify homonyms having different meanings adequately.




# [Learning Language Specific Sub-network for Multilingual Machine Translation](https://aclanthology.org/2021.acl-long.25/)
- • LaSS shows its strong generalization performance at easy adaptation to new language pairs and zero-shot translation.

- Besides, LaSS can boost zero-shot translation by up to 26.5 BLEU.

- Secondly, we show that LaSS can also boost performance in zero-shot translation scenario, obtaining performance gains by up to 26.5 BLEU.

- In this paper, we propose to learn Language-Specific Sub-network (LaSS) for multilingual NMT.




# [Search from History and Reason for Future: Two-stage Reasoning on Temporal Knowledge Graphs](https://aclanthology.org/2021.acl-long.365/)
- Thus, we propose a new model called CluSTeR, consisting of two stages, Clue Searching (Stage 1) and Temporal Reasoning (Stage 2).

- In general, this paper makes the following contributions: • We formulate the TKG reasoning task from the view of human cognition and propose a two-stage model, CluSTeR, which is mainly composed of a RL-based clue searching stage and a GCN-based temporal reasoning stage.

- As illustrated in Figure 2, the model consists of two stages, clue searching and temporal reasoning.

-   for the temporal reasoning task addressed in this paper.




# [Span-based Semantic Parsing for Compositional Generalization](https://aclanthology.org/2021.acl-long.74/)
- We now present our experimental evaluation, which demonstrates the advantage of span-based parsing for compositional generalization.

- We define span-based semantic parsing as follows.

- In this work, we show that our span-based parser, SPANBASEDSP, that precisely describes how meaning is composed over the input utterance leads to dramatic improvements in compositional generalization.

- Our span-based parser assumes composition can only be done for adjacent spans that form together a contiguous span.




# [Multi-perspective Coherent Reasoning for Helpfulness Prediction of Multimodal Reviews](https://aclanthology.org/2021.acl-long.461/)
- We design a novel Multi-perspective Coherent Reasoning method (denoted as MCR) to tackle the MRHP task.

- Concretely, we propose a product-review coherent reasoning module to effectively capture the intra-and inter-modal coherence between the target product and the review.

- In this paper, we propose a product-review coherent reasoning module to effectively capture the intra-and inter-modal coherence between the target product and the review.

- We propose a multi-perspective coherent reasoning (MCR) method to solve MRHP task, which fully explores the product-review coherence and intra-review coherence from both textual and visual modalities.




# [xMoCo: Cross Momentum Contrastive Learning for Open-Domain Question Answering](https://aclanthology.org/2021.acl-long.477/)
- To solve the problems mentioned above, we propose a new momentum contrastive learning method, for passages.

- To solve this problem, we propose a new contrastive learning method called Cross Momentum Contrastive Learning (xMoCo).

- In this paper, we propose cross momentum contrastive learning (xMoCo), for passage retrieval task in open domain QA.

- xMoCo jointly optimizes question-to-passage and passage-to-question matching, enabling using separate encoders for questions and passages, while efficiently maintains a large pool of negative samples like the original MoCo.




# [Modeling Fine-Grained Entity Types with Box Embeddings](https://aclanthology.org/2021.acl-long.160/)
- In this paper, we investigated a box-based model for fine-grained entity typing.

- Our box-based model outperforms the vector-based model on two benchmarks, Ultra-fine Entity Typing and OntoNotes, achieving state-ofthe-art-performance.

- Our box-based model achieves better accuracy compared to the vector-based model on all supertypes.

- Even in those tricky cases, the box-based model shows reasonable performance.




# [DESCGEN: A Distantly Supervised Dataset for Generating Abstractive Entity Descriptions](https://aclanthology.org/2021.acl-long.35/)
- In summary, our contributions include: • We propose a new dataset DESCGEN that includes challenging, abstractive entity summaries.

- DESCGEN contains about 37K entity descriptions from Wikipedia and Fandom.

- In summary, our analysis suggests there is room for improvement in extractive content selection and abstractive generation, particularly for new and emerging entities from less popular domains.

- DESCGEN contains 37K entity descriptions extracted from Wikipedia and Fandom 2 .




# [Structured Sentiment Analysis as Dependency Graph Parsing](https://aclanthology.org/2021.acl-long.263/)
- Moreover, we cast sentiment analysis as a dependency graph parsing problem, where the sentiment expression is the root node, and the other elements have arcs which model the relationships between them.

- In this paper, we have proposed a dependency graph parsing approach to structured sentiment analysis and shown that these models outperform state-of-the-art sequence labeling models on five benchmark datasets.

- As such, we propose a unified approach to structured sentiment which jointly predicts all elements of an opinion tuple and their relations.

- We further propose methods to inject linguistic structure into the sentiment graphs using syntactic dependencies.




# [WARP: Word-level Adversarial ReProgramming](https://aclanthology.org/2021.acl-long.381/)
- The method outperforms existing methods with significantly more trainable parameters on GLUE benchmark tasks and shows an impressive performance in a few-shot setting on two SuperGLUE tasks.

- The approach based on frozen features does not require storing task-specific language models.

- Every input sentence can be concatenated with task-specific pretrained prompts in advance.

- We show that our method, using up to 25K trainable parameters per task, achieves 81.6 test score on the GLUE Leaderboard, outperforming all the other submissions that use up to three orders of magnitude more trainable parameters.




# [RADDLE: An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems](https://aclanthology.org/2021.acl-long.341/)
- We introduce RADDLE, a platform and collection of resources for evaluating and analyzing taskoriented dialog systems.

- An adversarially robust model is proposed to improve the generalization ability in noisy environments.

- We confirm (1) the utility of grounded pre-training and transfer learning methods in dialog systems: pre-training improves generalization in a limited data setting, and (2) adversarial training improves robustness, but still leaves room for improvement.

- To better understand the challenges posed by RADDLE, we conduct experiments with simple baselines and state-of-the-art task-oriented dialog models.




# [ERNIE-DOC: A Retrospective Long-Document Modeling Transformer](https://aclanthology.org/2021.acl-long.227/)
- In this paper, we proposed ERNIE-DOC, a document-level language pretraining model based on the Recurrence Transformers paradigm.

- Additionally, ERNIE-DOC is pretrained with a document-aware segment-reordering objective to explicitly learn the relationship among segments of a long context.

- Moreover, we introduce a segment-reordering objective to pretrain a document-level model.

- To address this limitation, we propose ERNIE-DOC (A Retrospective Long-Document Modeling Transformer) based on the Recurrence Transformer paradigm.




# [TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling](https://aclanthology.org/2021.acl-long.293/)
- Our main contributions are to: (1) present a new, flexible approach to few-shot style transfer, (2) use sentence adjacency as a means for inducing text style representations, (3) reframe style transfer as "targeted restyling" directional operations in style space, (4) introduce "tunable inference" for finergrained control of transfers, (5) show the effectiveness of "noisy" back-translation training, and (6) illustrate few-shot generalization to a range of style attributes including dialect, emotiveness, formality, politeness, and sentiment.

- Supervised style transfer has seen limited research due to the difficulty of obtaining parallel data.

- We have presented a unique approach to few-shot text style transfer that is competitive with systems trained with labels (an easier setting), while allowing control of how much of the input is changed.

- As mentioned at the outset, recent work on text style transfer falls into three classes: supervised, "unsupervised", and few-shot.




# [BanditMTL: Bandit-based Multi-task Learning for Text Classification](https://aclanthology.org/2021.acl-long.428/)
- BanditMTL is proposed based on linear adversarial multi-armed bandit and implemented with a mirror gradient ascent-descent algorithm.

- In this paper, we propose a task-varianceregularized multi-task learning algorithm based on mirror gradient ascent-descent, dubbed Ban-ditMTL.

- In this paper, the task-variance-regularized multitask learning is formulated as a linear adversarial multi-armed bandit problem.

- Our proposed approach can improve the performance of multi-task text classification.




# [A Cognitive Regularizer for Language Modeling](https://aclanthology.org/2021.acl-long.404/)
- Evidence for the UID hypothesis.

- In this paper, we propose a new experimental paradigm that uses modern-day NLP models to test the UID hypothesis.

- Moreover, observing lower perplexity in language models trained with this regularization would imply that the concept of UID is a good inductive bias for language modeling, thereby providing a new type of evidence for the UID hypothesis at scale.

- One such locus of optimization is outlined by the Uniform Information Density (UID) hypothesis.




# [Weight Distillation: Transferring the Knowledge in Neural Network Parameters](https://aclanthology.org/2021.acl-long.162/)
- In this work, we propose weight distillation to transfer knowledge in the parameters of the teacher network to the student network.

- Our experiments on three machine translation tasks show that weight distillation consistently outperforms knowledge distillation by producing a faster and better student network.

- It transfers the knowledge in parameters of the teacher network to the student network via a parameter generator.

- Our weight distillation, on the other hand, explores a new source of knowledge and a new way to leverage this knowledge.




# [ExCAR: Event Graph Knowledge Enhanced Explainable Causal Reasoning](https://aclanthology.org/2021.acl-long.183/)
- To fully exploit the potential of the evidence information, we present an Event graph knowledge enhanced explainable CAusal Reasoning (ExCAR) framework.

- We devise a novel explainable causal reasoning framework ExCAR.

- Then ExCAR conducts causal reasoning based on the logical rules using a Conditional Markov Neural Logic Network.

- To learn the conditional probabilistic of logical rules, we propose a conditional Markov neural logic network that combines the strengths of rulebased and neural models.




# [ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences](https://aclanthology.org/2021.acl-long.303/)
- Atomic clauses are fundamental text units for understanding complex sentences.

- We presented a new task to decompose complex sentences into simple ones, along with DeSSE, a new dataset designed for this task.

- We formulate the problem of converting complex sentences into covering sets of simple sentences as a graph segmentation problem.

- We proposed the neural ABCD model to predict four edits operations on sentence graphs, as part of a larger pipeline from our graph-edit problem formulation.




# [End-to-End AMR Coreference Resolution](https://aclanthology.org/2021.acl-long.324/)
- We investigated a novel end-to-end multi-sentence AMR coreference resolution model using a graph neural network.

- We propose an AMR coreference resolution model by extending an end-to-end text-based coreference resolution model (Lee et al., 2017).

- We consider coreference resolution as the prerequisite for creating multi-sentence AMRs, proposing the first end-to-end model for this task.

- This verifies the effectiveness of the end-to-end framework.




# [Unsupervised Neural Machine Translation for Low-Resource Domains via Meta-Learning](https://aclanthology.org/2021.acl-long.225/)
- We utilize the metalearning approach to address a low-resource challenge for unsupervised machine translation.

- Overall, our contributions can be summarized as follows: • We apply a meta-learning approach for UNMT.

- Transferring the knowledge from high-resource domains to a low-resource domain

- Unsupervised neural machine translation (UNMT) leverages unpaired monolingual corpora for its training, without requiring an already labeled, parallel corpus.




# [OoMMix: Out-of-manifold Regularization in Contextual Embedding Space for Text Classification](https://aclanthology.org/2021.acl-long.49/)
- In this work, we propose a novel approach to discovering and leveraging the out-of-manifold for contextual embedding regularization.

- In the end, the fine-tuning on the synthesized out-of-manifold embeddings tightly regularizes the contextual embedding space of BERT.

- In this section, we propose a novel mixup approach, termed as OoMMix, to regularize the outof-manifold in contextual embedding space for text classification.

- In conclusion, discovering the out-of-manifold and applying mixup for such subspace are beneficial in contextual embedding space.




# [Language Model Evaluation Beyond Perplexity](https://aclanthology.org/2021.acl-long.414/)
- That is, we pose the question: Do neural language models exhibit the statistical tendencies of human language?

- In this work, we present a framework for determining the linguistic properties learned by language models through analysis of statistical trends in generated text.

- We find that neural language models accurately capture only a subset of natural language distributions and that this subset is highly dependent on both model architecture and generation strategy; no one configuration stands out as capturing all linguistic distributions.

- Language models are probability distributions over natural language sentences.




# [StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling](https://aclanthology.org/2021.acl-long.559/)
- We evaluate the proposed model on three tasks: Masked Language Modeling, Unsupervised Constituency Parsing and Unsupervised Dependency Parsing.

- Incorporating the new parsing mechanism, the dependency-constrained self-attention, and the Transformer architecture, we introduce a new model named StructFormer.

- Based on the framework, we propose StructFormer, a new unsupervised parsing algorithm that does unsupervised dependency and constituency parsing at the same time.

- Unsupervised constituency parsing has recently received more attention.




# [Vocabulary Learning via Optimal Transport for Neural Machine Translation](https://aclanthology.org/2021.acl-long.571/)
- To address the above problems, we propose a VOcabulary Learning approach via optimal Transport, VOLT for short.

- Thus, trial training is required to find the optimal size, which brings high computation costs.

- In this paper, we propose to explore automatic vocabularization by simultaneously considering entropy and vocabulary size without expensive trial training.

- On English-German translation, VOLT only takes 30 GPU hours to find vocabularies, while the traditional BPE-Search solution takes 384 GPU hours.




# [Unsupervised Extractive Summarization-Based Representations for Accurate and Explainable Collaborative Filtering](https://aclanthology.org/2021.acl-long.232/)
- Nevertheless, to the best of our knowledge, there does not appear to be a comprehensive set of criteria that assesses the real-life explainability of explanations.

- The strength of ESCOFILT lies in the fact that it uniquely unifies representation and explanation.

- Therefore, in this paper, we propose the first extractive summarization-based collaborative filtering model, ESCOFILT.

- Our proposed summary-level explanation closely resembles real-life explanations, wherein the explanation text is derived from multiple reviews.




# [N -ary Constituent Tree Parsing with Recursive Semi-Markov Model](https://aclanthology.org/2021.acl-long.205/)
- (1) We propose a novel graph-based framework, recursive semi-Markov model, for n-ary constituent tree parsing, which can model the dependencies of sibling nodes.

- 3 The Recursive Semi-Markov Model

- In this paper, a recursive semi-Markov model is proposed for n-ary constituent tree parsing, with the advantage of modeling the sibling relations within n-ary node.

- The whole framework is a 1-order semi-Markov model.




# [Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks](https://aclanthology.org/2021.acl-long.28/)
- We propose a Multi-channel Graph Neural Networks model with Sentiment-awareness (MGNNS) for multimodal sentiment analysis that consists of three stages.

- We focus on multimodal sentiment detection for image-text pairs in social media posts.

- Our main contributions are summarized as follows: • We propose a novel MGNNS framework that models the global characteristics of the dataset to handle the multimodal sentiment detection task.

- As far as we know, this is the first application of graph neural networks in image-text multimodal sentiment analysis.




# [Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech](https://aclanthology.org/2021.acl-long.250/)
- Hybrid models for data collection.

- In this paper we presented a novel HITL methodology for data collection based on an author-reviewer framework.

- To the best of our knowledge, this is the first multi-target expert-based HS/CN dataset constructed through a semi-automatic mechanism and can be downloaded at the following link: https://github.com/marcoguerini/CONAN.

- This data collection session lasted roughly one month.




# [CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction](https://aclanthology.org/2021.acl-long.483/)
- • We propose a novel contrastive instance learning method to boost the DSRE model performances under the MIL framework.

- Thus, we propose a contrastive instance learning method CIL to boost the MIL model performances.

- In this paper, we go beyond typical MIL framework and propose a novel Contrastive Instance Learning (CIL) framework.

- Accordingly, the major contributions of this paper are summarized as follows: • We discuss the long-standing MIL framework and point out that it can not effectively utilize abundant instances inside MIL bags.




# [Towards Quantifiable Dialogue Coherence Evaluation](https://aclanthology.org/2021.acl-long.211/)
- To address the above limitations, we propose a novel dialogue coherence metric training framework, named as Quantifiable Dialogue Coherence Evaluation (QuantiDCE).

- To summarize our contributions: 1) We propose QuantiDCE, a novel quantifiable training framework for dialogue coherence evaluation, which aims to align the automatic scores with the actual human rating standards via MLR pre-training and KD fine-tuning.

- At the MLR pre-training stage, a new multilevel ranking (MLR) loss is proposed for learning the coarse judgement of coherence degrees.

- To the best of our knowledge, it is the first attempt to consider the quantifiable problem for dialogue coherence evaluation.




# [UXLA: A Robust Unsupervised Data Augmentation Framework for Zero-Resource Cross-Lingual NLP](https://aclanthology.org/2021.acl-long.154/)
- We propose a novel data augmentation framework, UXLA, for zero-resource cross-lingual task adaptation.

- Motivated by this, we present UXLA, our unsupervised data augmentation framework for zero-resource cross-lingual task adaptation.

- It performs simultaneous self-training with data augmentation and unsupervised sample selection.

- We consider three tasks in the zero-resource crosslingual transfer setting.




# [Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition](https://aclanthology.org/2021.acl-long.487/)
- To handle the issues, we propose the Mining Undefined Classes from Other-class (MUCO) model to leverage the rich semantics to improve few-shot NER.

- In this paper, we propose Mining Undefined Classes from Other-class (MUCO) to utilize the rich semantics in O class to improve few-shot NER.

- • We propose a novel zero-shot classification method for undefined class detection.

- Our contributions can be summarized as follows: • We propose a novel approach MUCO to leverage rich semantics in O class to improve fewshot NER.




# [RepSum: Unsupervised Dialogue Summarization based on Replacement Strategy](https://aclanthology.org/2021.acl-long.471/)
- Our main contributions are as follows: • We propose RepSum, an unsupervised (or self-supervised) strategy for dialogue summarization, which roots from the hypothesis that a superior summary approximates a replacement of the original dialogue for completing other tasks.

- Based on the RepSum strategy, we propose the corresponding model and employ it to both extractive and abstractive summarization.

- This work investigates the problem of unsupervised dialogue summarization.

- In this paper, we propose an innovative unsupervised strategy, dubbed RepSum, which can be applied to both extractive and abstractive summarization.




# [Robustifying Multi-hop Question Answering through Pseudo-Evidentiality Training](https://aclanthology.org/2021.acl-long.476/)
- This section includes the results of our model for multi-hop reasoning.

- In this paper, we propose a new approach to train multi-hop QA models, not to take reasoning shortcuts of guessing right answers without sufficient evidences.

- is our proposed method for a multi-hop QA task?

- Our focus is to identify and alleviate reasoning shortcuts in multi-hop QA, without evidence annotations.




# [A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections](https://aclanthology.org/2021.acl-long.301/)
- We propose an architecture for joint document and snippet ranking, i.e., stages (ii) and (iii), which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents.

- We modified Natural Questions to fit the BIOASQ setting.

- Our contributions can be summarized as follows: (1) We proposed an architecture to jointly rank documents and snippets with respect to a question, two particularly important stages in QA for large document collections; our architecture can be used with any neural text relevance model.

- We make our code and the modified Natural Questions publicly available.




# [Robustness Testing of Language Understanding in Task-Oriented Dialog](https://aclanthology.org/2021.acl-long.192/)
- Robustness in LU has always been a challenge in task-oriented dialog.

- This paper aims to provide an automatic way to test the LU robustness in task-oriented dialog.

- In this paper, we present a systematic robustness evaluation of language understanding (LU) in taskoriented dialog from three aspects: language variety, speech characteristics, and noise perturbation.

- (4) Quality and user evaluation results demonstrate that the augmented data are representative of real-world noisy data, therefore can be used for future research to test the LU robustness in task-oriented dialog 1 .




# [End-to-End Training of Neural Retrievers for Open-Domain Question Answering](https://aclanthology.org/2021.acl-long.519/)
- We then present two approaches for end-to-end training of the reader and retriever components in OpenQA.

- • Our end-to-end training approach obtains new state-of-the-art performance on retrieval accuracy.

- In this section, we explore two supervised training approaches to end-to-end train the reader and retriever components from the task-specific data.

- We propose a unified approach to train the retriever: unsupervised pre-training followed by supervised finetuning.




# [Maria: A Visual Experience Powered Conversational Agent](https://aclanthology.org/2021.acl-long.435/)
- The experimental results on Reddit Conversation Corpus (Dziri et al., 2019a) demonstrate that Maria significantly outperforms previous state-of-the-art methods, and can generate informative responses with visual commonsense of our physical world.

- ( 4) ReCoSa: A hierarchical transformer-based model ) that achieves the state-of-the-art performance on benchmarks of dialog generation.

- Overall, the contributions of this paper are summarized as follows: • We explore the task of image-grounded dialog generation under a fully open-ended setting where no specific image-dialog pairs are assumed available, i.e., zero-resource imagegrounded conversation.

- Specifically, we present Maria, a neural conversational agent powered by visual world experiences which are retrieved from a pre-built image index, e.g., the Open Images Dataset (Kuznetsova et al., 2018).




# [Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data](https://aclanthology.org/2021.acl-long.66/)
- We presented NMT-Adapt, a novel approach for neural machine translation of low-resource languages which assumes zero parallel data or bilingual lexicon in the low-resource language.

- We describe the NMT-Adapt approach to translating a low-resource language into and out of English without utilizing any low-resource language parallel data.

- We first evaluate performance of translating into the low-resource language.

- Utilizing parallel data in a similar high resource language as well as monolingual data in the low-resource language, we apply unsupervised adaptation to facilitate translation to and from the low-resource language.




# [Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains](https://aclanthology.org/2021.acl-long.236/)
- In this paper, we propose the Meta-Knowledge Distillation (Meta-KD) framework, which facilities cross-domain KD.

- The meta-teacher is jointly trained with multi-domain datasets to acquire the instance-level and feature-level metaknowledge.

- Learning Instance-level Transferable Knowledge.

- • We propose the Meta-KD framework to address the task.




# [Detecting Propaganda Techniques in Memes](https://aclanthology.org/2021.acl-long.516/)
- We have proposed a new multi-class multi-label multimodal task: detecting the type of propaganda techniques used in memes.

- We further created and released a corpus of 950 memes annotated with 22 propaganda techniques, which can appear in the text, in the image, or in both.

- • We develop a multi-modal annotation schema, and we create and release a new dataset for the task, consisting of 950 memes, which we manually annotate with 22 propaganda techniques.

- Our contributions can be summarized as follows: • We formulate a new multimodal task: propaganda detection in memes, and we discuss how it relates and differs from previous work.




# [PROTAUGMENT: Unsupervised diverse short-texts paraphrasing for intent detection meta-learning](https://aclanthology.org/2021.acl-long.191/)
- Paired with paraphrasing user utterances and its consistency loss incorporated in Prototypical net-works, our model is the best method for intent detection meta-learning on 4 public datasets, with neither extra labeling efforts nor domain-specific conditional language model fine-tuning.

- The measured diversity strongly correlates with the average accuracy of the intent detection task (Table 4).

- In this work, we proposed PROTAUGMENT, an architecture for meta-learning for the problem of classifying user-generated short-texts (intents).

- The ours method is PROTAUG-MENT (unsupervised consistency loss using diverse paraphrases) equipped with different paraphrasing strategies.




# [A Hierarchical VAE for Calibrating Attributes while Generating Text using Normalizing Flow](https://aclanthology.org/2021.acl-long.187/)
- We propose a hierarchical model using Variational Autoencoders (Kingma and Welling, 2013) to achieve fine grained control over attribute space while maintaining the quality of the generated sen- tences.

- Automatic generation of content with fine regulation of attributes like sentiment and style is extremely beneficial in this context.

- We demonstrate the effectiveness of CTVAE to generate controlled text by fine tuning two different attributes namely sentiment and formality.

- State-of-the-art methods for style transfer are categorized as supervised and unsupervised techniques.




# [Knowledge-Enriched Event Causality Identification via Latent Structure Induction Networks](https://aclanthology.org/2021.acl-long.376/)
- • To exploit the descriptive knowledge, we devise a descriptive graph induction module.

- Specifically, we devise a Descriptive Graph Induction module to make use of the descriptive knowledge.

- Meanwhile, we propose a Relational Graph Induction module to leverage the relational knowledge.

- Our contributions are summarized as follows: • We propose a novel Latent Structure Induction Network (LSIN) to leverage the external structural knowledge.




# [CLEVE: Contrastive Pre-training for Event Extraction](https://aclanthology.org/2021.acl-long.491/)
- In this paper, we propose CLEVE, a contrastive pre-training framework for event extraction to utilize the rich event knowledge lying in large unsupervised data.

- CLEVE consists of two components, including a text encoder to learn event semantics and a graph encoder to learn event structure information.

- Considering the fact that the AMR structures of large-scale unsupervised data can be easily obtained with automatic parsers (Wang et al., 2015), we propose CLEVE, an event-oriented contrastive pre-training framework utilizing AMR structures to build self-supervision signals.

- In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data.




# [Hierarchy-aware Label Semantics Matching Network for Hierarchical Text Classification](https://aclanthology.org/2021.acl-long.337/)
- 2. We propose a hierarchy-aware label semantics matching network (HiMatch), in which we introduce a joint embedding loss and a matching learn-ing loss to learn the text-label semantics matching relationship in a hierarchy-aware manner.

- In this section, we will describe the details about our Hierarchy-aware Label Semantics Matching Network.

- In this paper, we formulate the interaction between text and label as a semantic matching problem and propose a Hierarchy-aware Label Semantics Matching Network (HiMatch).

- Φ t , Φ l ∈ R dϕ represent text semantics and label semantics in joint embedding space, respectively.




# [DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling](https://aclanthology.org/2021.acl-long.6/)
- To our knowledge, DeepRapper is the first system that models rhythms for rap generation.

- Both objective and subjective evaluations verify the advantages of DeepRapper in generating rap lyrics with rhymes and rhythms.

- In this paper, we develop DeepRapper, a Transformer (Vaswani et al., 2017) based rap generation system which can model both rhymes and rhythms.

- Therefore, how to generate rap lyrics with good rhymes and rhythms




# [OTTers: One-turn Topic Transitions for Open-Domain Dialogue](https://aclanthology.org/2021.acl-long.194/)
- Our contributions are as follows: • We propose a new Natural Language Generation task based on one-turn topic transitions for open-domain dialogue based on a "bridging" strategy, which promotes grounding on KG entities.

- We have defined a new NLG task exploring one-turn topic transitions for mixedinitiative in open-domain systems.

- Baseline models based on state-of-the-art approaches to text generation illustrate possible approaches to the task and show that there is room for improvement.

- 3 One-turn Topic Transitions 3.1 Task Design and Data Collection Task Description.




# [Recursive Tree-Structured Self-Attention for Answer Sentence Selection](https://aclanthology.org/2021.acl-long.358/)
- We introduce the Tree Aggregation Transformer: a novel, recursive and tree-structured self-attention model for AS2.

- We introduce the Tree Aggregation Transformer: a novel recursive and treestructured self-attention model for Answer Sentence Selection.

- Our method establishes a new state of the art in the TrecQA and WikiQA benchmark datasets with only one additional selfattention layer.

- Without using AS2 datasets for transfer learning, our model establishes a new state of the art for the clean versions of TrecQA and WikiQA, two widely used benchmark datasets in question answering and AS2.




# [UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning](https://aclanthology.org/2021.acl-long.202/)
- Cross-Modal Contrastive Learning

- In this work, we propose UNIMO, a unified-modal pre-training architecture to leverage the large scale of non-paired text corpus and image collections for cross-modal learning.

- We further make several ablation studies to validate that textual knowledge and visual knowledge can enhance each other in the unified semantic space.

- We verify that UNIMO provides an effective way for textual knowledge and visual knowledge to mutually enhance each other in a unified semantic space, and UNIMO successfully adapts to both single-modal and multi-modal understanding and generation tasks.




# [Rethinking Stealthiness of Backdoor Attack against NLP Models](https://aclanthology.org/2021.acl-long.431/)
- Moreover, in response to the shortcomings of existing backdoor attacking methods, we propose a novel word-based backdoor attacking method which considers both the stealthiness to system deployers and users, making an important step towards achieving stealthy backdoor attacks.

- Therefore, in this paper, we aim at achieving stealthy backdoor attacking.

- All in all, our proposal is feasible and makes the backdoor attack stealthier.

- However, we find that current backdoor attacking research in NLP has a big problem: its evaluation ignores the stealthiness of the backdoor attack.




# [Capturing Event Argument Interaction via A Bi-Directional Entity-Level Recurrent Decoder](https://aclanthology.org/2021.acl-long.18/)
- Different from traditional token-level Seq2Seq models, we use a bi-directional entity-level recurrent decoder (BERD) with a classifier to generate a sequence of argument roles entity by entity.

- We formalize EAE as a Seq2Seq-like learning problem as follows.

- We adopt BERT (Devlin et al., 2019) as encoder and the proposed bi-directional entity-level recurrent decoder as decoder for the experiment.

- To fully utilize both left-and rightside argument role information, inspired by the bidirectional decoder for machine translation (Zhang et al., 2018), we propose a neural architecture with a novel Bi-directional Entity-level Recurrent Decoder (BERD) to generate event argument roles entity by entity.




# [End-to-End Lexically Constrained Machine Translation for Morphologically Rich Languages](https://aclanthology.org/2021.acl-long.311/)
- We described the problem of word inflection in lexically constrained machine translation.

- We evaluate different methods of lexically constrained machine translation on the Czech language.

- We propose an approach to deal with word inflection in lexically constrained translation.

- To illustrate the problem, Figure 1 shows a sentence translation from English to Czech with outputs from three methods.




# [Document-level Event Extraction via Parallel Prediction Networks](https://aclanthology.org/2021.acl-long.492/)
- • We introduce a novel matching loss function to train the end-to-end model, which can bootstrap a global optimization.

- DE-PPN is based on an encoder-decoder framework that can extract structured events from a whole document in a parallel manner.

- For training the parallel networks, we propose a matching loss function to perform a global optimization.

- In summary, our contributions are as follows: • We propose an encoder-decoder model, DE-PPN, that is based on a document-level encoder and a multi-granularity decoder to extract events in parallel with document-aware representations.




# [Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation](https://aclanthology.org/2021.acl-long.10/)
- Here we explore the performance of monolingual and multilingual models on concept-to-text datasets.

- Results show that multilingual models outperform monolingual models, and that LAD outperforms previous work in improving the performance of multilingual models, especially in low resource conditions.

- We proposed Language Agnostic Delexicalisation, a novel delexicalisation framework that matches and delexicalises MR values in the text independently of the language.

- We propose Language Agnostic Delexicalisation (LAD), a novel delexicalisation method that aims to identify and delexicalise values in the text independently of the language.




# [Meta-Learning to Compositionally Generalize](https://aclanthology.org/2021.acl-long.258/)
- Central to this approach is the generation of tasks for meta-learning by sub-sampling training data.

- We summarise our contributions as follows: • We approach the problem of compositional generalization with a meta-learning objective that tries to explicitly reduce input memorization using similarity-driven virtual tasks.

- We introduce the meta-learning augmented approach to supervised learning from ; Wang et al. (2020a) that explicitly optimizes for outof-distribution generalization.

- For compositional generalization, Lake (2019) proposes a meta-learning procedure to train a memory-augmented neural model.




# [VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation](https://aclanthology.org/2021.acl-long.308/)
- Based on the above observation, we propose to plug a cross-attention module (query!=key/value) into the Transformer encoder and design a crossattention MLM task to explicitly capture the interdependence between languages.

- We present VECO, a variable and flexible crosslingual pre-training model, targets at explicitly capturing the interdependence between languages via a plug-and-play cross-attention module.

- This phenomenon reflects that our model can better build the interdependence between languages.

- This paper endeavors to build a unified crosslingual model for NLU and NLG tasks via a plugand-play cross-attention module.




# [MulDA: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual NER](https://aclanthology.org/2021.acl-long.453/)
- We have proposed a multilingual data augmentation framework for low resource cross-lingual NER.

- These recent methods have demonstrated promising zero-shot cross-lingual NER performance.

- Our generation-based multilingual data augmentation method generates high quality synthetic training data to add more diversity.

- • We propose a generation-based multilingual data augmentation method for NER, which leverages the multilingual language models to add more diversity to the training data.




# [Employing Argumentation Knowledge Graphs for Neural Argument Generation](https://aclanthology.org/2021.acl-long.366/)
- This paper tackles argument generation through the use of argumentation knowledge graphs.

- Nevertheless, the experiment results show that our approach for controlling the generated arguments using argumentation knowledge graphs improves the quality.

- This emphasizes the impact of encoding the knowledge of the graphs into argumentative data for argument generation.

- To the best of our knowledge, however, our approach is the first to employ external knowledge from knowledge graphs for the task of argument generation.




# [G-Transformer for Document-level Machine Translation](https://aclanthology.org/2021.acl-long.267/)
- Experiments show that G-Transformer converges faster and more stably than Transformer on different settings, obtaining the state-of-the-art results under both non-pretraining and pre-training settings.

- Experiments show that the resulting G-Transformer converges fast and stably on small and large data, giving the state-of-the-art results compared to existing models under both pre-training and random initialization settings.

- The above experiments show that training failure on Transformer can be caused by local minima.

- In other words, the hypothesis space for target-to-source attention is increased.




# [Explaining Contextualization in Language Models using Visual Analytics](https://aclanthology.org/2021.acl-long.39/)
- Additionally, we show that contextualization is neither entirely driven by polysemy nor context variation.

- Thus, we decided to investigate function and content words in more detail, using the LMExplorer to explore contextualization in BERT with respect to the functionality continuum.

- This paper presented new insights on the contextualization of the functionality continuum, showing that BERT fails to capture the nature of semifunctional-semi-content words.

- Overall, the contribution of this paper is twofold: (1) we generate insights as to how BERT captures function vs. content words (Sections 4 and 5), and (2) present a novel visual analytics technique that facilitates such insights by explaining LMs through contextualization scoring (Section 3).




# [Bird's Eye: Probing for Linguistic Graph Structures with a Simple Information-Theoretic Approach](https://aclanthology.org/2021.acl-long.145/)
- In this work, we introduce a new probing approach, Bird's Eye, which can be used to detect if contextualized text representations encode entire linguistic graphs.

- Bird's Eye allows us to probe for entire linguisitic structures.

- We call this probe, Worm's Eye.

- We further extend Bird's Eye to probe for localized linguistic information in the linguistic graphs such as POS or dependency arc labels in dependency parses.




# [OntoED: Low-resource Event Detection with Ontology Embedding](https://aclanthology.org/2021.acl-long.220/)
- Given the event ontology with correlations among event types, we infer new event correlations based on existing ones.

- This illustrates the effectiveness of OntoED handling new unseen event types without introducing outsourcing data.

- As data-rich event types can propagate correlation knowledge to data-poor ones, and new event types can establish linkages to the event ontology.

- As the first attempt to construct such event ontology, we propose a novel ED framework with ontology embedding called OntoED.




# [Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation](https://aclanthology.org/2021.acl-long.22/)
- We conclude from this experiment that MBR is more robust to copy noise in the training data.

- We empirically study the properties of MBR decoding with common MT metrics as utility functions, and find it still exhibits a length bias and token frequency bias similar to beam search.

- To summarize, we find that MBR decoding has a higher domain robustness than beam search.

- Susceptibility to copy noise: Copied content in the training data disproportionately affects translation quality.




# [Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction](https://aclanthology.org/2021.acl-long.261/)
- Moreover, we propose a graph-based model to enhance the semantic dependencies between a candidate clause and a given emotion clause by extracting relevant knowledge paths from ConceptNet.

- We first define the Emotion Cause Extraction (ECE) task here.

- To alleviate the position bias problem, we propose to leverage the commonsense knowledge to enhance the semantic dependencies between a candidate clause and the emotion clause.

- The experimental results show that our proposed method achieves comparative performance to the state-of-the-art methods, and is more robust against adversarial attacks.




# [Adapting Unsupervised Syntactic Parsing Methodology for Discourse Dependency Parsing](https://aclanthology.org/2021.acl-long.449/)
- We adapt two current state-of-the-art models in unsupervised syntactic dependency parsing for discourse parsing.

- In this paper, we propose a method to adapt unsupervised syntactic parsing methods for discourse dependency parsing.

- We apply the adaptations to two unsupervised syntactic dependency parsing methods.

- Adaptation from syntactic dependency parsing to discourse dependency parsing has two challenges.




# [Comprehensive Study: How the Context Information of Different Granularity Affects Dialogue State Tracking?](https://aclanthology.org/2021.acl-long.193/)
- • How to combine multiple granularities for dialogue state tracking?

- for dialogue state tracking

- • Application of context information granularity in few-shot learning scenario.

- Thus, in this paper, we will study and discuss how the context information of different granularity affects dialogue state tracking.




# [BERTGEN: Multi-task Generation through BERT](https://aclanthology.org/2021.acl-long.503/)
- In this paper, we presented BERTGEN, a novel generative, decoder-only model which extends BERT by combining multimodal and multilingual pretrained models.

- Our ablation studies show that BERTGEN is able to efficiently transfer relevant inductive biases from the pre-trained models and benefits from multi-task learning without suffering from catastrophic forgetting.

- Zero-shot performance.

- The multi-task (and zero-shot) generation ability of BERTGEN is mostly inspired by Ha et al. (2016) and Johnson et al. (2017).




# [Self-Guided Contrastive Learning for BERT Sentence Representations](https://aclanthology.org/2021.acl-long.197/)
- In this paper, we have proposed a contrastive learning method with self-guidance for improving BERT sentence embeddings.

- Contrastive Representation Learning.

- To conclude, we demonstrate that our self-guided contrastive learning is effective in improving the quality of BERT sentence embeddings when tested on STS tasks.

- In this work, we propose a contrastive learning method that makes use of a newly proposed selfguidance mechanism to tackle the aforementioned problem.




# [Guiding Teacher Forcing with Seer Forcing for Neural Machine Translation](https://aclanthology.org/2021.acl-long.223/)
- This means knowledge distillation does transfer future information from the seer decoder to the conventional decoder.

- The behaviors of the conventional decoder are guided by the seer decoder via knowledge distillation.

- To this end, we employ the method of knowledge distillation to transfer future information from the seer decoder to the conventional decoder.

- Hence we need to introduce knowledge distillation to reinforce the influence of the seer decoder to the conventional decoder.




# [Societal Biases in Language Generation: Progress and Challenges](https://aclanthology.org/2021.acl-long.330/)
- Our contributions are a comprehensive survey on societal biases in language generation and an experimental study on biases from decoding techniques.

- Motivated by the importance of fairness in language generation, we present the first comprehensive survey on societal biases in language generation.

- In this work, we present a survey and commentary on the progress and challenges for studying societal biases in language generation.

- As a fairly nascent area of exploration, the study of biases in language generation still poses many challenges.




# [FEW-NERD: A Few-shot Named Entity Recognition Dataset](https://aclanthology.org/2021.acl-long.248/)
- We propose FEW-NERD, a large-scale few-shot NER dataset with fine-grained entity types.

- This is the first few-shot NER dataset and also one of the largest human-annotated NER dataset.

- In this paper, we present a human-annotated dataset, FEW-NERD, for few-shot learning in NER.

- To the best of our knowledge, FEW-NERD is the first dataset specially constructed for few-shot NER and also one of the largest human-annotated NER dataset (statistics in Section 5.1).




# [Matching Distributions between Model and Data: Cross-domain Knowledge Distillation for Unsupervised Domain Adaptation](https://aclanthology.org/2021.acl-long.421/)
- • For the first time, the gradient information of the source domain is exploited to boost the UDA performance.

- In this paper, we propose a generic framework named Cross-domain Knowledge Distillation (CdKD).

- CdKD learned the collective knowledge across different domains including domain-invariant and discriminative features by matching the joint distributions between a trained source model and a set of target data.

- The main contributions are outlined as, • We propose to investigate the problem of UDA without needing source data by exploring the distribution discrepancy between a source model and a set of target data.




# [HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation](https://aclanthology.org/2021.acl-long.423/)
- Besides, we propose a hierarchical user interest matching framework to match candidate news with different levels of interest representations to target user interests more accurately.

- Thus, we propose a hierarchical user interest modeling framework, which learns a hierarchical interest tree to capture diverse and multi-grained user interest.

- In this paper, we propose a personalized news recommendation method named HieRec for hierarchical user interest modeling, which can effectively model diverse and multi-grained user interests.

- Thus, we propose a hierarchical user interest matching framework, which models user interests in candidate news from different interest granularities.




# [TIMEDIAL: Temporal Commonsense Reasoning in Dialog](https://aclanthology.org/2021.acl-long.549/)
- Our findings indicate that large-scale pre-trained models even after fine-tuning may not be sufficient for robust temporal reasoning in dialogs, and motivate future research toward modeling temporal concepts over diverse everyday events, and contextual reasoning about them.

- We formulate the problem as a crowd-sourced cloze task with multiple choices based on dialogs in the DailyDialog dataset (Li et al., 2017).

- We design a new task for dialog-based temporal reasoning and present a new challenge set in English, called TIMEDIAL, to evaluate language understanding models on the task.

- We introduced TIMEDIAL, a challenge set consistting of 1.1K multiple-choice cloze questions for temporal commonsense reasoning in dialog.




# [Changing the World by Changing the Data](https://aclanthology.org/2021.acl-long.170/)
- DL models learn spurious patterns present in the data.

- This position paper brings together the arguments for and against curating data 2 from linguistic and ethical perspectives ( §2).

- 2.2 Why Not to Change the Data? Since this is a position paper arguing that data curation is unavoidable, the arguments against it are presented together with the defense.

- Deciding what should not be remembered is clearly a data curation issue.




# [Multi-Task Retrieval for Knowledge-Intensive Tasks](https://aclanthology.org/2021.acl-long.89/)
- • We propose a single general-purpose "universal" retrieval model, able to perform comparably or better than specialised retriever approaches in both zero-shot (leave-one-out) and few-shot retrieval.

- Large-scale pre-trained models have been shown to store knowledge directly into their parameters.

- In order to evaluate the suitability of a multi-task trained retriever as a starting checkpoint for few-shot training, we take the various leave-one-out models and finetune them on our few-shot training sets.

- In this recent work, the authors multi-task train a pre-trained model on around 50 datasets, before performing the final fine-tuning.




# [Concept-Based Label Embedding via Dynamic Routing for Hierarchical Text Classification](https://aclanthology.org/2021.acl-long.388/)
- We propose a novel concept-based label embedding model.

- To further exploit the information of concept for HTC, we propose a novel concept-based label embedding method which can explicitly represent the concepts and model the sharing mechanism among classes.

- Then a concept sharing module is designed for extracting concepts and modeling the sharing mechanism among classes.

- The visualization of the concepts and the learnt concept-based label embeddings re-




# [Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models](https://aclanthology.org/2021.acl-long.523/)
- We propose Polyjuice, a general-purpose generator that produces fluent and diverse counterfactuals, allowing for control over the kinds and locations of perturbations.

- 2 General-Purpose Counterfactuals

- Finally, Polyjuice supports counterfactual error analysis ( §5).

- In another application, Polyjuice produces counterfactual explanations ( §4), providing significant insight on top of state-of-the-art explanation techniques.




# [Exploring Dynamic Selection of Branch Expansion Orders for Code Generation](https://aclanthology.org/2021.acl-long.394/)
- In this section, we extend the conventional Seq2Tree model with a context-based branch selector, which dynamically determines optimal expansion orders of branches for multi-branch AST nodes.

- Specifically, we propose to equip the conventional Seq2Tree model with a context-based Branch Selector, which dynamically quantifies the priorities of expanding different branches for multi-branch nodes during AST generations.

- Then we propose an extended Seq2Tree model equipped with a context-based branch selector, which is capable of dynamically determining optimal branch expansion orders for multi-branch nodes.

- In this way, the model is trained to determine optimal expansion orders of branches for multi-branch nodes, which will contribute to AST generations.




# [Dynamic Contextualized Word Embeddings](https://aclanthology.org/2021.acl-long.542/)
- We introduce dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context.

- We have introduced dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context.

- Do dynamic contextualized word embeddings indeed capture interpretable dynamics in word meaning?

- In this paper, we introduce dynamic contextualized word embeddings that combine the strengths of contextualized word embeddings with the flexibility of dynamic word embeddings.




# [Superbizarre Is Not Superb: Derivational Morphology Improves BERT's Interpretation of Complex Words](https://aclanthology.org/2021.acl-long.279/)
- Conceptually, PLMs can thus be interpreted as serial dual-route models.

- Overall, our findings suggest that the generalization capabilities of PLMs could be further improved if a morphologically-informed vocabulary of input tokens were used.

- This suggests that the generalization capabilities of PLMs could be further improved if a morphologically-informed vocabulary of input tokens were used.

- We present the first study examining how the input segmentation of PLMs, specifically BERT, affects their interpretations of derivationally complex English words.




# [ONE2SET: Generating Diverse Keyphrases as a Set](https://aclanthology.org/2021.acl-long.354/)
- To successfully train under ONE2SET paradigm, we propose a K-step target assignment mechanism and a separate set loss, which greatly increases the number and diversity of the generated keyphrases.

- An attempt is first made to remove the K-step target assignment mechanism, which means that we employ a fixed sequential matching strategy as in the ONE2SEQ paradigm.

- We summarize our main contributions as follows: (1) we propose a new training paradigm ONE2SET without predefining an order to concatenate the keyphrases; (2) we propose a novel set prediction model that can generate a set of diverse keyphrases in parallel and a dynamic target assignment mechanism to solve the intractable training problem under the ONE2SET paradigm; (3) our method consistently outperforms all the state-of-the-art methods and greatly reduces the duplication ratio.

- (2) The bipartite characteristics of the Kstep target assignment forces the model to predict unique keyphrases, which reduces the duplication ratio of predictions.




# [Controllable Open-ended Question Generation with A New Question Type Ontology](https://aclanthology.org/2021.acl-long.502/)
- Distri- 4 Type-aware Open-ended Question Generation In this section, we present our type-aware question generation framework.

- Automatic metrics show that our type-aware question generation model outperforms competitive comparisons, highlighting the effectiveness of semantic graph-augmented representation and joint modeling of focus prediction and question generation.

- Based on this framework, we also enhance the controllability and diversity of generated questions by employing template exemplars or automatically generated templates.

- Second, our question type ontology provides a new perspective for question diversity evaluation.




# [Comparing Test Sets with Item Response Theory](https://aclanthology.org/2021.acl-long.92/)
- We also find span selection as the most effective task format for discriminating between strong and weak models.

- • Span-based QA is an effective task format for discriminating between strong and weak models.

- Span Selection We observe that span selection datasets are the most discriminative.

- The dataset is formulated as a span selection QA task.




# [Multi-Label Few-Shot Learning for Aspect Category Detection](https://aclanthology.org/2021.acl-long.495/)
- Multi-Label Few-Shot Learning Compared with single-label FSL, the multi-label FSL has been underexplored.

- • To alleviate the noise from the support set and query set, we design two effective attention mechanisms, i.e., support-set attention and query-set attention.

- Therefore, we propose a multi-label FSL method based on the prototypical network.

- Specifically, we design two effective attention mechanisms for the support set and query set to alleviate the noise from both sets.




# [Robust Knowledge Graph Completion with Stacked Convolutions and a Student Re-Ranking Network](https://aclanthology.org/2021.acl-long.82/)
- We develop an expressive deep convolutional model that utilizes textual entity representations more effectively and improves sparse KGC.

- • We develop a re-ranking procedure that distills knowledge from our ranking model into a student network that re-ranks promising candidate entities.

- • We develop a deep convolutional architecture that utilizes textual embeddings more effectively than existing neural KGC models and significantly improves performance for sparse KGC.

- We develop an entity re-ranking procedure and demonstrate the effectiveness of the re-ranking paradigm for KGC.




# [Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open Domain Question Answering](https://aclanthology.org/2021.acl-long.315/)
- In this section, we describe our method for hybrid open-domain question answering.

- • We propose a simple but effective generative approach that takes both textual and tabular evidence and generates either direct answers or SQL queries, automatically determined by the context.

- Our highlighted contributions are as follows: • We propose a multi-modal framework that incorporates hybrid knowledge sources with the Text2SQL ability for ODQA tasks.

- Open-domain question answering (ODQA) is a task to answer factoid questions without a prespecified domain.




# [Transfer Learning for Sequence Generation: from Single-source to Multi-source](https://aclanthology.org/2021.acl-long.446/)
- The proposed framework aims to reduce the pretrain-finetune discrepancy and learn better multi-source representations.

- Therefore, we propose a two-stage finetuning method named gradual finetuning.

- In this paper, we propose a two-stage finetuning method named gradual finetuning.

- To alleviate the pretrain-finetune discrepancy, we adopt the gradual finetuning method to better transfer from single-source to multi-source.




# [Learning from Perturbations: Diverse and Informative Dialogue Generation with Inverse Adversarial Training](https://aclanthology.org/2021.acl-long.57/)
- (2) Do inverse adversarial training help neural dialogue models generate more diverse, engaging, and informative dialogue responses?

- Experimental results show IAT helps neural dialogue systems model dialogue history better and generate more diverse and informative responses.

- Experimental results on two benchmark datasets show that the proposed inverse adversarial training algorithm helps dialogue models capture dialogue history better and generate more diverse and consistent responses.

- To address the above issues, in this paper, we propose Inverse Adversarial Training (IAT) algorithm for training neural dialogue systems to avoid generic responses and model dialogue history better, thus generating diverse and informative responses.




# [Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation](https://aclanthology.org/2021.acl-long.562/)
- We propose a divergent-aware framework for NMT ( § 4.1) that successfully mitigates their impact ( § 4.3).

- This paper aims to understand and mitigate the impact of fine-grained semantic divergences in NMT.

- Based on these findings, we introduce a divergent-aware NMT framework that incorporates information about which tokens are indicative of semantic divergences between the source and target side of a training sample.

- Our experiments on EN↔FR tasks show that fine-grained semantic divergences hurt translation quality when they overwhelm the training data.




# [Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation](https://aclanthology.org/2021.acl-long.468/)
- Consequently, we propose a novel embedding transfer strategy with a plug-and-play embedding generator.

- As shown in Figure 1, we extend the prior pretrain-finetune paradigm with an embedding transfer stage.

- Figure 1: Illustration of our pretrain-finetune pipeline.

- • We propose a simple, flexible, and generalized pretrain-finetune training strategy, where an embedding generator is introduced to leverage the knowledge of the pre-trained model to initialize embeddings of any required tokens.




# [More Identifiable yet Equally Performant Transformers for Text Classification](https://aclanthology.org/2021.acl-long.94/)
- Thus, our contribution are as follows: • We provide a concrete theoretical analysis of identifiability of attention weights which was missing in the previous work by Brunner et al. (2019).

- In this work, we probe the identifiability of attention weights in Transformer from a perspective that was ignored in Brunner et al. (2019).

- It is important that the identifiability of attention weights should not come at the cost of reduced performance of the model.

- This may lead to more identifiable attention weights.




# [PHMOSpell: Phonological and Morphological Knowledge Guided Chinese Spelling Check](https://aclanthology.org/2021.acl-long.464/)
- 2) We design a novel adaptive gating mechanism, which effectively incorporates the multi-modal information into a pre-trained language model in an end-to-end trainable way.

- PHMOSpell incorporates pinyin and glyph features into a pre-trained language model via an adaptive gating module for CSC.

- In this research, we propose a novel end-to-end trainable model called PHMOSpell for CSC, which incorporates both phonological and morphological knowledge from two feature extractors into a pretrained language model by an effective adaptive gating mechanism.

- The contributions of this paper are in three folds: 1) We derive both phonological and morphological knowledge of Chinese characters from multimodality and apply them to CSC.




# [Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval](https://aclanthology.org/2021.acl-long.392/)
- Our main contributions can be summarized as follows: • We propose a novel approach to represent the document with multiple pseudo query embeddings which are generated by a clustering process.

- In this paper, we propose a method to improve the performance of the first-stage retrieval model which is based on Bi-encoder and semi-interactive aggregator.

- Experimental results show that our approach achieves state-of-theart retrieval performance while still remaining efficient computation.

- Specifically, our method mimics the real queries by an iterative K-means clustering algorithm.




# [Data Augmentation for Text Generation Without Any Augmented Data](https://aclanthology.org/2021.acl-long.173/)
- In this section, we aim to formulate the problem of data augmentation for general text generation models without any use of augmented data mapping functions.

- To answer this question, we aim to formulate the problem of data augmentation for general text generation models without any use of augmented data mapping functions.

- We have proposed an objective of formulating data augmentation without any use of any augmented data mapping function.

- The proposed approach provides a new paradigm and understanding of data augmentation for text generation.




# [DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations](https://aclanthology.org/2021.acl-long.547/)
- • We design multi-turn reasoning modules to extract and integrate emotional clues by iteratively performing the intuitive retrieving process and conscious reasoning process, which imitates human unique cognitive thinking.

- Sec-ondly, in the cognitive phase, we design multi-turn reasoning modules to iteratively extract and integrate the emotional clues.

- In the cognitive phase, we design multi-turn reasoning modules to iteratively perform the intuitive retrieving process and conscious reasoning process, which imitates human unique cognitive thinking.

- Therefore, in the cognitive phase, we design multi-turn reasoning modules to iteratively extract and integrate the emotional clues.




# [A Human-machine Collaborative Framework for Evaluating Malevolence in Dialogues](https://aclanthology.org/2021.acl-long.436/)
- HMCEval achieves around 99% evaluation accuracy (compared to human evaluation) with as much as half of the human effort saved.

- HMCEval formulates the dialogue evaluation task as a sample assignment problem, i.e., if the machine can provide accurate outcomes, most evaluation samples should be assigned to the machine; otherwise, we should assign more samples to human evaluators.

- Experiments on the task of evaluating malevolence in dialogue responses show that HMCEval can achieve around 99% reliability with half human effort spared.

- We propose the human-machine collaborative evaluation (HMCEval) framework to solve this task.




# [On Finding the K-best Non-projective Dependency Trees](https://aclanthology.org/2021.acl-long.106/)
- All three papers utilize the K-best spanning tree algorithm of Camerini et al. (1980).

- Furthermore, we provided a novel extension to the algorithm that decodes the K-best dependency trees in O(KN 2 ).

- In this paper, we provided a simplification to Camerini et al. (1980)'s O(KN 2 ) K-best spanning trees algorithm.

- 5 Finding the K th Best Dependency Tree In this section, we present a novel extension to the algorithm presented thus far, that allows us to efficiently find the K-best dependency trees.




# [ILDC for CJPE: Indian Legal Documents Corpus for Court Judgment Prediction and Explanation](https://aclanthology.org/2021.acl-long.313/)
- Based on ILDC, we propose a new task: COURT JUDGMENT PREDICTION AND EXPLANATION (CJPE).

- We release the ILDC and code for the prediction and explanation models via GitHub 1 .

- We experimented with the best judgment prediction model (Hierarchical Transformer (XLNet + BiGRU)) for all the explainable algorithms.

- 3. We develop a battery of baseline models for the CJPE task.




# [Towards Emotional Support Dialog Systems](https://aclanthology.org/2021.acl-long.269/)
- We then construct an Emotional Support Conversation dataset, ESConv.

- In this work, we define the task of Emotional Support Conversation and present an ESC Framework.

- In this paper, we define the task of Emotional Support Conversation (ESC), aiming to provide

- Training and Examination To teach crowdworkers how to provide effective emotional support




# [Distributed Representations of Emotion Categories in Emotion Space](https://aclanthology.org/2021.acl-long.184/)
- Any model that outputs are soft labels can be employed to learn the distributed representations for emotion categories.

- Distributed representations of emotion categories in emotion space can also benefit NLP applications.

- As far as we know, this is the first work to learn the distributed representations for emotion categories in emotion space rather than semantic space.

- The further experiments show that our representations in emotion space can express emotion relations much better than word vectors in semantic space.




# [Online Learning Meets Machine Translation Evaluation: Finding the Best Systems with the Least Human Effort](https://aclanthology.org/2021.acl-long.242/)
- We proposed an online learning approach to address the issue of finding the best MT systems among an ensemble, while making the most of existing human feedback.

- We contribute with an online MT ensemble that allows to reduce human effort by immediately incorporating human feedback in order to dynamically converge to the best systems 1 .

- In Machine Translation (MT), measuring the quality of a large amount of automatic translations can be a challenge.

- For fr-de (Table 4), our online approach often converges to the top 3 systems (or a subset of them) throughout the learning process (even at just 10 iterations), and it also converges to the best system when using EWAF with human-comet.




# [GhostBERT: Generate More Features with Cheap Operations for BERT](https://aclanthology.org/2021.acl-long.509/)
- In this paper, we propose GhostBERT to generate more features in pre-trained model with cheap operations.

- Empirical results on BERT, RoBERTa and ELECTRA demonstrate that adding the proposed ghost modules enhances the representation power and boosts the performance of the original model by supplying more features.

- 1: Development set results of the baseline pre-trained language models and our proposed method on the GLUE benchmark.

- When the cheap ghost modules are directly applied to these unpruned pretrained models, better performances are achieved with only negligible additional parameters and FLOPs.




# [Handling Extreme Class Imbalance in Technical Logbook Datasets](https://aclanthology.org/2021.acl-long.312/)
- The feedback loop approach for selecting training data is generic and could easily be applied to any learning problem with substantial class imbalances.

- In this work, we used a set of 7 logbook datasets from the aviation, automotive, and facility domains available at MaintNet (Akhbardeh et al., 2020a).

- The methodology presented in this paper could be applied to other maintenance corpora from a variety of technical domains.

- As discussed in Section 1, the extreme class imbalance observed in these technical datasets substantially affects learning algorithms' performance.




# [When Do You Need Billions of Words of Pretraining Data?](https://aclanthology.org/2021.acl-long.90/)
- The commonsense learning curve (for Winograd coref. only) rises far later, and is projected to continue to rise long after syntactic and semantic features stop improving.

- We find that ability in syntax and semantics largely saturates after only 10M to 100M words of pretraining data-on par with the data available to human learners-while learning factual knowledge requires much more data.

- Figure 3 shows the aggregated learning curves of syntactic, semantic, and commonsense tasks.

- We find the greatest improvement in overall BLiMP performance between 1M and 100M words of pretraining data.




# [Positional Artefacts Propagate Through Masked Language Model Embeddings](https://aclanthology.org/2021.acl-long.413/)
- Lastly, we find that "clipping" does not affect models' performance on three supervised tasks.

- With this insight in mind, the contributions of our work are as follows: 1. We introduce a neuron-level method for analyzing the origin of a model's outliers.

- Neuron-level analysis In order to test the extent to which BERT and RoBERTa's outliers are related to positional information, we employ a probing technique inspired by Durrani et al. (2020).

- Also relevant are the studies of Dalvi et al. (2018), who introduce a neuron-level analysis method, and Durrani et al. (2020), who use this method to analyze individual neurons in contextualized word vectors.




# [A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies](https://aclanthology.org/2021.acl-long.131/)
- is to inform researchers in computational linguistics (CL) and language technologies about the linguistic and social aspects of code-switching (C-S) found in multilingual contexts (e.g. Europe and India) and how linguists describe and model them.

- 3 Why do speakers code-switch?

- To date, the literature focusing on the social and linguistic aspects of C-S is less visible for CL researchers.

- We identify three main limitations of the current state of computational processing of C-S: data, evaluation and user-facing applications.




# [Conditional Generation of Temporally-ordered Event Sequences](https://aclanthology.org/2021.acl-long.555/)
- We propose a conditional generation model to tackle temporal event ordering and event infilling, and train it as a denoising autoencoder over outof-context temporal event sequences.

- This work presents a BART-based conditional generation model and a denoising autoencoder framework to learn temporal event knowledge, and addresses both temporal ordering and event infilling tasks by pretraining on automatically collected data.

- Our model can be formulated as a denoising autoencoder if x is created as a noised version of y.

- Our experiments demonstrate that our model is able to perform temporal ordering and infilling in a zeroshot manner, not fine-tuned on our target datasets, which suggests that it can also be applied to other settings requiring event schematic and temporal knowledge.




# [PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity](https://aclanthology.org/2021.acl-long.424/)
- Thus, incorporating popular news has the potential to alleviate the cold-start and diversity problems in personalized news recommendation.

- We propose a unified model to predict time-aware news popularity based on news content, recency, and near real-time CTR.

- Extensive experiments on two real-world datasets show PP-Rec can effectively improve the performance of news recommendation in terms of both accuracy and diversity.

- In this paper, we propose a new news recommendation method named PP-Rec to alleviate the coldstart and diversity problems of personalized news recommendation, which can consider both the personal interest of users and the popularity of candidate news.




# [Multi-Head Highly Parallelized LSTM Decoder for Neural Machine Translation](https://aclanthology.org/2021.acl-long.23/)
- In practice, however, LSTM is slower than the self-attention network in training.

- To constrain the number of LSTM parameters, we compute several small HPLSTMs in parallel like multi-head self-attention.

- The Transformer can be trained efficiently due to the highly parallelized self-attention network.

- In our experiments, we empirically show that the MHPLSTM model achieves better performance than self-attention networks, while being even slightly faster in training, and much faster in decoding, than the self-attention Transformer decoder.




# [Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities](https://aclanthology.org/2021.acl-long.203/)
- In summary, the main contributions of this work are: 1) We propose a unified model, Missing Modality Imagination Network (MMIN), to improve the robustness of emotion recognition systems under uncertain missing-modality testing con-ditions.

- In order to learn robust joint multimodal representations, we propose a unified model, Missing Modality Imagination Network (MMIN), which can deal with different uncertain missing-modality conditions in real application scenarios.

- In this work, we propose a novel unified model, Missing Modality Imagination Network (MMIN), to address the above issues.

- To the best of our knowledge, this is the first work that investigates a unified model for multimodal emotion recognition with uncertain missing-modality.




# [Rational LAMOL: A Rationale-Based Lifelong Learning Framework](https://aclanthology.org/2021.acl-long.229/)
- Then we introduce the core lifelong learning framework of Rational LAMOL in Section 3.2.

- In this paper, we improve existing LLL strategies by proposing Rational LAMOL, a rationale-based lifelong learning framework which equips the original LAMOL with critical freezing (Nguyen et al., 2020) to further prevent catastrophic forgetting.

- We introduce Rational LAMOL and its detailed implementation in this section.

- Moreover, using unsupervised rationale generation instead of human rationales also yielded competitive performance, achieving average improvements of 2.67% from original LAMOL.




# [A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering](https://aclanthology.org/2021.acl-long.318/)
- To alleviate the spurious solution problem in weakly supervised QA, we propose to explicitly exploit the semantic correlations between a question and its solution via mutual information maximization.

- Our contributions are as follows: (1) We propose a mutual information maximization approach for the spurious solution problem in weakly supervised QA, which exploits the semantic correlations between a question and its solution; (2) We conducted extensive experiments on four QA datasets.

- To exploit the semantic correlations between a question and its solution, we propose to maximize the mutual information between question-answer pairs and model-predicted solutions.

- They do not explicitly exploit the semantic correlations between a question and its solution.




# [TAN-NTM: Topic Attention Networks for Neural Topic Modeling](https://aclanthology.org/2021.acl-long.299/)
- Our contributions can be summarised as: • We propose a document encoding framework for topic modeling which leverages the topicword distribution to perform attention effectively in a topic aware manner.

- • We show that our topic model encoder can be adapted to improve the topic guided supervised keyphrase generation achieving improved performance on this task.

- NTM to discover topics in a document corpus by performing attention on sequentially processed tokens in a topic guided manner.

- Fundamentally different from these, we use topic-word distribution to attend on sequentially processed tokens via novel topic guided attention for performing variational inference, learning better document-topic features and improving topic modeling.




# [Intrinsic Bias Metrics Do Not Correlate with Application Bias](https://aclanthology.org/2021.acl-long.150/)
- These plots show no relationship at all between intrinsic and extrinsic metrics.

- To answer this question, we analyse the relationship between intrinsic and extrinsic bias.

- We need to measure the relationship between intrinsic and extrinsic metrics as bias changes, we must generate many datapoints for each experiment.

- If intrinsic and extrinsic measures do not correlate with simple embeddings, this result is unlikely to be changed by adding more architectural layers and configurable hyperparameters.




# [Generating SOAP Notes from Doctor-Patient Conversations Using Modular Summarization Techniques](https://aclanthology.org/2021.acl-long.384/)
- In this paper, we introduce the first end-to-end methods for generating whole SOAP notes based on clinical conversations.

- Our first methodological contribution is to propose a spectrum of methods, for decomposing summarizaton tasks into extractive and abstractive subtasks.

- We proposed a spectrum of extractive-abstractive summarization methods that leverage: (i) section-structured form of the SOAP notes and (ii) linked conversation utterances associated with every SOAP note sentence.

- • A new collection of extractive-abstractive approaches for generating long sectionsegmented summaries of conversations, including new methods that leverage annotations attributing summary sentences to conversation utterances.




# [Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models](https://aclanthology.org/2021.acl-long.114/)
- CGMH and REFLECTIVE DECODING both return multiple sampled, ranked paraphrases.

- We present REFLECTIVE DECODING, a novel unsupervised text generation method for tasks that do not fit the text continuation paradigm.

- REFLECTIVE DECODING requires no supervision, only two complementary off-the-shelf LMs-one forward ( −→ LM) and one backward ( ←− LM).

- REFLECTIVE DECODING Out-of-the-Box A major advantage to applying REFLECTIVE DECOD-ING




# [HiddenCut: Simple Data Augmentation for Natural Language Understanding with Better Generalization](https://aclanthology.org/2021.acl-long.338/)
- To sum up, our contributions are: • We propose a simple data augmentation method, HiddenCut, to regularize PLMs during fine-tuning by cutting contiguous spans of representations in the hidden space.

- Results show that our method consistently outperforms baselines, especially on out-of-distribution and challenging counterexamples.

- To overcome these limitations, we propose a simple yet effective data augmentation method, Hid-denCut, to regularize PLMs during the fine-tuning stage.

- Through extensive experiments on indistribution datasets (GLUE benchmarks) and outof-distribution datasets (challenging counterexamples), HiddenCut consistently and significantly outperformed state-of-the-art baselines, and demonstrated superior generalization performances.




# [COINS: Dynamically Generating COntextualized Inference Rules for Narrative Story Completion](https://aclanthology.org/2021.acl-long.395/)
- Narrative Story Completion.

- Specifically, we focus on Narrative Story Completion (NSC), a new task setting for story generation.

- We addressed a Narrative Story Completion task that allows us to probe the coherence capabilities of a neural generation model.

- Our main contributions are as follows: 1) We propose a new setting for a Narrative Story Completion task, which asks a system to complete a narrative story given its beginning and ending, with the aim of examining the reasoning capacities of a model that solves the task.




# [Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data](https://aclanthology.org/2021.acl-long.322/)
- In this paper, we investigated the learning of language and multimodal representations of typed text collected from mobile data.

- As a step towards privacy-preserving learning, we also propose approaches that obfuscate user identity while remaining predictive of daily mood.

- We then study our proposed method of learning privacy-preserving features to determine whether it can obfuscate user identity while remaining predictive of daily mood.

- learning to predict daily mood labels y.




# [Optimizing Deeper Transformers on Small Datasets](https://aclanthology.org/2021.acl-long.163/)
- We call our method the Data-dependent Transformer Fixed-update initialization scheme, DT-Fixup.

- We first apply DT-Fixup on the task of crossdomain Text-to-SQL semantic parsing.

- Based on the derivation, we propose a data-dependent initialization strategy for the mixed setup of the new transformers on pre-trained encodings.

- On two important tasks, Text-to-SQL semantic parsing and logical reading comprehension that require reasoning and structural understanding, applying DT-Fixup achieves SOTA or near-SOTA results by simplying using extra transformer layers on top of the pre-trained models.




# [Do Context-Aware Translation Models Pay the Right Attention?](https://aclanthology.org/2021.acl-long.65/)
- We apply attention regularization to guide model attention to increase alignment with the supporting context from SCAT.

- Next, we study NMT models and quantify the degree to which the model's attention is aligned with the supporting context from professional translators.

- We measure alignment between the baseline context-aware model's attention and human rationales across various model attention heads and layers.

- We observe a relatively high alignment between self attention scores from the top encoder layers and the source-side supporting context marked by translators, however, the model's attention is poorly aligned with target-side supporting context.




# [Revisiting the Negative Data of Distantly Supervised Relation Extraction](https://aclanthology.org/2021.acl-long.277/)
- • Our empirical evaluations show that the proposed method consistently outperforms existing approaches, and achieves excellent perfor-mance even learned with a large quantity of false positive samples.

- In this paper, we revisit the negative data in relation extraction task.

- In this paper, we address these challenges caused by negative data.

- and propose a multi-label collective loss function.




# [Competence-based Multimodal Curriculum Learning for Medical Report Generation](https://aclanthology.org/2021.acl-long.234/)
- Then we propose the multiple difficulty-based curriculum learning for medical report generation.

- • We assess the difficulty of each training instance from multiple perspectives and propose a competence-based multimodal curriculum learning framework (CMCL) to consider multiple difficulties simultaneously.

- To this end, we propose a novel Competencebased Multimodal Curriculum Learning framework (CMCL) which progressively learns medical reports following an easy-to-hard fashion.

- In this section, we briefly describe typical medical report generation approaches and introduce the proposed Competence-based Multimodal Curriculum Learning (CMCL).




# [Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data](https://aclanthology.org/2021.acl-long.140/)
- • Noise-Aware Loss Function.

- • Weak Label Completion. As the weakly labeled data suffer from severe missing entity issue, we propose a weak label completion procedure.

- • NAL: Noise-aware loss function, i.e., Eq.( 4).

- • FT: Final fine-tuning on strongly labeled data (Stage III).




# [Taming Pre-trained Language Models with N-gram Representations for Low-Resource Domain Adaptation](https://aclanthology.org/2021.acl-long.259/)
- 4 Motivated by this find-3337 ing, we propose a light-weight Transformer-based Domain-aware N-gram Adaptor (T-DNA) by incorporating n-gram representations to bridge the domain gap between source and target vocabulary.

- To better represent and incorporate unseen and domain-specific n-grams, we first need to find and extract them.

- Overall, for both FT and TAPT experiments, the results show that T-DNA significantly improves domain adaptation performance based on a generic pre-trained model.

- Experimental results demonstrate that T-DNA significantly improves domain adaptation performance based on a generic pre-trained model and outperforms all baselines on eight classification tasks (on eight datasets).




# [The Art of Abstention: Selective Prediction and Error Regularization for Natural Language Processing](https://aclanthology.org/2021.acl-long.84/)
- In this paper, we introduce the problem of selective prediction for NLP.

- In this part, we show that our simple regularization trick improves selective prediction performance.

- We provide theoretical background and evaluation metrics for the problem, and also propose a simple error regularization method that improves selective prediction performance for NLP models.

- We also propose a simple trick, error regularization, which can be applied to any of these models and confidence estimators, and improve their selective prediction performance.




# [Learning Prototypical Functions for Physical Artifacts](https://aclanthology.org/2021.acl-long.540/)
- Our work uses a subset of frames from FrameNet to represent the prototypical functions for human-made physical artifacts.

- We explored several approaches for learning the prototypical functions of human-made physical artifacts.

- This paper focuses on the specific task of learning the prototypical functions for human-made physical artifacts using a subset of FrameNet frames as the set of function types.

- We introduced the new task of learning prototypical functions for human-made physical artifacts, and 7 In fact, snowplow can also refer to a skiing action, although WordNet does not contain that word sense.




# [Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger](https://aclanthology.org/2021.acl-long.37/)
- Textual backdoor attacks are much less investigated.

- In this paper, we propose to use the syntactic structure as the trigger of textual backdoor attacks for the first time.

- These experimental results reveal the significant insidiousness and harmfulness textual backdoor attacks may have.

- Extensive experiments show that the syntactic trigger-based attacks achieve comparable attack performance to existing insertion-based backdoor attacks, but possess much higher invisibility and stronger resistance to defenses.




# [Dialogue Response Selection with Hierarchical Curriculum Learning](https://aclanthology.org/2021.acl-long.137/)
- Dialogue Response Selection.

- We propose a hierarchical curriculum learning (HCL) framework for training neural matching models.

- In this work, we propose a novel hierarchical curriculum learning framework for training response selection models for multi-turn conversations.

- Our learning framework jointly employs the corpus-level and instance-level curriculum.




# [Evaluating Evaluation Measures for Ordinal Classification and Ordinal Quantification](https://aclanthology.org/2021.acl-long.214/)
- We conducted extensive evaluations of nine measures in the context of OC tasks and six measures in the context of OQ tasks, using data from SemEval and NTCIR.

- In the present study, we use data from the SemEval and NTCIR communities to clarify the properties of nine evaluation measures in the context of OC tasks, and six measures in the context of OQ tasks.

- Table 4 summarises the properties of the nine measures we examined in the context of OC tasks.

- Surprisingly, however, there are only a small number of known evaluation measures that meet this requirement.




# [Generating Landmark Navigation Instructions from Maps as a Graph-to-Text Problem](https://aclanthology.org/2021.acl-long.41/)
- The main contributions of this paper are: • We collect and publish a large scale dataset of natural language landmark navigation instructions that are validated by human navigation runs in Street View.

- We presented a dataset and suitable graph-to-text architecture to generate landmark navigation instructions in natural language from OpenStreetMap geographical data.

- • We present a method to represent geospatial routes as a graph and propose an appropriate graph-to-text architecture that learns to generate navigation instructions from real-world data.

- We present a neural model that takes a real-world map repre-sentation from OpenStreetMap 1 as input and generates navigation instructions that contain salient landmarks, learned directly from human natural language instructions.




# [The Limitations of Limited Context for Constituency Parsing](https://aclanthology.org/2021.acl-long.208/)
- To formalize our results, we con-sider the well-established sandbox of probabilistic context-free grammars (PCFGs).

- In this section, we formalize the results characterizing the representational power of the ON-LSTM architecture.

- On the flipside, we show that restricting the context even mildly can considerably decrease the representational power.

- We consider several neural architectures that have shown success in various syntactic tasks, most notably unsupervised constituency parsing and syntax-aware language modeling.




# [ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer](https://aclanthology.org/2021.acl-long.393/)
- We name our approach ConSERT, a Contrastive Framework for SEntence Representation Transfer.

- In this paper, we propose ConSERT, a selfsupervised contrastive learning framework for transferring sentence representations to downstream tasks.

- In this section, we present ConSERT for sentence representation transfer.

- This work proposes a contrastive learning based framework to solve the collapse issue of BERT and transfer BERT sentence representations to target data distribution.




# [Modeling Language Usage and Listener Engagement in Podcasts](https://aclanthology.org/2021.acl-long.52/)
- The BERT classifiers achieve nearly 81% accuracy, indicating that podcast content is highly predictive of engagement.

- We also show that the overall textual information in podcasts is highly predictive of engagement in this experiment, with an accuracy as high as 81%.

- This paper presents the first quantitative analysis of how linguistic style and textual attributes in podcasts relate to listener engagement using automatically computed features.

- We perform a series of descriptive tests to examine differences in language usage between high and low engagement podcasts, and build predictive models.




# [Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation](https://aclanthology.org/2021.acl-long.221/)
- • We propose an uncertainty-based sampling strategy for self-training, which selects more complementary sentences for the authentic parallel data.

- Inspired by the above finding, we propose an uncertainty-based sampling strategy for selftraining, in which monolingual sentences with higher uncertainty would be selected with higher probability ( §3.1).

- In this work, we demonstrate the necessity of distinguishing monolingual sentences for self-training in NMT, and propose an uncertainty-based sampling strategy to sample monolingual data.

- In this section, we introduced the uncertainty-based sampling strategy for self-training and the overall framework.




# [Bootstrapped Unsupervised Sentence Representation Learning](https://aclanthology.org/2021.acl-long.402/)
- It achieves state-of-the-art results on multiple semantic textual similarity (STS) tasks.

- The experimental results demonstrate that our method could significantly outperform the state-of-the-art unsupervised methods and it can be further extended for learning multilingual sentence representations.

- In this paper, we propose BSL for unsupervised sentence representation learning.

- We further extend our method for learning multilingual sentence representations and demonstrate that it is able to outperform strong multilingual baselines on cross-lingual STS tasks under both unsupervised and supervised settings.




# [Learning Event Graph Knowledge for Abductive Reasoning](https://aclanthology.org/2021.acl-long.403/)
- (c) A latent variable z is employed to learn the commonsense knowledge from event graph.

- To facilitate this,  proposed a text based abductive reasoning task αNLI.

- In this paper, we employ event graph knowledge for guiding the abductive reasoning.

- To this end, we propose a variational autoencoder based framework ege-RoBERTa, which employs a latent variable z to implicitly capture the necessary event graph knowledge and enhance the pretrained language model RoBERTa.




# [PhotoChat: A Human-Human Dialogue Dataset with Photo Sharing Behavior for Joint Image-Text Modeling](https://aclanthology.org/2021.acl-long.479/)
- In summary, our main contributions are: • We create the first human-human dialogue with photo sharing acts via crowd-sourcing.

- The best photo-sharing intent prediction baseline model achieves 58.1% F1 score with 58.2% precision and 57.9% recall.

- The maximum sequence length of BERT, ALBERT, and T5 for the photo-sharing intent prediction task is 512.

- To facilitate research on building intelligent photo-suggest system, we have introduced two new challenging tasks that aim at improving the photo-sharing experience: photo-sharing intent prediction task and image retrieval task.




# [Examining the Inductive Bias of Neural Language Models with Artificial Languages](https://aclanthology.org/2021.acl-long.38/)
- We suggest that properly investigating the inductive biases of language models will likely require artificial languages.

- Thus, we offer a study investigating the inductive biases of language models through the construction of artificial languages.

- We propose a novel methodology for the investigation of the inductive bias of language models using the technique of creating carefully controlled artificial languages.

- In order to compare inductive biases across architectures, two neural architectures were tested: transformers and LSTMs.




# [Verb Knowledge Injection for Multilingual Event Processing](https://aclanthology.org/2021.acl-long.541/)
- Cleanliness of Verb Knowledge.

- We proposed an auxiliary pretraining task to inject VerbNet-and FrameNet-based lexical verb knowledge into dedicated verb adapter modules.

- Figure 1 illustrates our framework for injecting verb knowledge from VerbNet or FrameNet and leveraging it in downstream event processing tasks.

- Crucially, we showed that the benefits of the knowledge from resourcerich languages can be extended to other, resourceleaner languages through translation-based transfer of verb class/frame membership information.




# [HATECHECK: Functional Tests for Hate Speech Detection Models](https://aclanthology.org/2021.acl-long.4/)
- In this article, we introduced HATECHECK, a suite of functional tests for hate speech detection models.

- To enable more targeted diagnostic insights, we introduce HATECHECK, a suite of functional tests for hate speech detection models.

- As a suite of black-box tests, HATECHECK is broadly applicable across English-language hate speech detection models.

- 1 HATECHECK is broadly applicable across English-language hate speech detection models.




# [Towards Robustness of Text-to-SQL Models against Synonym Substitution](https://aclanthology.org/2021.acl-long.195/)
- We present two categories of approaches for improving model robustness to synonym substitution.

- In short, we make the following contributions: • We conduct a comprehensive study to evaluate the robustness of text-to-SQL models against synonym substitution.

- cluding text-to-SQL translation.

- Text-to-SQL translation.




# [Assessing the Representations of Idiomaticity in Vector Models with a Noun Compound Dataset Labeled at Type and Token Levels](https://aclanthology.org/2021.acl-long.212/)
- This section displays some of the comparative analyses for the relevance of type and token annotation for idiomaticity detection.

- In this paper we take advantage of the NCTTI dataset to observe whether vector representations obtained with different strategies correlate with human annotations at both type and token levels.

- These results indicate at best weak (NC out Comp ) to moderate (NC out) correlations between models' predictions and human judgments, both at type and token levels.

- This paper presented the NCTTI, a dataset of NCs in English and Portuguese annotated at type and token level with human judgments about idiomaticity, and with suggestions of paraphrases.




# [UNIRE: A Unified Label Space for Entity Relation Extraction](https://aclanthology.org/2021.acl-long.19/)
- We propose a novel table that presents entities and relations as squares and rectangles.

- The joint decoder is set to find the best squares and rectangles.

- For example, entities and relations correspond to squares and rectangles in the table.

- First, filling the table is to predict each word pair's label, which is similar to arc prediction task in dependency parsing.




# [Deep Differential Amplifier for Extractive Summarization](https://aclanthology.org/2021.acl-long.31/)
- Our contributions in this work are concluded as follows: • We propose a novel conceptualization of extractive summarization as rebalance problem.

- To rebalance the bias of minority 1-class and majority 0-class, we have built a deep differential amplifier to amplify and capture the unique information for summary sentences.

- Specifically, we calculate and amplify the semantic difference between each sentence and other sentences, by the subtraction operation.

- Besides, another heuristic method is to make our model pay more attention to 1-class: a weighted cross-entropy function.




# [Measure and Evaluation of Semantic Divergence across Two Languages](https://aclanthology.org/2021.acl-long.100/)
- Semantic change across languages.

- BERT embeddings coupled with a clustering step lead to the best performance on synthetic corpora.

- We analyse the semantic divergence of wordtranslation pairs in a bilingual corpus of news articles.

- To do so, we define a set of monolingual and bilingual semantic change scenarios and evaluate our different approaches on them.




# [Mention Flags (MF): Constraining Transformer-based Text Generators](https://aclanthology.org/2021.acl-long.9/)
- These two signals allow the model to achieve high constraint satisfaction and help to maintain high text quality

- Mention Flags are a general mechanism that improves constraint satisfaction in the non-pre-trained and pre-trained S2S Transformer-based models.

- This section first formulates constrained text generation tasks, then introduces Mention Flags and their integration with Transformer-based text generators.

- This section shows that Mention Flags are still useful for improving the constraint satisfaction and generated text quality when trained with many fewer instances.




# [Learning to Perturb Word Embeddings for Out-of-distribution QA](https://aclanthology.org/2021.acl-long.434/)
- We proposed a simple yet effective data augmentation method based on a stochastic word embedding perturbation for out-of-distribution QA tasks.

- • We propose a simple yet effective data augmentation method to improve the generalization performance of pretrained language models for QA tasks.

- Specifically, our stochastic noise generator learns to generate the adaptive noise depending on the contextualized embedding of each word.

- To address such limitations of the existing data augmentation techniques for QA, we propose a novel DA method based on learnable word-level perturbation, which effectively regularizes the model to improve its generalization to unseen questions and contexts with distributional shifts.




# [Every Bite Is an Experience: Key Point Analysis of Business Reviews](https://aclanthology.org/2021.acl-long.262/)
- Aspect-based sentiment summarization.

- Overall, this work makes a dual contribution: first, it proposes a new framework for review summarization.

- Previous work on review summarization was dominated by two paradigms: aspect-based sentiment summarization and multi-document opinion summarization.

- The resulting summary provides both textual and quantitative   views of the data, as illustrated in Table 1.




# [Language Model Augmented Relevance Score](https://aclanthology.org/2021.acl-long.521/)
- In this paper, we present MARS (Language Model Augmented Relevance Score), a new NLG evaluation metric that requires neither supervision from human ratings nor additional training on specific domains.

- We have proposed MARS, a context-aware and easy-to-deploy NLG metric built upon an off-theshelf language model (GPT-2).

- Second, MARS is context-aware.

- off-the-shelf




# [EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets](https://aclanthology.org/2021.acl-long.171/)
- Our work makes the first attempt of introducing LTH to both efficient pre-training and efficient fine-tuning of BERT.

- Inspired by this, we set out to explore whether there are structured winning tickets in the early stage of BERT training that can significantly accelerate language model pre-training and fine-tuning.

- By instead using network slimming (Liu et al., 2017) on the self-attention and fully-connected sub-layers inside a transformer, we are the first to introduce an effective approach that can identify structured winning tickets in the early stage of BERT training, that are successfully applied for efficient language modeling pre-training and fine-tuning.

- In this paper, we present EarlyBERT, an efficient framework for large-scale language model pretraining and fine-tuning.




# [Risk Minimization for Zero-shot Sequence Labeling](https://aclanthology.org/2021.acl-long.380/)
- In this paper, we propose two approaches to the zero-shot sequence labeling problem.

- It suggests that our LVM approach learns the relations between predicted labels from the source models and true labels better than MRT.

- We design a new decomposable risk function parameterized by a fixed matrix that models the relations between the noisy predictions from the source models and the true labels.

- Meanwhile, introducing uncertainties for the relations between the predicted labels from the source models and the true labels in both training and prediction processes significantly benefit our approaches.




# [Discovering Dialogue Slots with Weak Supervision](https://aclanthology.org/2021.acl-long.189/)
- To verify the usefulness of the labels discovered by our method, we use them to train and evaluate an end-to-end task-oriented dialogue system.

- We achieve state-of-the-art results for slot tagging without manual supervision in four different domains, with a 6-16% absolute F1 score increase over the previous benchmark.

- Based on the discovered slots, we train a slot tagger to annotate in-domain utterances.

- We train an end-to-end neural dialogue system using our automatically discovered slots in the restaurant domain and demonstrate that our approach improves performance over an unsupervised model, finding the correct venue in 5% more cases (35% more when no restaurant ontology is provided).




# [Surprisal Estimators for Human Reading Times Need Character Models](https://aclanthology.org/2021.acl-long.290/)
- This paper presents a character model that can be used to estimate word generation probabilities in a structural parser-based processing model.

- To answer this question, this paper presents a character model that can be used to estimate word generation probabilities in a structural parser-based processing model.

- Regression analyses on self-paced reading, eye-tracking, and fMRI data demonstrate that surprisal estimates calculated from this character-based structural processing model contribute to substantially better fits compared to those calculated from large-scale language models, despite the fact that these other models are trained on much more data and show lower perplexities on test data.

- Furthermore, it suggests that the character-based structural processing model may provide a more humanlike account of processing difficulty and may suggest a larger role of morphology, phonotactics, and orthographic complexity than was previously thought.




# [AdaTag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding](https://aclanthology.org/2021.acl-long.362/)
- We propose to generate a decoder on the fly for each attribute based on its embedding.

- Incorporated with pretrained attribute embeddings, our model shows marked improvements over previous methods.

- But we utilize an adaptive decoding approach, where the decoding network is parameterized with the attribute embedding.

- In this paper we address the limitations of the existing contribution lines, through adaptive decoder parameterization.




# [Exploiting Document Structures and Cluster Consistencies for Event Coreference Resolution](https://aclanthology.org/2021.acl-long.374/)
- We present a novel end-to-end coreference resolution framework for event mentions based on deep learning.

- Compared to previous deep learning works for ECR, our model presents a novel representation learning framework based on document structures to explicitly encode important interactions between relevant objects, and representation regularization to exploit the cluster consistency between golden and predicted clusters for event mentions.

- Second, several regularization techniques are proposed to exploit the consistencies between human-provided and machine-generated clusters of event mentions in documents.

- The experiments demonstrate the benefits of the proposed methods and lead to state-of-the-art performance for ECR.




# [How to Adapt Your Pretrained Multilingual Model to 1600 Languages](https://aclanthology.org/2021.acl-long.351/)
- Pretrained multilingual models (PMMs) are a straightforward way to enable zero-shot learning via cross-lingual transfer, thus eliminating the need for labeled data for the target task and language.

- We further observe that in our setting the simplest adaptation method, continued pretraining, performs best for both tasks, achieving gains of up to 17.69% accuracy for POS tagging, and 6.29 F1 for NER

- Using only the New Testament, we show that continued pretraining is the best per-forming adaptation approach, leading to gains of 6.29 F1 on NER and 17.69% accuracy on POS tagging.

- Similarly, translation-based approaches, as well as few-shot learning may offer additional benefits over a zero-shot setting.




# [Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence](https://aclanthology.org/2021.acl-long.499/)
- We summarize our contributions in two folds: I. We propose a generation model named HINT for long text generation.

- In this paper, we propose HINT, a generation model equipped with HIgh-level representations for loNg Text generation.

- Results show that HINT can learn meaningful high-level representations and generate more coherent long texts than baselines.

- HINT can generate more coherent stories than baselines.




# [Energy-Based Reranking: Improving Neural Machine Translation Using Energy-Based Models](https://aclanthology.org/2021.acl-long.349/)
- We introduce energy-based re-ranking (EBR) to improve the performance of autoregressive neural machine translation.

- The former results in the effective maximum likelihood estimation (MLE) for training the parameters of NMT models.

- Autoregressive models are widely used for neural machine translation (NMT) (Bahdanau et al., 2015;Gehring et al., 2017;Vaswani et al., 2017).

- Finally, as an upper bound for the best achievable result, we also extract the translations from the sample that are closest to the gold data (based on BLEU score).




# [On Sample Based Explanation Methods for NLP: Efficiency, Faithfulness, and Semantic Evaluation](https://aclanthology.org/2021.acl-long.419/)
- A benchmark of evaluating sample-based explanation methods has not been agreed upon.

- Our contributions are: 1. We propose a new explanation framework that can use arbitrary explanation units as explanations and be Hessian-free and faithful at the same time; 2. A new metric to measure the semantic relatedness between a test instance and its explanation for BERT-based deep models.

- The new method allows for arbitrary text spans as the explanation unit and is Hessian-free while being faithful to the final model.

- In this work, we discuss faithfulness in the sample-based explanations framework.




# [Alignment Rationale for Natural Language Inference](https://aclanthology.org/2021.acl-long.417/)
- To resolve above problems, this paper proposes AREC, a post-hoc local approach to generate Alignment Rationale Explanation for Co-attention based models.

- Our contributions are summarized as follows: 1) We come up with AREC, a post-hoc local explanation method to extract the alignment rationale for co-attention based models.

- As mentioned before, AREC is a post-hoc approach for explaining co-attention based models.

- Experimental results show that our method could generate more faithful and readable explanations.




# [DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue](https://aclanthology.org/2021.acl-long.439/)
- We have introduced DVD, a diagnostic dataset signed to analyze video-grounded dialogue systems.

- To address the limitations of existing benchmarks and analyze dialogue systems more efficiently, we propose DVD, a Diagnostic Dataset for Video-grounded Dialogues.

- VIT is a new reasoning requirement in video-grounded dialogue tasks.

- We designed our experiments to systematically analyze model capabilities and shortcomings through unique challenges in video-grounded dialogue systems.




# [A Knowledge-Guided Framework for Frame Identification](https://aclanthology.org/2021.acl-long.407/)
- Thus, in this paper, we propose a Knowledge Guided Frame Identification framework (KGFI) which consists of a Bert-based context encoder and a frame encoder based on a specialized graph convolutional network (FrameGC-N).

- In particular, the frame encoder incorporates multiple types of frame knowledge into frame representation which guides the KGFI to jointly map target words and frames into the same embedding space.

- Frame Identification (FI) is the task of predicting a frame evoked by the target word in a sentence.

- Frame Identification (FI) aims to find the exact frame evoked by a target word in a given sentence.




# [Pre-training Universal Language Representation](https://aclanthology.org/2021.acl-long.398/)
- Furthermore, we conduct experiments on a wide range of downstream tasks from the GLUE benchmark and a question answering task.

- This work formally introduces universal language representation learning to enable unified vector operations among different language hierarchies.

- A universal language representation model encodes linguistic units such as words, phrases or sentences into fixed-sized vectors and handles multiple layers of linguistic objects in a unified way.

- ULR-BERT Our universal language representation model trained on Wikipedia with MLM and MiSAD.




# [Contributions of Transformer Attention Heads in Multi-and Cross-lingual Tasks](https://aclanthology.org/2021.acl-long.152/)
- In this paper, we explore the roles of attention heads in cross-lingual and multi-lingual tasks for two reasons.

- This paper studied the contributions of attention heads in Transformer-based models.

- As Table 3 shows, pruning attention heads generally has positive effects on our cross-lingual and multi-lingual NER models.

- The contributions of this paper are three-fold: • We explore the roles of attention heads in multilingual Transformer-based models and find that pruning certain heads leads to comparable or better performance in cross-lingual and multilingual sequence labeling tasks.




# [Transition-based Bubble Parsing: Improvements on Coordination Structure Prediction](https://aclanthology.org/2021.acl-long.557/)
- Empirically, our bubble parsers achieve state-of-the-art results on the task of coordination structure prediction on two English datasets.

- Task and Evaluation We validate the utility of our transition-based parser using the task of coordination structure prediction.

- Experiments on the English Penn Treebank (PTB;  extended with coordination annotation  and the English GENIA treebank  demonstrate the effectiveness of our proposed transition-based bubble parsing on the task of coordination structure prediction.

- Our method achieves state-of-the-art performance on both datasets and improves accuracy on the subset of sentences exhibiting complex coordination structures.




# [Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models](https://aclanthology.org/2021.acl-long.333/)
- In this work, we formalized the relationship between self-attention and convolution.

- Our empirical results provide evidence for future research integrating convolutions and self-attention for NLP.

- Our findings provide a solid foundation from which to study convolutions and self-attention in language tasks.

- Finally, recent work has proved theoretical relationships between self-attention and convolution.




# [SENT: Sentence-level Distant Relation Extraction via Negative Training](https://aclanthology.org/2021.acl-long.484/)
- Based on NT, we propose SENT, a sentencelevel framework for distant RE.

- • We present a sentence-level framework, SENT, which includes a noise-filtering and a re-labeling strategy for re-fining distant data.

- To summarize the contribution of this work: • We propose the use of negative training for sentence-level distant RE, which greatly protects the model from noisy information.

- In this work, we propose the use of negative training (NT) (Kim et al., 2019) for distant RE.




# [Anonymisation Models for Text Data: State of the Art, Challenges and Future Directions](https://aclanthology.org/2021.acl-long.323/)
- The case study illustrates a number of issues facing current methods for text anonymisation.

- This position paper discussed a number of unresolved challenges in text anonymisation.

- The five annotators were researchers without previous experience in text anonymisation.

- In this paper, we review the core concepts underlying text anonymisation, and survey the approaches put forward to solve this task.




# [Structural Knowledge Distillation: Tractably Distilling Information for Structured Predictor](https://aclanthology.org/2021.acl-long.46/)
- 3. The teacher factorization produces more finegrained substructures than the student factorization.

- 2. The student factorization produces more finegrained substructures than the teacher factorization.

- We derive a factorized form of the structural KD objective and make it tractable to compute and optimize for many typical choices of teacher and student models.

- In this paper, we propose structural knowledge distillation, which transfers knowledge between structured prediction models.




# [A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters](https://aclanthology.org/2021.acl-long.447/)
- Zero-/Few-Shot Crosslingual Transfer.

- Variance of Few-Shot Transfer.

- We have presented an extensive study of few-shot crosslingual transfer.

- In this work, however, we demonstrate that the gains from few-shot transfer exhibit a high degree of sensitivity to the selection of few shots.




# [Shortformer: Better Language Modeling Using Shorter Inputs](https://aclanthology.org/2021.acl-long.427/)
- We then introduce new methods based on shorter input subsequences that improve runtime, memory efficiency, and perplexity.

- We propose a two-stage training routine that initially uses short input subsequences followed by long subsequences.

- as in the WikiText-103 models

- Finally, we show additive gains from combining staged training and position-infused attention (Shortformer, §6), resulting in a model that trains much quicker and achieves better perplexity on WikiText-103.




# [Breaking Down the Invisible Wall of Informal Fallacies in Online Discussions](https://aclanthology.org/2021.acl-long.53/)
- In this appendix, we review the most frequent fallacies on Reddit.

- In our work, we align frequent fallacies on Reddit with these rules, with the goal of formalizing their definitions.

- We find frequent fallacy mentions on Reddit and the subreddits in which they are the most prevalent.

- The remaining fallacies are selected for the creation of an annotated dataset of fallacies.




# [De-Confounded Variational Encoder-Decoder for Logical Table-to-Text Generation](https://aclanthology.org/2021.acl-long.430/)
- Logical Table-to-Text Generation.

- In this paper, we propose a de-confounded variational encoder-decoder for the logical table-to-text generation.

- In this paper, we view the logical table-totext generation from the perspective of causal inference and propose a de-confounded variational encoder-decoder (DCVED).

- And we apply the causal intervention method to reduce the spurious correlations.




# [Glancing Transformer for Non-Autoregressive Neural Machine Translation](https://aclanthology.org/2021.acl-long.155/)
- Based on GLM, we develop the glancing Transformer (GLAT) for neural machine translation.

- In this paper, we propose Glancing Transformer with a glancing language model to improve the performance of single-pass parallel generation models.

- The glancing Transformer (GLAT) formulates a glancing language model (GLM) during training.

- Experimental results show that our approach significantly improves the performance of non-autoregressive machine translation with single-pass parallel generation.




# [A Large-Scale Chinese Multimodal NER Dataset with Speech Clues](https://aclanthology.org/2021.acl-long.218/)
- • We further propose a multimodal multitask method by introducing a speech-to-text alignment auxiliary task.

- In this paper, we explore Chinese multimodal NER with both textual and acoustic contents.

- Furthermore, we propose a simple multimodal multitask method by introducing a speechto-text alignment auxiliary task.

- But differently from previous studies, we pay special attention to Chinese multimodal NER with both textual and acoustic contents.




# [Unsupervised Out-of-Domain Detection via Pre-trained Transformers](https://aclanthology.org/2021.acl-long.85/)
- To the best of our knowledge, our method is the first to incorporate transformers and pre-training techniques to improve out-of-domain detection.

- We explore two domain-specific fine-tuning approaches.

- We study the problem of detecting out-of-domain samples with unsupervised in-domain data, which is a more general setting for out-of-domain detection.

- Two domain-specific fine-tuning methods, IMLM and BCAD, can be further applied to BERT to boost detection accuracy.




# [Enhancing the generalization for Intent Classification and Out-of-Domain Detection in SLU](https://aclanthology.org/2021.acl-long.190/)
- The evaluation on four datasets shows that DRM can consistently improve upon previous state-of-the-art methods.

- Thus, we mainly focus on comparing our method with strong baselines with BERT and RoBERTa models.

- This structure is probabilistically motivated and empirically leads to a better generalization in both intent classification and OOD detection.

- Among all OOD detection approaches, our proposed L-Mahalanobis OOD detection approach achieves the best performance for both linear and DRM combined BERT and RoBERTa models.




# [Claim Matching Beyond English to Scale Global Fact-Checking](https://aclanthology.org/2021.acl-long.347/)
- Human fact-checking is high-quality but timeconsuming.

- We also compare our system with other state-of-the-art multilingual embedding models used for reranking, namely LASER and LaBSE.

- To construct a dataset for claim matching, we design a two-step sampling and annotation process.

- Scaling human-led fact-checking efforts requires matching messages with the same claims.




# [Improving Encoder by Auxiliary Supervision Tasks for Table-to-Text Generation](https://aclanthology.org/2021.acl-long.466/)
- Furthermore, we investigate the impact of the two auxiliary tasks on table-to-text generation.

- More specifically, two auxiliary tasks named Number Ranking (NR) and Importance Ranking (IR) are proposed to supervise the learning of the different parts of the Record Encoder, respectively.

- Furthermore, we utilize different auxiliary tasks to help the encoder capture the different relations among records.

- And then, we introduce a reasoning module to perform reasoning on the graph.




# [Enhancing Content Preservation in Text Style Transfer Using Reverse Attention and Conditional Layer Normalization](https://aclanthology.org/2021.acl-long.8/)
- In this paper, we propose a novel method for text style transfer.

- • Aside from building style-independent content representation, our approach utilizes conditional layer normalization to construct content-dependent style representation.

- Thus, unlike previous attempts in text style transfer, the style representation is dynamic respect to the content, being content-dependent embedding.

- The contributions are as follows: • We introduce reverse attention as a way to suppress style information while preserving content information when building a content representation of an input.




# [Integrated Directional Gradients: Feature Interaction Attribution for Neural NLP Models](https://aclanthology.org/2021.acl-long.71/)
- • Second, we state a set of axioms that a well behaved feature group attribution method should satisfy (Section 2.2).

- In this section we present a solution to the "feature group attribution problem" that we call the Integrated Directional Gradients method or IDG.

- • Third, we present the method of Integrated Directional Gradients or IDG as a solution to the feature group attribution problem that satisfies the stated axioms (Section 2.3).

- In this paper we investigated the problem of feature group attribution and proposed a set of axioms that any framework for feature group attribution should fulfill.




# [Exploring Distantly-Labeled Rationales in Neural Network Models](https://aclanthology.org/2021.acl-long.433/)
- Distantly-labeled rationale words may vary dramatically in quality.

- In practice, distantly-labeled rationales serve as a plausible alternative.

- Although distantly-labeled rationale words are often universally helpful, given a specific context, different rationale words may exhibit varied importance.

- We refer to the distantly-labeled rationale words that are not helpful in a specific instance as Non-Important Rationales, or NoIRs for short.




# [Selective Knowledge Distillation for Neural Machine Translation](https://aclanthology.org/2021.acl-long.504/)
- Experimental results show that our approach yields an improvement of +1.28 and + 0.89 BLEU points over the Transformer baseline.

- In summary, our contributions are as follows: • We propose a novel protocol for analyzing the property for the suitable medium samples for transferring teacher's knowledge.

- • We propose two selective strategies: batchlevel selection and global-level selection.

- To address this problem, we propose two simple yet effective strategies, namely the batch-level selection and global-level selection.




# [Verb Metaphor Detection via Contextual Relation Learning](https://aclanthology.org/2021.acl-long.327/)
- We propose the Metaphor-relation BERT (Mr-BERT) model to realize verb metaphor detection as a relation classification task.

- This paper presented the Metaphor-relation BERT (MrBERT) model for verb metaphor detection.

- We can see that MrBERT achieves superior or competitive performance compared with previous work on verb metaphor detection.

- As shown in Figure 1, we propose to formulate verb metaphor detection as a relation extraction problem, instead of token classification or sequence labeling formulations.




# [Accelerating Text Communication via Abbreviated Sentence Input](https://aclanthology.org/2021.acl-long.514/)
- Abbreviated input.

- We investigated abbreviation by omitting midword vowels.

- Finally, after practice, users wrote only slightly slower using sentence abbreviated input at 9.6 words-per-minute compared to a conventional keyboard with word predictions at 9.9 words-per-minute.

- We now describe how we used our optimized language models to recognize noisy abbreviated input.




# [CitationIE: Leveraging the Citation Graph for Scientific Information Extraction](https://aclanthology.org/2021.acl-long.59/)
- In this paper, we show citation graph embeddings can improve scientific information extraction.

- This leads to a sizable increase in the performance of the end-to-end CitationIE system relative to the current state-of-the-art, Jain et al. (2020).

- We break down the end-to-end information extraction process as a sequence of these four related tasks, with each task taking the output of the preceding tasks as input.

- The structure of the citation graph can contextualize a document within the greater body of work.




# [CHASE: A Large-Scale and Pragmatic Chinese Dataset for Cross-Database Context-Dependent Text-to-SQL](https://aclanthology.org/2021.acl-long.180/)
- • CHASE, to the best of our knowledge, is the first large-scale and pragmatic Chinese dataset for XDTS.

- Given the limitations of existing datasets, we present CHASE, a large-scale and pragmatic Chinese dataset for XDTS.

- Upon identifying the limitations of existing datasets, we present CHASE, a large-scale and pragmatic Chinese dataset for XDTS.

- This work presents CHASE, a free and open dataset for the research community to study the crossdatabase context-dependent Text-to-SQL problem (XDTS).




# [Measuring Conversational Uptake: A Case Study on Student-Teacher Interactions](https://aclanthology.org/2021.acl-long.130/)
- We introduce a framework for computationally measuring uptake.

- We propose a framework for measuring uptake, a core conversational phenomenon with particularly high relevance in teaching contexts.

- We release an annotated dataset and develop and compare unsupervised measures of uptake, demonstrating significant correlation with educational outcomes across three datasets.

- Now we introduce our main uptake measure, used to capture a broader range of uptake phenomena beyond repetition including, e.g., acknowledgment and question answering (Section 2).




# [StereoSet: Measuring stereotypical bias in pretrained language models](https://aclanthology.org/2021.acl-long.416/)
- We show that current pretrained language models exhibit strong stereotypical biases.

- StereoSet measures stereotypical biases in gender, profession, race, and religion.

- In this work, we assess the stereotypical biases of popular pretrained language models.

- In this work, we propose methods to evaluate stereotypical bias of pretrained language models.




# [Factorising Meaning and Form for Intent-Preserving Paraphrasing](https://aclanthology.org/2021.acl-long.112/)
- We present SEPARATOR, a method for generating paraphrases that balances high variation in surface form with strong intent preservation.

- In this paper, we propose SEPARATOR, a method for generating paraphrases that exhibit high variation in surface form while still retaining the original intent.

- SEPARATOR is able to generate question paraphrases with a better balance of diversity and intent preservation compared to prior work.

- Extensive experiments and a human evaluation show that our approach leads to separated encoding spaces with negligible loss of expressivity, and is able to generate paraphrases with a better balance of variation and semantic fidelity than prior methods.




# [Changes in European Solidarity Before and During COVID-19: Evidence from a Large Crowd-and Expert-Annotated Twitter Dataset](https://aclanthology.org/2021.acl-long.129/)
- (iii) We present novel empirical evidence regarding changes in European solidarity debates before and after the outbreak of the COVID-19 pandemic.

- In this paper, we contributed the first large-scale human and automatically annotated dataset labeled for solidarity and its contestation, anti-solidarity.

- 2 Definition of Social Solidarity.

- Against this background, we ask whether we can detect changes in the debates on European solidarity before and after the outbreak of COVID-19.




# [MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation](https://aclanthology.org/2021.acl-long.440/)
- In this paper, we propose an multimodal fused graph convolutional network (MMGCN) for multimodal emotion recognition in conversation (ERC).

- In order to capture the utterance-level contextual dependencies across multiple modalities, we propose a Multimodal fused Graph Convolutional Network (MMGCN).

- In order to effectively explore the multimodal information and at the same time capture longdistance contextual information, we propose a new multimodal fused graph convolutional network (MMGCN) model in this work.

- MMGCN provides a more effective way of utilizing both multimodal and long-distance contextual information.




# [Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis](https://aclanthology.org/2021.acl-long.26/)
- We proposed a new framework to automatically generate counterfactual augmented data (CAD) for enhancing the robustness of sentiment analysis models.

- We further show that our methods can achieve better performance even when compared to models trained with humangenerated counterfactuals.

- To the best of our knowledge, we are the first to automatically generate counterfactuals for use as augmented data to improve the robustness of neural classifiers, which can outperform existing, state-ofthe-art, human-in-the-loop approaches.

- It suggests that our method have its absolute advantage for data augmentation in sentiment analysis when compared to the state-of-theart style-transfer models.




# [Automated Concatenation of Embeddings for Structured Prediction](https://aclanthology.org/2021.acl-long.206/)
- In this paper, we propose Automated Concatenation of Embeddings, which automatically searches for better embedding concatenation for structured prediction tasks.

- Together with fine-tuned embeddings, ACE achieves state-of-the-art performance in 6 tasks over 21 datasets.

- In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks.

- Results show that ACE with fine-tuned embeddings achieves state-of-the-art performance in all test sets, which shows that finding a good embedding concatenation helps structured prediction tasks.




# [ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining](https://aclanthology.org/2021.acl-long.535/)
- Benchmarking Other Conversation Summarization Datasets We benchmark our models on widely used meeting summarization datasets.

- We believe that such benchmarking will facilitate a more straightforward comparison of conversation summarization models across domains.

- In addition to introducing manually-curated datasets for conversation summarization, we also aim to unify previous work in conversation summarization.

- As we propose novel conversation summarization datasets and modeling components, this section is divided into the following two parts.




# [Evaluation Examples Are Not Equally Informative: How Should That Change NLP Leaderboards?](https://aclanthology.org/2021.acl-long.346/)
- This paper advocates incorporating decades of research in crafting education tests to improve how we evaluate the capabilities of NLP models.

- Then we show that-like in educational testing-IRT identifies good and bad items.

- We propose and validate an alternate IRT ranking method for leaderboard evaluations, show it can guide annotation, detect annotation error, and naturally partition evaluation data.

- However, in educational testing-a field measuring skill and knowledge in humans-IRT is a primary measurement instrument (Hambleton, 1991, p. 2).




# [Can vectors read minds better than experts? Comparing data augmentation strategies for the automated scoring of children's mindreading ability](https://aclanthology.org/2021.acl-long.96/)
- We also showed that data augmentation improves the performance on unseen data.

- First, data augmentation can successfully be used to improve the performance of the systems on the MIND-CA corpus.

- Second, data augmentation also improves the performance of the automated systems on the unseen examples from UK-MIND-20.

- We demonstrated that data augmentation can improve the performance of automated systems including on novel, unseen data.




# [Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference](https://aclanthology.org/2021.acl-long.488/)
- In this paper, we introduce KECI (Knowledge-Enhanced Collective Inference), a novel end-to-end framework that utilizes external domain knowledge for joint entity and relation extraction.

- In this work, we propose a novel span-based framework named KECI that utilizes external domain knowledge for joint entity and relation extraction from biomedical text.

- For example, KECI achieves absolute improvements of 4.59% and 4.91% in F1 scores over the state-of-the-art on the BioRelEx entity and relation extraction tasks.

- Also, joint entity and relation extraction can be naturally formulated as the task of extracting a span graph from an input document .




# [Attention Calibration for Transformer in Neural Machine Translation](https://aclanthology.org/2021.acl-long.103/)
- • Detailed analyses show that calibrated attention weights are more uniform at lower layers while more focused at the higher layers.

- We further find a greater need for calibration in the original attention weights with high entropy.

- Highentropy attention weights are found to have great needs for calibration at all layers.

- The NMT model decodes with the calibrated attention weights.




# [All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text](https://aclanthology.org/2021.acl-long.565/)
- We found that evaluators were unable to distinguish between GPT3and human-authored text across story, news, and recipe domains.

- We found that untrained evaluators were unable to distinguish between human-and GPT3-generated text from three domains.

- 3 Can we train evaluators to better identify machine-generated text?

- While the accuracy of classifying GPT2-vs. human-authored text is significantly 7 different from chance, evaluators' accuracy distinguishing GPT3-and humanauthored text is not.




# [On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study](https://aclanthology.org/2021.acl-long.517/)
- In this paper, we conduct a large-scale randomized controlled study to address these questions.

- Results on an extensive collection of out-of-domain evaluation sets suggest that ADC training data does not offer clear benefits vis-à-vis robustness under distribution shift.

- Finally, we perform a qualitative analysis over the collected data, revealing profound differences with models in (versus out of) the loop.

- In this paper, we demonstrated that across a variety of models and datasets, training on adversarial data leads to better performance on evaluation sets created in a similar fashion, but tends to yield worse performance on out-of-domain evaluation sets not created adversarially.




# [Prosodic segmentation for parsing spoken dialogue](https://aclanthology.org/2021.acl-long.79/)
- Our primary contributions are: • We show that a parser that has access to prosody can perform both SU segmentation and parsing as well as a model that only has to parse.

- However, pitch and intensity features are extracted from the speech signal at the frame level.

- The turn-based parser's task is also more com- plex: it has to perform both SU segmentation and parsing, rather than parsing alone.

- We report the model's performance with and without prosodic features during the segmentation and parsing steps.




# [W-RST: Towards a Weighted RST-style Discourse Framework](https://aclanthology.org/2021.acl-long.302/)
- In particular, we find that automatically generated, weighted discourse trees can benefit key NLP downstream tasks.

- We further show that real-valued importance scores (at least partially) align with human annotations and can interestingly also capture uncertainty in human annotators, implying some alignment of the importance distributions with linguistic ambiguity.

- Based on this observation, we investigate the potential of replacing the binary nuclearity assessment postulated by RST with automatically generated, real-valued importance scores in a new, Weighted-RST framework.

- Crucially for our purposes, this approach internally generates real-valued importance-weights for trees.




# [Modeling Transitions of Focal Entities for Conversational Knowledge Base Question Answering](https://aclanthology.org/2021.acl-long.255/)
- (2) We propose a graphbased neural network model to capture the transitions of focal entities and derive a focal entity distribution that can be plugged into a standard single-turn KBQA system.

- In this paper, we present a method to model the transitions of focal entities in a conversation in order to improve conversational KBQA.

- Our work also intends to capture the flow of the conversation but we specifically model the transitions of focal entities.

- In this paper, we explicitly model the focal entities and their transitions in a conversation in order to improve conversational KBQA.




# [EnsLM: Ensemble Language Model for Data Diversity by Semantic Clustering](https://aclanthology.org/2021.acl-long.230/)
- Inspired by them, in order to jointly consider topic learning and sample clustering, we propose the autoencoding topic model with mixture prior (mATM).

- Guided by clustering assignments that describe the data diversity, EnsLM learns both shared and cluster-specific knowledge by weight modulations.

- To jointly consider the clustering and topic modeling for better clustering (as shown in Fig. 1b) and for joint training with the following LM, we firstly introduce an autoencoding topic model with mixture priors (mATM).

- Our proposed mATM and EnsLM enjoy the following distinguished properties: • The mATM learns the mixture-prior latent semantic space to define a soft clustering assignment for each sample.




# [Learning Latent Structures for Cross Action Phrase Relations in Wet Lab Protocols](https://aclanthology.org/2021.acl-long.525/)
- This paper is organized around two main contributions: (i): the WLP-MSTG Corpus that extends the WLP Corpus (Kulkarni et al., 2018) by including intra-and cross-sentence temporal and causal relationships and (ii): a novel model that builds upon latent structures to resolve implicit arguments and long-range relations spanning multiple sentences.

- We also report significant improvements in understanding implicit arguments and identifying long range relationships across multiple sentences.

- This analysis provides valuable insight about the challenges in the form of long-range relations and implicit arguments that are present in extracting MSTGs from WLPs.

- We develop a latent structure model for jointly learning entity and relations within and across multiple sentences.




# [Semi-Supervised Text Classification with Balanced Deep Representation Distributions](https://aclanthology.org/2021.acl-long.391/)
- To alleviate the aforementioned problem, we propose a novel SSTC method built on BERT with AM loss, namely Semi-Supervised Text Classification with Balanced Deep representation Distributions (S 2 TC-BDD).

- 3 The Proposed S 2 TC-BDD Method In this section, we describe the proposed deep selftraining SSTC method, namely Semi-Supervised Text Classification with Balanced Deep representation Distributions (S 2 TC-BDD).

- We can estimate each label angle variance over both labeled and pseudo-labeled texts during the self-training loops.

- In particular, we develop a Balanced Deep representation Distribution (BDD) loss, aiming at more accurate pseudo-labels for unlabeled texts.




# [CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding](https://aclanthology.org/2021.acl-long.181/)
- To train a robust semantic-aware PLM, we propose Contrastive Learning with semantIc Negative Examples (CLINE).

- Thus, it is crucial to explore the appropriate methods to learn changed semantics from semantic negative examples.

- 3) By constructing positive and negative examples for contrastive learning in pre-training stage, our method CLINE-B and CLINE-R learn better sentence representation and detect small semantic changes.

- introduce a token-level perturbation to improves the robustness of PLMs.




# [An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models](https://aclanthology.org/2021.acl-long.178/)
- To bridge this gap, in this paper, we propose an experimental study for fine-tuning pre-trained language models using the HuggingFace library.

- The main contributions of this work are: • We empirically study the performance of three HPO methods on two pre-trained language models and on the GLUE benchmark; • We design an experimental procedure which proves useful to systematically troubleshoot the failures in HPO for fine-tuning; • We report and analyze the execution results of the experimental procedure, which sheds light on future work; 2 Definition of HPO on Language Model Fine-Tuning Given a pre-trained language model, a fine-tuning task, and a dataset containing D train , D val , D test

- With these observations, we propose two general strategies for troubleshooting the failure cases in HPO as well as an overall experimental procedure (Figure 1).

- First, can automated HPO methods outperform grid search?




# [Neural Machine Translation with Monolingual Translation Memory *](https://aclanthology.org/2021.acl-long.567/)
- (3) Our model gains a strong cross-domain transferability by hot-swapping domain-specific monolingual memory.

- (2) Our model can substantially boost translation quality in low-resource scenarios by utilizing extra monolingual TM that is not present in training pairs.

- TM-augmented NMT This work contributes primarily to the research line of Translation Memory (TM) augmented Neural Machine Translation (NMT).

- Then in §3.2, we describe the model design for the cross-lingual memory retrieval model.




# [Directed Acyclic Graph Network for Conversational Emotion Recognition](https://aclanthology.org/2021.acl-long.123/)
- We design a directed acyclic graph (DAG) to model the information propagation in a conversation.

- Secondly, inspired by DAGNN (Thost and Chen, 2021), we propose a directed acyclic graph neural network for ERC, namely DAG-ERC.

- In this paper, we presented a new idea of modeling conversation context with a directed acyclic graph (DAG) and proposed a directed acyclic graph neural network, namely DAG-ERC, for emotion recognition in conversation (ERC).

- In this section, we introduce the proposed Directed Acyclic Graph Neural Network for ERC (DAG-ERC).




# [Beyond Sentence-Level End-to-End Speech Translation: Context Helps](https://aclanthology.org/2021.acl-long.200/)
- Our experiments confirm the effectiveness of context-aware modeling for end-to-end speech translation.

- Context-aware ST improves general translation quality in BLEU, and also helps pronoun and homophone translation.

- • Contextual modeling improves translation quality and reduces latency and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a).

- In this paper, we answer this question through extensive experiments by exploring a concatenation-based context-aware ST model.




# [Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model](https://aclanthology.org/2021.acl-long.50/)
- In this exploratory study, we present a computational implementation of the Stereotype Content Model to better understand and counter stereotypes in text.

- Thus, our contributions are as follows: • To develop a computational method for automatically mapping textual information to the warmthcompetence plane as proposed in the Stereotype Content Model.

- We then discuss how we might use information about warmth and competence to generate anti-stereotypes with the specific goal of reducing biased thinking.

- Countering stereotypes through exposure to anti-stereotypical exemplars is based on a similar idea of deconstructing harmful beliefs with counter-facts.




# [DEXPERTS: Decoding-Time Controlled Text Generation with Experts and Anti-Experts](https://aclanthology.org/2021.acl-long.522/)
- We propose DEXPERTS, 1 a decoding-time method for controlled text generation based on a product of experts (Hinton, 2002).

- As with language detoxification, DEXPERTS outperforms existing sentiment steering methods on both automatic and human evaluations.

- Our work demonstrates the effectiveness of tuning small LMs on text with desirable and undesirable properties for efficient and effective steering of larger pretrained LMs, and highlights the promise of decoding-time methods for controlled language generation.

- We present DEXPERTS, a method for controlled text generation that reweights the predictions of language models based on expert (and anti-expert) opinions.




# [Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation](https://aclanthology.org/2021.acl-long.473/)
- Then, we propose a relationaware multi-document encoder that relates multiple input documents in a relation graph.

- Most of the previous related work section generation methods are extractive.

- Related Work Generation.

- In this section, we introduce the Relation-aware Related work Generator (RRG) in detail.




# [Early Detection of Sexual Predators in Chats](https://aclanthology.org/2021.acl-long.386/)
- We introduce the task of early sexual predator detection (eSPD) in chats.

- We defined the problem of early sexual predator detection (eSPD) in online chats and proposed an evaluation setup for this task.

- We hope that making our task setup accessible to the research community will encourage more research into the highly important topic of early sexual predator detection.

- New state of the art on SPD.




# [Evaluating morphological typology in zero-shot cross-lingual transfer](https://aclanthology.org/2021.acl-long.244/)
- We have compared performance of two state-of-the-art zero-shot cross-lingual models on two tasks (part-of-speech tagging and sentiment analysis) for 19 languages across four morphological typologies.

- Our results show that POS tagging is more sensitive to morphological typology than sentiment analysis and that the models perform much better on fusional languages, such as German, than on the other typologies.

- We fine-tune the models for part-of-speech tagging and sentiment analysis on 19 languages from four morphologically diverse typologies.

- Namely, to what degree do other variables contribute to effects on cross-lingual transfer.




# [MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation](https://aclanthology.org/2021.acl-long.86/)
- Specifically, our work makes the following contributions: • We present a text-based adversarial algorithm, MATE-KD, which increases the accuracy of the student model using KD.

- We have presented MATE-KD, a novel text-based adversarial training algorithm which improves the student model in KD by generating adversarial examples while accessing the logits of the teacher only.

- Our model is able to achieve stateof-the-art results for a 6 layer transformer model on the GLUE leaderboard.

- We observe that MATE-KD improves the baseline performance on both evaluation datasets.




# [Multilingual Speech Translation from Efficient Finetuning of Pretrained Models](https://aclanthology.org/2021.acl-long.68/)
- Our contributions are as follows: • We propose a simple and effective approach to combine pretrained single-modality modules to perform speech-to-text translation.

- • We present an efficient transfer learning strategy by only finetuning the LayerNorm and Attention (LNA) parameters of pretrained models.

- • Our approach is also effective for zero-shot multilingual translation (train on A → B and B → C, test on A → C), which provides an efficient approach for many-to-many speechto-text translation without dependency for parallel data for every direction.

- We evaluate our proposed models on two largescale multilingual speech translation benchmarks.




# [Obtaining Better Static Word Embeddings Using Contextual Embedding Models](https://aclanthology.org/2021.acl-long.408/)
- The resulting embeddings can also be used as a task-agnostic tool to measure the lexical information conveyed by contextual embedding models and allow a fair comparison with their static analogues.

- Our resulting embeddings outperform the current static embedding methods, as well as the current state-of-the-art static embedding distillation method on both unsupervised lexical similarity tasks as well as on downstream supervised tasks, by a significant margin.

- To ensure a fair comparison, we also evaluate SENT2VEC, CBOW and SKIPGRAM models that were trained on the same corpus.

- Our proposed distillation procedure is inspired by existing CBOW-based static word embedding algorithms, but during training plugs in any existing contextual representation to serve as the context element of each word.




# [An End-to-End Progressive Multi-Task Learning Framework for Medical Named Entity Recognition and Normalization](https://aclanthology.org/2021.acl-long.485/)
- To capture the semantic features of two tasks,  proposed a multi-task learning framework with an explicit feedback strategy for medical NER and NEN.

- Based on this idea, we propose an end-to-end progressive multi-task learning framework for medical named entity recognition and normalization (E2EMERN 1 ).

- In this paper, we reconsider the process of NER and NEN and propose the end-to-end progressive multitask learning framework for medical named entity recognition and normalization.

- And to learn the joint probability distribution of the NER and NEN tasks, a semi-markov based model was proposed by .




# [SPANNER: Named Entity Re-/Recognition as Span Prediction](https://aclanthology.org/2021.acl-long.558/)
- We first investigate what strengths and weaknesses are when NER is conceptualized as a span prediction task.

- Setup To explore how different mechanisms influence the performance of span prediction models

- In other words, the span prediction model play two roles showing in Fig. 1:

- Secondly, we reveal the unique advantage brought by the architectural bias of the span prediction framework: it can not only be used as a base system for named entity recognition but also serve as a meta-system to combine multiple NER systems' outputs.




# [Supporting Cognitive and Emotional Empathic Writing of Students](https://aclanthology.org/2021.acl-long.314/)
- We introduce a novel empathy annotation scheme and an annotated corpus of student-written peer reviews extracted from a real-world learning scenario.

- We gathered a corpus of 500 student-generated peer reviews written in German.

- Our corpus consisted of 500 student-written peer reviews that were annotated for review components and their emotional and cognitive empathy levels.

- Hence, we propose a new annotation scheme to model peer review components and their emotional and cognitive empathy levels that reflect the feedback discourse in peer review texts.




# [Element Intervention for Open Relation Extraction](https://aclanthology.org/2021.acl-long.361/)
- In this paper, we revisit the procedure of OpenRE from a causal view.

- We also provide two specific implementations of the interventions based on entity ranking and context contrasting.

- By formulating OpenRE using a structural causal model, we identify the cause of the above-mentioned problems, and alleviate the problems by Element Intervention.

- In this paper, we attempt to explain and resolve the above-mentioned problem in OpenRE from a causal view.




# [Neural semi-Markov CRF for Monolingual Word Alignment](https://aclanthology.org/2021.acl-long.531/)
- In this paper, we present a novel neural semi-Markov CRF alignment model, which unifies both word and phrase alignments though variablelength spans, calculates span-based semantic similarities, and takes alignment label transitions into consideration.

- In this work, we present the first neural semi-CRF word alignment model which achieves competitive performance on both in-domain and outof-domain evaluations.

- Finally, we demonstrate the utility of monolingual word alignment in two downstream applications, namely automatic text simplification and sentence pair classification.

- Our model exceeds 90% F1 in the in-domain evaluation and also has very good generalizability on three out-of-domain datasets.




# [Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets](https://aclanthology.org/2021.acl-long.81/)
- Rather, by applying a measurement modeling lens, our goal is to provide a constructive scaffolding for reasoning through and articulating the challenges of constructing and using such benchmarks.

- Measurement Modeling We apply a measurement modeling lens by viewing each benchmark as a measurement model (MM) (e.g., Quinn et al., 2010;Jacobs and Wallach, 2021).

- Using the measurement modeling lens, we investigate what each benchmark dataset measures (the construct) and how each dataset measures it (the operationalization of the construct).

- We inventory a range of pitfalls ( §4)including unstated assumptions, ambiguities, and inconsistencies-surrounding the conceptualization and operationalization of stereotyping implied by both the individual tests (pairs of contrastive sentences) and their construction.




# [PASS: Perturb-and-Select Summarizer for Product Reviews](https://aclanthology.org/2021.acl-long.30/)
- Finally, we show that the resulting PASS system, outperforms SOTA models in the domain of product reviews in terms of informativeness, CP-Diversity and coherence.

- 3 Perturb-and-Select Summarizer In this section, we propose a system that employs a large pre-trained Transformer-based model (T5) in a few-shot fine-tuning scheme for multiple reviews abstractive summarization.

- Our proposed method starts by fine-tuning a strong pre-trained language model for product reviews summarization in a few-shot setup.

- In summary, the main contributions of this work are: (1) highlight two shortcomings of existing product reviews summarizers, namely low CP-Diversity and self-inconsistency, and propose a dedicated metric for the former.




# [OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics](https://aclanthology.org/2021.acl-long.500/)
- We present OpenMEVA, a benchmark to comprehensively assess capabilities of metrics for evaluating open-ended story generation.

- Therefore, we propose OpenMEVA, a benchmark for Open-ended story generation Metrics Evaluation.

- We assess the ability of the unreferenced metrics 8 to judge story coherence based on the discrimination test set of AUTOS.

- We conduct extensive experiments to assess the capabilities of existing automatic metrics on Open-MEVA.




# [Making Pre-trained Language Models Better Few-shot Learners](https://aclanthology.org/2021.acl-long.295/)
- Few-shot learning.

- We refer to our approach as LM-BFF, better few-shot fine-tuning of language models: a strong, taskagnostic method for few-shot learning.

- We present a systematic evaluation for analyzing few-shot performance on 8 single-sentence and 7 sentence-pair NLP tasks.

- Fine-tuning of language models.




# [AUGNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation](https://aclanthology.org/2021.acl-long.95/)
- In this paper, we proposed AUGNLG, a novel data augmentation approach that combines a self-trained retrieval model with a few-shot learned NLU, to automatically create MR-to-Text data from opendomain texts.

- Our approach, in contrast, generates MR-to-Text data by jointly employing a self-trained neural retrieval model with a few-shot learned NLU model.

- In order to go beyond this restriction, this paper proposes AUGNLG, a novel data augmentation approach, that automatically creates MR-to-Text data from open-domain texts by combining a self-trained neural retrieval model with a few-shot learned NLU model.

- To this end, we build a few-shot NLU model by fine-tuning a BERT model with in-domain groundtruth data.




# [CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes](https://aclanthology.org/2021.acl-long.109/)
- Similar to prior work on multi-aspect extractive summarization, we employ sentencelevel multi-label classification techniques (Hayashi et al., 2020).

- We evaluated BERT-based models that incorporate multi-sentence context, and introduced a novel task-targeted pre-training approach that can reduce pre-training time while maintaining similar performance to models pre-trained on much larger in-domain datasets.

- Our results show that the common regime of finetuning a large pre-trained model is a useful method for our task of extracting clinical action items.

- We develop a method for tasktargeted pre-training data selection, in which a model trained on the downstream task selects unlabeled document segments for fine-tuning a BERT model.




# [Automatic ICD Coding via Interactive Shared Representation Networks with Self-distillation Mechanism](https://aclanthology.org/2021.acl-long.463/)
- In this paper, we propose an Interactive Shared Representation Network with Self-Distillation Mechanism (ISD) to address the above issues.

- This paper proposes an interactive shared representation network and a self-distillation mechanism for the automatic ICD coding task, to address the long-tail and noisy text issues.

- 2) To relieve the long-tail issue, we propose an interactive shared representation network, which can capture the internal connections among codes with different frequencies.

- This section describes our interactive shared representation learning mechanism and self-distillation learning paradigm for ICD coding.




# [A Systematic Investigation of KB-Text Embedding Alignment at Scale](https://aclanthology.org/2021.acl-long.139/)
- We compare the performance of different alignment methods using two evaluation tasks -few-shot link prediction and analogical reasoning.

- We define two tasks, few-shot link prediction and analogical reasoning, to evaluate the effectiveness of injecting text information into KB embeddings and injecting KB information into text embeddings, respectively, based on which we evaluate and compare an array of embedding alignment methods.

- Finally, using COVID-19 as a case study, we also demonstrate that such alignment can effectively inject text information into KB embeddings to complete KBs on emerging entities and events.

- 2. We set up a novel evaluation framework with two evaluation tasks, few-shot link prediction and analogical reasoning, to facilitate future research on this important problem.




# [Modeling Bilingual Conversational Characteristics for Neural Chat Translation](https://aclanthology.org/2021.acl-long.444/)
- We propose to model bilingual conversational characteristics through tailored latent variables for neural chat translation.

- CPCC contains three specific latent variational modules to learn the distributions of role preference, dialogue coherence, and translation consistency, respectively.

- We design three tailored latent variational modules to learn the distributions of inherent bilingual conversational characteristics, i.e., role preference, dialogue coherence, and translation consistency.

- Our contributions are summarized as follows: • To the best of our knowledge, we are the first to incorporate the role preference, dialogue coherence, and translation consistency into neural chat translation.




# [A unified approach to sentence segmentation of punctuated text in many languages](https://aclanthology.org/2021.acl-long.309/)
- We propose a simple window-based model and semi-supervised training paradigm for the segmentation of punctuated text ( §3).

- We release these data splits along with our tool, ERSATZ, as open source. 1

- We show here that a simple context-based model can produce state-of-the-art results with a modest hyperparameter search, trained on noisy annotations from imperfectly-segmented data.

- Yet at the same time, sentences in the wild rarely exist with marked sentence boundaries.




# [How Did This Get Funded?! Automatically Identifying Quirky Scientific Achievements](https://aclanthology.org/2021.acl-long.2/)
- • We construct a dataset containing thousands of funny scientific papers.

- In this work, we presented a novel task in humor recognition -detecting funny and unusual scientific papers, which represents a subtle and sophisticated humor type.

- We created a dataset of funny papers and constructed models, distilling humor literature into features as well as harnessing SOTA advances in NLP.

- Our contributions are: • We formulate a novel humor recognition task in the scientific domain.




# [Database Reasoning Over Text](https://aclanthology.org/2021.acl-long.241/)
- We also introduce a modular architecture to support database reasoning over text and characterize its behavior on our reference dataset.

- Consequently, NLDBs can scale to large databases.

- Our architecture is capable of overcoming the limitations of transformer models because it runs multiple transformers in parallel, each taking a small set of facts.

- We introduce WIKINLDB, a benchmark dataset for exploring database reasoning over facts expressed in natural language.




# [QASR: QCRI Aljazeera Speech Resource A Large Scale Annotated Arabic Speech Corpus](https://aclanthology.org/2021.acl-long.177/)
- In this paper, we introduce a 2, 000 hours transcribed Arabic speech corpus, QASR.

- The QASR is publicly available for the research community.

- We report for the first time named entity recognition in Arabic news transcription.

- In this paper, we create and release 2 the largest corpus for transcribed Arabic speech.




# [Improving Dialog Systems for Negotiation with Personality Modeling](https://aclanthology.org/2021.acl-long.56/)
- Our analysis reveals that the agent demonstrates diverse negotiation behavior and adapts well to different types of opponents.

- Our approach provides the ability to model and infer personality types of opponents, predict changes in their mental state, and use this information to adapt the agent's high-level strategy in negotiation tasks.

- Our experiments show that our method using ToM inference achieves about 20% higher dialog agreement rate and utility compared to baselines on a mixed population of opponents.

- In this work, we proposed a novel framework to integrate the concept of Theory of Mind (ToM) into generating task-oriented dialogs.




# [Ruddit: Norms of Offensiveness for English Reddit Comments](https://aclanthology.org/2021.acl-long.210/)
- Finally, we benchmark several widely-used neural models in their ability to predict offensiveness scores on this new dataset.

- In this paper, we present the first dataset of 6000 English language Reddit comments that has finegrained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive) -normative offensiveness ratings for the comments.

- We used a comparative annotation technique called Best-Worst Scaling, which addresses the limitations of traditional rating scales.

- For the first time, we use comparative annotations to detect offensive language.




# [Interpretable and Low-Resource Entity Matching via Decoupling Feature Learning from Decision Making](https://aclanthology.org/2021.acl-long.215/)
- HIF and KAT Induction are trained separately.

- We design a self-supervised training method for HIF to learn from unlabeled data.

- To address the two limitations, we propose a novel EM framework to decouple feature representation from matching decision.

- We present a decoupled framework for interpretable entity matching.




# [Style is NOT a single variable: Case Studies for Cross-Style Language Understanding](https://aclanthology.org/2021.acl-long.185/)
- We introduce a benchmark XSLUE of mostly existing datasets for studying cross-style language understanding and evaluation.

- Our work has following contributions: • Aggregate 15 different styles and 23 sentencelevel classification tasks ( §3).

- Using XSLUE, we found interesting cross-style observations in classification, correlation, and generation case studies.

- • Collect a cross-style set by annotating 15 styles on the same text for valid evaluation of crossstylistic variation ( §3.3).




# [Out-of-Scope Intent Detection with Self-Supervision and Discriminative Training](https://aclanthology.org/2021.acl-long.273/)
- Particularly, we construct a set of pseudo out-of-scope examples to aid the training process.

- We have proposed a simple, effective, and efficient approach for out-of-scope intent detection by overcoming the limitation of previous methods via matching train-test conditions.

- • We propose a novel out-of-scope intent detection approach by matching training and test tasks to bridge the gap between fitting to training data and generalizing to test data.

- The second group identifies out-of-scope sentences through reconstruction loss.




# [Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction](https://aclanthology.org/2021.acl-long.360/)
- In this work, we introduce a Frame-aware Event Argument Extraction (FEAE) learning framework for IEAE.

- In summary, our contributions in this work are as follows: 1) We introduce a Frame-aware Event Argument Extraction framework to train models for implicit event argument extraction.

- Specifically, we introduce a curriculum knowledge distillation strategy, FEAE, to train an MRC model that could focus on frame-aware information to identify implicit arguments.

- We achieve new state-of-the-art performance on the RAMS dataset.




# [Article Reranking by Memory-Enhanced Key Sentence Matching for Detecting Previously Fact-Checked Claims](https://aclanthology.org/2021.acl-long.425/)
- Our main contributions are as follows: • We propose a novel reranker MTM for factchecked claim detection, which can better identify key sentences in fact-checking articles by exploiting their characteristics.

- We did our best to make the new Weibo dataset for academic purpose reliable.

- Experiments on the public Twitter dataset and the private Weibo dataset show that MTM outperforms the state of the art.

- The claim is longer than general posts on Weibo and contains many details, making the model lose focus on the key elements of the event description.




# [Multi-TimeLine Summarization (MTLS): Improving Timeline Summarization by Generating Multiple Summaries](https://aclanthology.org/2021.acl-long.32/)
- MTLS improves the performance of timeline summarization by generating multiple summaries.

- We propose here the Multiple TimeLine Summarization (MTLS) task that enhances and further generalizes TLS.

- We introduced MTLS task to generalize the timeline summarization problem.

- To the best of our knowledge, the idea of multiple timeline summarization has not been formally proposed yet.




# [VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation](https://aclanthology.org/2021.acl-long.80/)
- In this paper, we introduce VoxPopuli, a largescale multilingual speech corpus for representation learning, semi-supervised learning and interpretation.

- In this paper, we introduce a large-scale multilingual speech corpus, VoxPopuli, for representation learning, semi-supervised learning and interpretation.

- We provide VoxPopuli ASR baselines and validate the versatility of VoxPopuli unlabeled data in semi-supervised learning under challenging out-ofdomain settings.

- We provide VoxPopuli ASR baselines and validate the versatility of VoxPopuli unlabeled data in unsupervised representation learning and semisupervised learning for ASR as well as ST.




# [Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning](https://aclanthology.org/2021.acl-long.102/)
- The work also evaluates a few multilingual language models (ML-LMs) for cross-lingual commonsense reasoning (XCSR), and introduced a new model which outperforms them.

- Experiments show that the proposed MCP objective indeed significantly improves the performance of state-ofthe-art ML-LMs in cross-lingual commonsense reasoning.

- We propose multilingual contrastive pretraining, a simple and effective sentence-level pretext task for enhancing ML-LMs in cross-lingual commonsense reasoning, which significantly improves the state-of-the-art ML-LMs in crosslingual commonsense reasoning.

- We propose the MICKEYPROBE, a language-agnostic probing task for analyzing common sense of ML-LMs in a zero-shot manner.




# [Consistency Regularization for Cross-Lingual Fine-Tuning](https://aclanthology.org/2021.acl-long.264/)
- We propose to improve cross-lingual fine-tuning with two consistency regularization methods, so that we can effectively leverage cross-lingual data augmentations.

- We summarize our contributions as follows: • We propose XTUNE, a cross-lingual finetuning method to better utilize data augmentations based on consistency regularization.

- Second, we introduce model consistency to regularize the models trained with various augmentation strategies.

- In contrast, we propose to utilize consistency regularization to better leverage data augmentation for cross-lingual fine-tuning.




# [Few-Shot Question Answering by Pretraining Span Selection](https://aclanthology.org/2021.acl-long.239/)
- We introduce Splinter (span-level pointer), a pretrained model for few-shot question answering.

- We formulate a new task for pretraining question answering from unlabeled text: recurring span selection.

- The recurring span selection objective was designed to emulate extractive question answering using unlabeled text.

- We investigate the task of few-shot question answering by sampling small training sets from existing question answering benchmarks.




# [Text-Free Image-to-Speech Synthesis Using Learned Segmental Units](https://aclanthology.org/2021.acl-long.411/)
- Specifically, we introduce a model capable of directly generating fluent spoken audio captions of images without the need for natural language text, either as an intermediate representation or a form of supervision during training (Figure 1).

- The main contributions of our paper are as follows: 1. The first methodology for fluent image-tospeech synthesis that does not rely on text.

- Instead, we leverage sub-word speech units discovered using a self-supervised learning objective as a drop-in replacement for the text.

- 5. Over 600,000 spoken audio captions for the MSCOCO dataset.




# [A Conditional Splitting Framework for Efficient Constituency Parsing](https://aclanthology.org/2021.acl-long.450/)
- Discourse Parsing For measuring discourse parsing speed

- We have presented a novel, generic parsing method for constituency parsing based on a Seq2Seq framework.

- In this paper, we propose a generic top-down neural framework for constituency parsing that we validate on both syntactic and sentence-level discourse parsing.

- Our main contributions are: • We cast the constituency parsing task into a series of conditional splitting decisions and use a seq2seq architecture to model the splitting decision at each decoding step.




# [LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification](https://aclanthology.org/2021.acl-long.276/)
- We analyze the effect of the learnable dual augmentation for event causality identification.

- • Our framework is knowledge guided and learnable.

- Moreover, our framework is knowledge guided and learnable.

- Our framework can leverage the duality between identification and generation via dual learning which can learn to generate task-related sentences for ECI.




# [Space Efficient Context Encoding for Non-Task-Oriented Dialogue Generation with Graph Attention Transformer](https://aclanthology.org/2021.acl-long.546/)
- We propose an approach to integrate a concise encoding of knowledge graphs into a Transformer-based decoder architecture for knowledge-grounded dialogue generation.

- The evaluation results prove that our encoding reduces space requirements without negative effects on the precision of reproduction of knowledge and perceived consistency.

- Space Efficient Context Encoding For our proposed encoding, we generate dialogue-specific local knowledge graphs (subgraphs of a background knowledge graph) that capture the information relevant to the dialogue (similar to (Chaudhuri et al., 2021)).

- Our comprehensive human evaluation with models trained with the publicly available datasets KOMODIS (Galetzka et al., 2020) and OPENDIALKG (Moon et al., 2019), both providing dialogues enriched with structured knowledge, shows that we can reduce the space requirement for context without negative effects on the precision of reproduction of knowledge and perceived consistency.




# [Unleash GPT-2 Power for Event Detection](https://aclanthology.org/2021.acl-long.490/)
- Hence, we propose a teacher-student network, in which the teacher is first trained on O to learn the anchor knowledge.

- To avoid noises in the generated data, we propose a novel teacher-student architecture in a multi-task learning framework.

- We introduce a mechanism for knowledge consistency enforcement to mitigate noises from generated data based on optimal transport.

- This table reveals that the teacher-student architecture GPTEDOT significantly improves the performance over previous state-of-the-art models for ED in cybersecurity domain.




# [Weakly Supervised Named Entity Tagging with Learnable Logical Rules](https://aclanthology.org/2021.acl-long.352/)
- We also design a dynamic label selection method to select accurate pseudo labels generated from learned rules for training a discriminative tagging model.

- To ensure the quality of generated pseudo labels, we design a dynamic label selection strategy to select highly 1 Noun phrases are extracted using spaCy noun chunks.

- We defined five types of simple logical rules and introduced compound logical rules that are composed from simple rules to detect entity boundaries and classify their types simultaneously.

- Compound Logical Rules.




# [Cross-replication Reliability -An Empirical Approach to Interpreting Inter-rater Reliability](https://aclanthology.org/2021.acl-long.548/)
- We call it cross-replication reliability (xRR).

- We extend it to cross-kappa (κ x ) to measure cross-replication reliability.

- We opensource a large-scale replication dataset of facial expression judgements analyzed with the proposed framework.

- We present in Appendix A the International Replication (IRep) dataset, 1 a large-scale crowdsourced dataset of four million judgements of human facial expressions in videos.




# [Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering](https://aclanthology.org/2021.acl-long.478/)
- We propose a consistency training framework for conversational question answering, which enhances QA models' abilities to understand conversational context.

- Our framework leverages both the original and self-contained questions for explicit guidance on how to resolve conversational dependency.

- • Our framework encourages QA models to learn how to resolve the conversational dependency via consistency regularization.

- 3 On the other hand, our framework enhances QA models' reasoning abilities for CQA by jointly utilizing original and self-contained questions.




# [BACO: A Background Knowledge-and Content-Based Framework for Citing Sentence Generation](https://aclanthology.org/2021.acl-long.116/)
- We summarize our contributions as follows: • We propose a BAckground knowledge-and COntent-based framework, named BACO, for citing sentence generation.

- In this paper, we propose a BAckground knowledge-and COntent-based framework, named BACO.

- We integrated them into BACO, a BAckground knowledge-and COntent-based framework for citing sentence generation, which learns and uses information that relate to (1) background knowledge; and (2) content.

- We designed a citation network pre-training method for providing the background knowledge.




# [Conversations Are Not Flat: Modeling the Dynamic Information Flow across Dialogue Utterances](https://aclanthology.org/2021.acl-long.11/)
- To capture the dynamic information flow across the dialogue utterances, we design a Flow module to model the context changing scheme.

- Specifically, we employed a uni-directional Flow module to model the context flow and designed three training objectives to optimize the DialoFlow model.

- Our contributions are summarized as follows: • We propose the DialoFlow, a new paradigm to construct the dynamic information flow in the dialogue history by addressing the semantic influence brought about by each utterance.

- In this work, we proposed the DialoFlow to model the dynamic information flow across dialogue utterances by addressing the semantic influence brought about by each utterance.




# [Can Generative Pre-trained Language Models Serve as Knowledge Bases for Closed-book QA?](https://aclanthology.org/2021.acl-long.251/)
- 2 Using SQuAD for Closed-book QA In the closed-book QA task , a model needs to answer questions without external resources.

- firstly use closed-book QA to detect how much knowledge is in pre-trained language models' parameters.

- We investigated by using SQuAD, finding that closed-book QA is still challenging for generative pre-trained language models such as BART.

- While the other three datasets are used by following previous work , we make a novel adaptation of the SQuAD dataset for closed-book QA.




# [A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations](https://aclanthology.org/2021.acl-long.511/)
- Our main contributions are summarized below: • A novel objective to train disentangled representations from attributes.

- We develop new tools to build disentangled textual representations and evaluate them on fair classifi-cation and two sentence generation tasks, namely, style transfer and conditional sentence generation.

- However, the Renyi's surrogate achieves slightly better-disentangled representations.

- Proposed approaches to tackle textual style transfer (Zhang et al., 2020; can be divided into two main categories.




# [Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction](https://aclanthology.org/2021.acl-long.385/)
- In summary, our contributions are as follows: • A new framework named tail-to-tail nonautoregressive sequence prediction (TtT) is proposed to tackle the problem of CGEC.

- We propose a new framework named tail-to-tail non-autoregressive sequence prediction, which abbreviated as TtT, for the problem of CGEC.

- A BERT based sequence encoder is introduced to conduct bidirectional representation learning.

- • Focal loss penalty strategy is adopted to alleviate the class imbalance problem considering that most of the tokens in a sentence are not changed.




# [A Targeted Assessment of Incremental Processing in Neural Language Models and Humans](https://aclanthology.org/2021.acl-long.76/)
- For human incremental processing data, we use by-word reaction times (RTs).

- We find that models systematically under-predict the observed human data.

- We combine these two approaches with a targeted assessment of incremental processing in neural language models and humans.

- Do differences in surprisal accurately predict the slowdowns observed in human reaction time data?




# [Generating Query Focused Summaries from Query-Free Resources](https://aclanthology.org/2021.acl-long.475/)
- In this work we proposed an abstractive framework for query focused summarization.

- In this work, we propose to decompose QFS into two sub-tasks, namely query modeling and conditional language modeling.

- Under this formulation, we use generic summarization data not only for conditional language modeling, but also for learning an evidence ranking model.

- Experimental results across datasets show that the proposed system yields state-of-the-art performance despite the weakly supervised setting, and produces more relevant and coherent summaries compared to existing approaches.




# [The statistical advantage of automatic NLG metrics at the system level](https://aclanthology.org/2021.acl-long.533/)
- With the decomposition, we can adjust metric errors estimates to a noise-free and infinite test set setting by taking only their bias component.

- Note that this term is also the true error of a metric estimator in a noise-free, infinite test set setting.

- With the bias-variance-noise decomposition, we can adjust our observed error estimates to the noise-free, infinite test set setting of the true error.

- Our bias-variance-noise decomposition shows that the observed pairwise accuracy is very close to the true pairwise accuracy from a noise-free, infinite test set setting ( §4.4).




# [AdvPicker: Effectively Leveraging Unlabeled Data via Adversarial Discriminator for Cross-Lingual NER](https://aclanthology.org/2021.acl-long.61/)
- Language-Independent Data To leverage such less language-dependent data

- Specifically, we first train an encoder and a NER classifier on labeled source-language data to learn entity domain knowledge.

- Our experimental results show that the proposed method benefits strongly from this data selection process and outperforms existing SOTA methods; without requiring any additional external resources (e.g., gazetteers or machine translation).

- Furthermore, to address the described problems, we enhance the source-language NER model with unlabeled target language data via adversarial training.




# [Syntax-augmented Multilingual BERT for Cross-lingual Transfer](https://aclanthology.org/2021.acl-long.350/)
- 1. Does augmenting mBERT with syntax improve (generalized) cross-lingual transfer?

- Our main idea is to augment mBERT with language syntax for zero-shot cross-lingual transfer.

- We highlight the cross-lingual transfer gap for mBERT and syntax-augmented mBERT on the evaluation tasks in Table 7.

- Experiment results show that augmenting mBERT with syntax improves cross-lingual transfer, such as in PAWS-X and MLQA, by 1.4 and 1.6 points on average across all the target languages.




# [Dual Graph Convolutional Networks for Aspect-based Sentiment Analysis](https://aclanthology.org/2021.acl-long.494/)
- • We propose orthogonal and differential regularizers.

- To improve the semantic representation, we propose two regularizers for the SemGCN module, i.e., orthogonal and differential regularizers.

- Our contributions are highlighted as follows: • We propose a DualGCN model for the ABSA task.

- Therefore, the dependency probability matrix is used to alleviate dependency parsing errors.




# [Answering Ambiguous Questions through Generative Evidence Fusion and Round-Trip Prediction](https://aclanthology.org/2021.acl-long.253/)
- The proposed round-trip prediction is a general approach for answering ambiguous open-domain questions, which improves our REFUEL as well as several baseline models.

- Finally, we propose a round-trip prediction approach to find additional interpretations that RE-FUEL fails to predict in the first pass.

- To address these issues, we propose REFUEL, Round-trip Evidence FUsion via gEneration with retrievaL, a new framework for answering ambiguous open-domain questions.

- Finally, the proposed round-trip prediction is a model-agnostic general approach for answering ambiguous questions, which improves our REFUEL as well as several baseline models up to 3.7% for the overall performance.




# [Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning](https://aclanthology.org/2021.acl-long.528/)
- Datasets for Geometry Problem Solving.

- This paper further presents a novel geometry solving approach with formal language and symbolic reasoning, called Interpretable Geometry Problem Solver (Inter-GPS).

- Approaches for Geometry Problem Solving.

- Therefore, we build a new large-scale geometry problem benchmark, called Geometry3K.




