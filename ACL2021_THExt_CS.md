# [How Knowledge Graph and Attention Help? A Quantitative Analysis into Bag-level Relation Extraction](https://aclanthology.org/2021.acl-long.359/)
- To quantitatively evaluate the effect of attention and KG on Bag-level RE, we first define two metrics to measure the noise pattern (Section 4.1).

- To evaluate the effects of attention and KG, we design two straightforward Bag-level RE models without the attention module, BRE and BRE+CE.

- Bag-level relation extraction (RE) takes a bag of sentences B = {s 1 , s 2 , . . . , s m } as input.

- We summarize our contributions as follows: • To the best of our knowledge, our proposed framework is the first work to quantitatively analyze the working mechanism of Knowledge Graph and attention for bag-level RE. 1 dumps.wikimedia.org/wikidatawiki/entities/20201109/ • We have conducted extensive experiments to inspire and support us with the above findings.




# [Which Linguist Invented the Lightbulb? Presupposition Verification for Question-Answering](https://aclanthology.org/2021.acl-long.304/)
- Presuppositions and unanswerability.

- We conducted a side-by-side study with 100 unanswerable questions.

- This is the oracle behavior of closed-book QA systems that allow Unanswerable as an answer.

- Through an NQ dataset analysis and a user preference study, we demonstrated that a significant portion of unanswerable questions can be answered more effectively by calling out unverifiable presuppositions.




# [R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling](https://aclanthology.org/2021.acl-long.379/)
- • We propose an efficient optimization algorithm to scale up our approach to a linear number of composition steps (Section 2.2).

- We make the following contributions: • Our novel CKY-based recursive Transformer on differentiable trees model is able to learn both representations and tree structure (Section 2.1).

- Our encoder parser operates in a bottom-up fashion akin to CKY parsing, yet runs in linear time with regard to the number of composition steps,

- To alleviate the above shortcomings, we extend pre-training and the Transformer model to structural language models.




# [Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation](https://aclanthology.org/2021.acl-long.91/)
- Some of our findings are: (1) models trained with more data rely on source information more and have more sharp token contributions; (2) the training process is non-monotonic with several distinct stages.

- Now we turn to analyzing the training process of an NMT model.

- When analyzing the training process, we find that changes in training are non-monotonic and form several distinct stages (e.g., stages changing direction from decreasing influence of source to increasing).

- We analyze changes in these contributions when conditioning on different types of prefixes (reference, generated by a model or random translations), when varying training objective or the amount of training data, and during the training process.




# [Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?](https://aclanthology.org/2021.acl-long.75/)
- In this paper, we ask: can we develop a semantic parsing approach that handles both natural language variation and compositional generalization?

- Figure 2: We evaluate semantic parsing approaches across a diverse set of evaluations focused on natural language variation, compositional generalization, or both.

- This paper proposed to expand the set of benchmarks used to evaluate compositional generalization in semantic parsing.

- Notably, designing approaches that can handle both compositional generalization and the natural language variation of non-synthetic datasets is difficult.




# [BERTAC: Enhancing Transformer-based Language Models with Adversarially Pretrained Convolutional Neural Networks](https://aclanthology.org/2021.acl-long.164/)
- Our work uses a GAN-style training scheme only for pretraining CNNs, not for fine-tuning TLMs.

- We also show that BERTAC outperformed the SOTA method of open-domain QA on Quasar-T and SearchQA.

- We tested all nine CNN models for BERTAC in our GLUE and open-domain QA experiments (Section 5).

- The generator was trained in a GAN-style manner using QA datasets.




# [Knowing the No-match: Entity Alignment with Dangling Cases](https://aclanthology.org/2021.acl-long.278/)
- Dangling entity detection.

- It has two jointly optimized modules, i.e., entity alignment and dangling entity detection.

- It consists of two jointly optimized modules for entity alignment and dangling entity detection, respectively.

- We construct a dataset to support the study of the proposed problem setting, and design a multi-learning framework for both entity alignment and dangling entity detection.




# [Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval](https://aclanthology.org/2021.acl-long.118/)
- Our contributions are summarized as follows: • We provide in-depth analysis on informationseeking QA datasets, namely on Natural Questions and TyDi QA to identify the remaining headrooms.

- In this work, we investigate what makes information-seeking question answering (QA) more challenging, focusing on the Natural Questions (NQ; Kwiatkowski et al., 2019) and TyDi QA (Clark et al., 2020) datasets.

- • We show that answerability prediction and paragraph retrieval remain challenging even for state-of-the-art models through controlled experiments using four different models.

- We conduct the same experiments on SQuAD 2.0, to highlight the unique challenges of the information-seeking queries.




# [Value-Agnostic Conversational Semantic Parsing](https://aclanthology.org/2021.acl-long.284/)
- We use the same hyperparameters for all of our conversational semantic parsing experiments.

- We showed that abstracting away values while encoding the dialogue history and decoding programs significantly improves conversational semantic parsing accuracy.

- Figure 1: Illustration of the conversational semantic parsing problem that we focus on and the representations that we use.

- The meta-computation operators are only required for the conversational semantic parsing datasets, and SMCALFLOW already makes use of them.




# [Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation](https://aclanthology.org/2021.acl-long.480/)
- Multimodal Machine Translation (MMT) aims at designing better translation systems by extending conventional text-only translation systems to take into account multimodal information, especially from visual modality Wang et al., 2019).

- First, we revisit the need for visual context in the popular task of multimodal machine translation and find that: (1) under sufficient textual context, the MMT models' improvements over text-only counterparts result from the regularization effect (Section 5.2).

- In this paper we devise two interpretable models that exhibit state-of-the-art performance on the widely adopted MMT datasets -Multi30k and the new video-based dataset -VaTex.

- This motivates us to revisit the importance of visual context for translation in MMT models.




# [Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection](https://aclanthology.org/2021.acl-long.132/)
- First, we present a human-andmodel-in-the-loop process for training online hate detection models.

- We presented a human-and-model-in-the-loop process for training an online hate detection system.

- To address these challenges, we present a human-and-model-in-the-loop process for collecting data and training hate detection models.

- 3 The platform supports human-andmodel-in-the-loop dataset creation for a variety of NLP tasks.




# [Joint Verification and Reranking for Open Fact Checking Over Tables](https://aclanthology.org/2021.acl-long.529/)
- 3. In addition to our open-domain performance, our model achieves a new closed-domain stateof-the-art result.

- Our contributions can be summarized as follows: 1. We introduce the first model for open-domain table fact verification, demonstrating strong performance exceeding the previous closedsetting state of the art.

- When using an oracle to retrieve a reference table, our approach also represents a new closed-domain state of the art.

- With an accuracy of 75.1%, we obtain the best open-domain results with our model using the joint reranking-and-verification loss and five tables.




# [Reliability Testing for Natural Language Processing Systems](https://aclanthology.org/2021.acl-long.321/)
- Hence, we argue for the need for reliability testing (especially worst-case testing) in NLP by contextualizing it among existing work on promoting accountability and improving generalization beyond the training distribution.

- We contribute a reliability testing framework -DOCTOR -that translates safety and fairness concerns around NLP systems into quantitative tests.

- Next, we showed how adversarial attacks can be reframed as worst-case tests.

- We argue that reliability testing, by reframing the concept of adversarial attacks, has the potential to fill this gap.




# [De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention](https://aclanthology.org/2021.acl-long.371/)
- (3) Our method can effectively resolve both intra-dictionary and inter-dictionary biases.

- Generally, the main contributions of this paper are: • We proposed a causal framework, which not only fundamentally formulates the DS-NER process, but also explains the causes of both intra-dictionary bias and inter-dictionary bias.

- This section describes causal invariance regularizer to eliminate the inter-dictionary bias.

- For inter-dictionary bias, we design a causal invariance regularizer to capture the dictionary-invariant evidence for NER.




# [A Unified Generative Framework for Aspect-Based Sentiment Analysis](https://aclanthology.org/2021.acl-long.188/)
- We implement the BART to generate the target sequence in an end-to-end process based on the unified task formulation.

- However, their inference process is not an end-to-end process.

- However, the pipeline process is not end-to-end.

- In conclusion, all the experiment results confirm that our proposed method, which unifies the training and the inference to an end-to-end generative framework, provides a new SOTA solution for the whole ABSA task.




# [Multimodal Multi-Speaker Merger & Acquisition Financial Modeling: A New Task, Dataset, and Neural Baselines](https://aclanthology.org/2021.acl-long.526/)
- We also present a strong baseline model using multimodal multi-speaker inputs from the M&A calls to perform financial forecasting.

- We present a dataset of M&A calls that can be utilized to predict financial risk following M&A calls.

- To the best of our knowledge, no such M&A conference call dataset exists in academia, and our proposed methodology, M3ANet is the first deep learning approach for financial predictions on M&A conference calls.

- Consequently, shareholders critically analyze the deals to estimate the potential stock price and stock price volatility post the M&A conference call.




# [Explanations for CommonsenseQA: New Dataset and Models](https://aclanthology.org/2021.acl-long.238/)
- 2 https://github.com/dair-iitd/ ECQA-Dataset as eXplanation Generator (XG), comprises a novel two step fine-tuned property generation model (XGP) to generate common-sense properties and a free-flow explanation generation model (XGF).

- We also curate a free-flow explanation for each QA pair.

- Here we generate the free-flow explanations in a two-step manner.

- Query (q, a, c): A.5 Anecdotal Examples: Free-Flow Explanation Generation Table 14 gives an example of free-flow explanation generation by the two variants of XGF system. (




# [Adversarial Learning for Discourse Rhetorical Structure Parsing](https://aclanthology.org/2021.acl-long.305/)
- In this work, we explore to adversarially train a discriminator to estimate the quality of the entire DRS tree for global optimization.

- For model learning, we have two goals: (i) learning of DRS parsing at each time step for local optimization and (ii) learning an adversarial bot to evaluate the pros and cons of the entire tree for global optimization.

-  state-of-the-art parsers on "Full".

- In this part, we compare with seven previous state-of-the-art (SOTA) parsers on text-level DRS parsing.




# [Improving Factual Consistency of Abstractive Summarization via Question Answering](https://aclanthology.org/2021.acl-long.536/)
- First, we propose an efficient automatic evaluation metric for factual consistency that is a simplification of the recently published QAGS protocol .

- We first proposed an efficient evaluation protocol called QUALS to measure factual consistency.

- In this paper we proposed to improve the factual consistency of abstractive summarization models.

- We demonstrate through experiments that our method improves the factual consistency of summarization models measured by both automatic metrics such as QAGS as well as human evaluation.




# [TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance](https://aclanthology.org/2021.acl-long.254/)
- To stimulate progress of QA research over such hybrid data, we propose a new dataset, named TAT-QA (Tabular And Textual dataset for Question Answering).

- Hybrid QA Model We adopt HyBrider (Chen et al., 2020b) as our baseline over hybrid data, which tackles tabular and textual data from Wikipedia.

- This well reveals the effectiveness of our method that reasons over both tabular and textual data involving lots 3283 of numerical contents.

- We further propose a novel TAGOP model based on TAT-QA.




# [Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection](https://aclanthology.org/2021.acl-long.125/)
- To sum up, our contributions are: • We are the first to propose a topic-driven approach for dialogue emotion detection.

- A topic-augmented language model based on finetuning has been developed for topic extraction.

- Different from existing approaches, we propose a topic-driven and knowledge-aware model built on a Transformer Encoder-Decoder structure for dialogue emotion detection.

- We have proposed a Topic-Driven and Knowledge-Aware Transformer model that incorporates topic representation and the commonsense knowledge from ATOMIC for emotion detection in dialogues.




# [Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment](https://aclanthology.org/2021.acl-long.265/)
- In this paper, we introduce denoising word alignment as a new cross-lingual pre-training task.

- In this paper, we introduce a new cross-lingual pre-training task, named as denoising word alignment.

- Our contributions are summarized as follows: • We present a cross-lingual pre-training paradigm that alternately self-labels and predicts word alignments.

- • We introduce a pre-training task, denoising word alignment, which predicts word alignments from perturbed translation pairs.




# [Explaining Relationships Between Scientific Documents](https://aclanthology.org/2021.acl-long.166/)
- We answer these questions by performing both automatic and human evaluations.

- Evaluating explanations of the relationships between scientific documents requires human judges with scientific expertise whose time and effort can be costly.

- In this paper we use citing sentences to operationalize the problem of generating natural language explanations of the relationships between two scientific papers.

- This appears to be a weakness in using citation sentences as proxies for relationship explanations.




# [Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation](https://aclanthology.org/2021.acl-long.409/)
- In this paper, we presented a model of variational semantic memory for few-shot WSD.

- Inspired by these advances, we introduce the first model of semantic memory for WSD in a meta-learning setting.

- We experimentally demonstrate the effectiveness of this approach for few-shot WSD, advancing the state of the art in this task.

- To leverage the shared common knowledge between tasks, we incorporate semantic memory the probabilistic model of prototypes in a hierarchical Bayesian framework.




# [Subsequence Based Deep Active Learning for Named Entity Recognition](https://aclanthology.org/2021.acl-long.332/)
- For example, in Named Entity Recognition (NER), each sentence is usually considered an instance.

- Finally, we summarise the AL algorithm proposed.

- On OntoNotes 5.0, Shen et al. (2017) achieve stateof-the-art performance with 25% of the original dataset querying full sentences, while we require only 13% of the dataset querying subsequences.

- We evaluate the efficacy and efficiency of the tested AL strategies in three ways.




# [Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding](https://aclanthology.org/2021.acl-long.429/)
- We attempted to solve this problem by using the Bregman divergence (Bregman, 1967) to provide a unified interpretation of the SCE and NS loss functions.

- Next, we introduce the Bregman divergence.

- Our code will be available at https://github.com/kamigaito/ acl2021kge 2 Softmax Cross Entropy and Bregman Divergence

- (6) By using the Bregman divergence, we can induce the following propositions for NS (θ ).




# [The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity](https://aclanthology.org/2021.acl-long.544/)
- Next we attempt to understand how existing systems handle the "are you a robot?" intent.

- §5 RQ3. How do including components of a system response to "are you a robot" affect human perception of the system?

- Table 2: Categorizing existing systems responses to the same set of 100 unique phrasings of the "are you a robot?" intent. Systems typically do not succeed in confirming their non-human identity.

- Assuming a system accurately recognizes a POS-ITIVE "are you a robot?" intent, what is the best response?




# [A Neural Transition-based Model for Argumentation Mining](https://aclanthology.org/2021.acl-long.497/)
- We present a neural transition-based model for AM, which can jointly learn ACTC and ARI.

- In this paper, we propose a neural transition-based model for argumentation mining, which can incrementally construct an argumentation graph by predicting a sequence of actions.

- Towards these issues, we present a neural transition-based model for AM, which can classify the types of ACs and identify ARs simultaneously.

- Our proposed model can handle both tree and non-tree structures, and often with linear parsing complexity.




# [BASS: Boosting Abstractive Summarization with Unified Semantic Graph](https://aclanthology.org/2021.acl-long.472/)
- We further propose a graph-based encoderdecoder model based on the unified semantic graph.

- • We propose a graph-based encoder-decoder model to improve both the document representation and summary generation process of the Seq2Seq architecture by leveraging the graph structure.

- Our main contributions are summarized as follows: • We present the unified semantic graph which aggregates co-referent phrases distributed in context for better modeling the longdistance relations and global structure in longdocument summarization and MDS.

- We further present a graph-based encoder-decoder model to improve both the document representation and summary generation process by leveraging the graph structure.




# [Discovering Dialog Structure Graph for Coherent Dialog Generation](https://aclanthology.org/2021.acl-long.136/)
- In this paper, we conduct unsupervised discovery of discrete dialog structure from chitchat corpora.

- Our contribution includes: (1) we identify the task of unsupervised dialog structure graph discovery in open-domain dialogs.

- In this paper, we propose a novel discrete variational auto-encoder with graph neural network (DVAE-GNN) to discover a two-layer dialog structure from chitchat corpus.

- (2) we propose a novel model, DVAE-GNN, for hierarchical dialog struc-ture graph discovery.




# [Long-Span Summarization via Local Attention and Content Selection](https://aclanthology.org/2021.acl-long.470/)
- Long-span Summarization.

- We study two methods for long-span summarization tasks.

- Furthermore, with a small-scale GPU card, our approach achieves comparable or superior performance to previous state-of-the-art systems.

- • Our work has set new state-of-the-art results on Spotify Podcast, arXiv and PubMed datasets in the ROUGE scores.




# [Defense against Synonym Substitution-based Adversarial Attacks via Dirichlet Neighborhood Ensemble](https://aclanthology.org/2021.acl-long.426/)
- By augmenting these adversarial examples with the original training data, the model is robust to such perturbations.

- Such expansions will slightly hurt the performance on the clean data.

- We demonstrated through extensive experimentation that our adversarially trained smooth classifiers consistently outperform all existing empirical and certified defenses by a significant margin on three datasets across different network architectures, establishing state-of-the-art for defenses against adversarial text attacks.

- The values of hyperparameters in Dirichlet Neighborhood Ensemble (DNE) are listed in Table 7.




# [A Joint Model for Dropped Pronoun Recovery and Conversational Discourse Parsing in Chinese Conversational Speech](https://aclanthology.org/2021.acl-long.138/)
- This paper presents that dropped pronoun recovery and conversational discourse parsing are two strongly related tasks.

- Dropped pronoun recovery is a critical technique that can benefit many downstream applications

- The task of dropped pronoun recovery (DPR) aims to locate the position of the dropped pronoun and identify its type.

- To overcome these shortcomings, we propose a novel neural model called DiscProReco to perform DPR and CDP jointly.




# [RAW-C: Relatedness of Ambiguous Words-in Context (A New Lexical Resource for English)](https://aclanthology.org/2021.acl-long.550/)
- As depicted in Figure 4, Cosine Distance tended to underestimate how related humans find same-sense uses to be, and overestimate how related humans find different-senses to be.

- Further, we see that Cosine Distance systematically overestimates how related participants find different-sense Homonyms to be.

- First, contextualized representations from both BERT and ELMo capture the distinction between same-sense and different-sense uses of a word, but their ability to distinguish between homonymy and polysemy is marginal at best.

- In other words, both language models could differentiate same-sense and different-sense uses of an ambiguous word, but their ability to discriminate between Homonymy and Polysemy was marginal at best.




# [Breaking Corpus Bottleneck for Context-Aware Neural Machine Translation with Cross-Task Pre-training](https://aclanthology.org/2021.acl-long.222/)
- Document-level context-aware neural machine translation (NMT) aims to translate sentences in a document under the guidance of document-level context.

- 3 Fine-tuning on Document-Level Parallel Dataset

- On the one hand, sentence-level parallel dataset is a natural resource to use.

- • Using either sentence-level parallel dataset or monolingual documents helps translation for both Transformer baselines and our contextaware models.




# [EMAILSUM: Abstractive Email Thread Summarization](https://aclanthology.org/2021.acl-long.537/)
- In this work, we propose an abstractive email thread summarization dataset, EMAILSUM, that contains 2,549 email threads with human-written short and long summaries.

- Following the branch of dialogue or thread summarization, we introduce a new abstractive Email Thread Summarization (EMAILSUM) dataset.

- We collect both short (< 30 words) and long (< 100 words) abstractive summaries per thread.

- Email Thread Summarization is not a new task.




# [Parameter-Efficient Transfer Learning with Diff Pruning](https://aclanthology.org/2021.acl-long.378/)
- We propose diff pruning as a simple approach for parameter-efficient transfer learning with pretrained models.

- Direct BERT compression methods also provide a straightforward approach to parameter-efficient transfer learning.

- Instead of modifying the architecture of the model, diff pruning extends the base model through a task-specific difference vector.

- After training, storing the task-specific diff vector requires storing a compressed version with both the nonzero positions and weights, which incurs additional storage requirements.




# [Aspect-Category-Opinion-Sentiment Quadruple Extraction with Implicit Aspects and Opinions](https://aclanthology.org/2021.acl-long.29/)
- Experiments demonstrate the advantages of the new task in aspect-based sentiment analysis with implicit aspects/opinions.

- We construct two new datasets for this task, with ACOS annotations including implicit aspects and implicit opinions.

- In this work, we introduce a new task named Aspect-Category-Opinion-Sentiment (ACOS) Quadruple Extraction, with the goal to extract all aspect-category-opinion-sentiment quadruples in a review sentence, and provide full support for aspect-level sentiment analysis with implicit aspects and opinions.

- In fact, product reviews contain a large amount of implicit aspects and opinions.




# [BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data](https://aclanthology.org/2021.acl-long.14/)
- • An unlikelihood training method with nondialogue inference data was introduced to enhance persona consistency understanding.

- • A BERT-based generative framework, BoB, was proposed for training persona-based dialogue models from limited data.

- Contributions in this work are three-fold: • We disentangled the task of persona-based dialogue generation into two sub-tasks: consistency understanding and dialogue generation.

- In this work, we propose a novel BERT-based dialogue model to learn from limited personalized data by disentangling response generation and consistency understanding.




# [Coreference Reasoning in Machine Reading Comprehension](https://aclanthology.org/2021.acl-long.448/)
- First, we propose a methodology for creating MRC datasets that better reflect the coreference reasoning challenge.

- Our main contributions are as follows: • We show that Quoref does not reflect the natural challenges of coreference reasoning and propose a methodology for creating MRC datasets that better reflect this challenge.

- We show that, while coreference resolution and MRC datasets are independent and belong to different domains, our approach improves the coreference reasoning of state-ofthe-art MRC models.

- To improve the coreference reasoning of QA models, we propose to use coreference resolution datasets to train MRC models.




# [Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models](https://aclanthology.org/2021.acl-long.144/)
- This study applied causal mediation analysis to discover and interpret the mechanisms behind syntactic agreement in pre-trained neural language models.

- In this paper, we apply causal mediation analysis in order to study the subject-verb agreement mechanisms in language models.

- To investigate whether the same neurons are implicated in subject-verb agreement across structures, we select the top 5% of neurons per layer by NIE and calculate the proportion of these high-NIE neurons that overlap between each pair of structures.

- GPT-2 Medium's overlap across structures at layer 21 (of 24) is visually similar to the hypothesis, indicating that this layer in GPT-2 shares neurons for subject-verb agreement across structures in a way that aligns with human intuitions about syntactic similarity.




# [Investigating label suggestions for opinion mining in German Covid-19 social media](https://aclanthology.org/2021.acl-long.1/)
- Label suggestions

- Interactive label suggestions (G3).

- Static label suggestions (G2).

- This work presents an investigation of efficient data annotation methods in a case study on social media data.




# [POS-Constrained Parallel Decoding for Non-autoregressive Generation](https://aclanthology.org/2021.acl-long.467/)
- The main contributions of this work could be summarized as follows: • For the first time, we experimentally reveal that the implicit assumption of knowledge distillation does not always hold for the tasks (e.g., text summarization, story ending generation, as demonstrated in our experiments).

- In both text summarization (XSUM) and story ending generation (ROCStories) tasks, the two original NAG models CMLM and DisCo outperform the AG model.

- As demonstrated in our experiments (See § 4.5), there are a number of such tasks beyond the assumption like text summarization and story ending generation.

- Experiments demonstrate that POSPD significantly and consistently improves the two NAG models and beats the sequence-level knowledge distillation with a considerable performance gap.




# [Determinantal Beam Search](https://aclanthology.org/2021.acl-long.512/)
- We derive determinantal beam search, a novel generalization of beam search that casts subset selection as the subdeterminant optimization problem.

- Stochastic Beam Search.

- Standard Beam Search.

- In our experiments, we explore the use of determinantal beam search as a diverse decoding strategy for language generation.




# [Focus Attention: Promoting Faithfulness and Diversity in Summarization](https://aclanthology.org/2021.acl-long.474/)
- Generating Diverse and Faithful Summaries with Focus Sampling.

- In this paper we introduce a Focus Attention MEchanism (or FAME) to transformer-based seq2seq architectures.

- In this section we present our experimental setup to assess the ability of our FAME models to generate faithful summaries and to demonstrate that focus sampling is more effective in generating diverse and faithful summaries than other sampling-based decoding methods.

- Table 2 presents results assessing focus sampling (Focus sample,k ), top-k sampling (Div top,k ) and nucleus sampling (Div nucleus ), for their abilities to generate diverse and faithful summaries.




# [Discontinuous Named Entity Recognition as Maximal Clique Discovery](https://aclanthology.org/2021.acl-long.63/)
- In this paper, we reformulate discontinuous NER as the task of discovering maximal cliques in a segment graph, and propose a novel Mac architecture.

- In this paper, we reformulate discontinuous NER as the task of maximal clique discovery by constructing a segment graph and leveraging the classic B-K backtracking algorithm (Bron and Kerbosch, 1973) to find all the maximum cliques as the entities.

- With the grid tagging scheme, we propose an endto-end neural architecture named Mac.

- Discontinuous NER requires to identify all entity mentions that have discontinuous structures.




# [Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding](https://aclanthology.org/2021.acl-long.462/)
- In this paper, we propose Shallow Aggressive Decoding (SAD) to accelerate online inference efficiency of the Transformer for instantaneous GEC.

- 3 Shallow Aggressive Decoding

- To better exploit the Transformer for instantaneous GEC in practice, we propose a novel approach -Shallow Aggressive Decoding (SAD) to improve the model's online inference efficiency.

- • We propose to combine aggressive decoding with the Transformer with a shallow decoder.




# [PENS: A Dataset and Generic Framework for Personalized News Headline Generation](https://aclanthology.org/2021.acl-long.7/)
- In this paper, we formulated the problem of personalized news headline generation.

- The problem of personalized news headline generation is formulated as follows.

- In this section, we formulate the problem of personalized news headline generation and differentiate it from personalized news recommendations.

- To the best of our knowledge, there are no exclusive methods for personalized news headline generation.




# [Edited Media Understanding Frames: Reasoning About the Intents and Implications of Visual Disinformation](https://aclanthology.org/2021.acl-long.158/)
- In addition, we introduce a new model, PELICAN, improving over competitive languageand-vision transformer baselines.

- For Edited Media Understanding Frames, not all image regions are created equal.

- In this section, we present a new model for Edited Media Understanding Frames, with a goal of kickstarting research on this challenging problem.

- Table 1: Questions for each of the frames in Edited Media Understanding Frames.




# [Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution](https://aclanthology.org/2021.acl-long.539/)
- This paper aims to more fully interpret the stepwise prediction decisions of neural abstractive summarization models.

- Finally, this work has focused chiefly on abstractive summarization models.

- However, for summarization, showing the model partial or ungrammatical inputs in the source may significantly alter the model's behavior.

- Our conclusion is that the pre-trained language model has likely memorized certain articles and their summaries.




# [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://aclanthology.org/2021.acl-long.353/)
- Fine-tuning for natural language generation.

- In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation (NLG) tasks, inspired by prompting.

- Inductive bias of prefix-tuning.

- We propose prefix-tuning as an alternative to full fine-tuning for conditional generation tasks.




# [StructuralLM: Structural Pre-training for Form Understanding](https://aclanthology.org/2021.acl-long.493/)
- In this paper, we propose StructuralLM to jointly exploit cell and layout information from scanned documents.

- Empirical results show that our StructuralLM outperforms strong baselines and achieves new state-of-the-art results in the downstream tasks.

- It is built upon an extension of the Transformer encoder, and jointly exploit cell and layout information from scanned documents.

- We conduct experiments on three benchmark datasets publicly available, and StructuralLM outperforms strong baselines and achieves new state-of-the-art results in the downstream tasks.




# [KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation](https://aclanthology.org/2021.acl-long.44/)
- In this paper, we propose Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based model capable of reasoning about and generating commonsense descriptions from cross modality inputs of images and texts.

- Our contributions in this work are three-folded: 1. We extend the BART model to process multimodal data of images and texts, and enable multimodal reasoning by introducing taskrelevant tokens.

- We propose the pretraining task of Knowledge-Based Commonsense Generation, which improves the reasoning ability of KM-BART by leveraging a large language model pretrained on external commonsense knowledge graphs.

- In this section, we describe our methodology for Visual Commonsense Generation.




# [Crafting Adversarial Examples for Neural Machine Translation](https://aclanthology.org/2021.acl-long.153/)
- We introduce a new definition of NMT adversarial example basing on the round-trip translation.

- Neural Machine Translation.

- • We propose a novel black-box word level NMT attack method that could effectively attack the mainstream NMT models, and exhibit high transferability when attacking popular online translators.

- Based on our new definition and metrics, we propose a promising black-box attack method called Word Saliency speedup Local Search (WSLS) that could effectively attack the mainstream NMT architectures, e.g. RNN and Transformer.




# [Intent Classification and Slot Filling for Privacy Policies](https://aclanthology.org/2021.acl-long.340/)
- We present PolicyIE, an intent classification and slot filling benchmark on privacy policies with two alternative neural approaches as baselines.

- We perform experiments using sequence tagging and sequence-to-sequence (Seq2Seq) learning models to jointly model intent classification and slot filling.

- Taking this as a motivation, we investigate the scope of Seq2Seq learning for joint intent classification and slot filling for privacy policy sentences.

- Intent Classification and Slot Filling Voice assistants and chat-bots frame the task of natural language understanding via classifying intents and filling slots given user utterances.




# [Bad Seeds: Evaluating Lexical Methods for Bias Measurement](https://aclanthology.org/2021.acl-long.148/)
- (2) We explore which features of seeds can cause instability, including both social biases and linguistic dimensions in our analysis.

- 1 Analysis: We provide a systematic framework for understanding the different sources of instability in seed sets that can affect bias measurements.

- Hand-curation can result in high precision seeds, but this method relies on the authors' correction for their own social biases.

- We use a mixture of literature survey, qualitative analysis of seed terms, and analytic methods to explore the use of seed sets for bias measurement through two overarching research questions.




# [Math Word Problem Solving with Explicit Numerical Values](https://aclanthology.org/2021.acl-long.455/)
- The results show that our model achieved better performance than existing state-of-theart methods.

- • We propose a numerical properties prediction mechanism to utilize numerical properties.

- The main contributions of this paper can be summarized as follows: • We explicitly incorporate numerical value information into math word problem solving tasks.

- With the category and comparison information, the model can better identify the interactive relationship between the numerals, and thus generate better results.




# [Learning Dense Representations of Phrases at Scale](https://aclanthology.org/2021.acl-long.518/)
- In this study, we show that we can learn dense representations of phrases at the Wikipedia scale, which are readily retrievable for open-domain QA and other knowledge-intensive NLP tasks.

- First, we aim to learn strong phrase representations from the supervision of reading comprehension tasks.

- We introduce DensePhrases, a phrase retrieval model that is built on fully dense representations.

- Ablation of phrase representations.




# [Word Sense Disambiguation: Towards Interactive Context Exploitation from Both Word and Sense Perspectives](https://aclanthology.org/2021.acl-long.406/)
- Sense Aware Context Exploitation in Supervised WSD

- Experiments on English and crosslingual all-words WSD datasets verify the effectiveness of our approach, surpassing previous state-of-the-art by large margins.

- In this paper, we propose an interactive context exploitation method from both word and sense perspectives in a supervised similarity-based WSD architecture.

- BEM is our direct baseline, which utilizes two encoders to learn context and sense embedding separately and achieves state-of-the-art with only SemCor.




# [Can Sequence-to-Sequence Models Crack Substitution Ciphers?](https://aclanthology.org/2021.acl-long.561/)
- The contributions of our work are: • We propose an end-to-end multilingual decipherment model that can solve 1:1 substi-tution ciphers without explicit plaintext language identification, which we demonstrate on ciphers of 14 different languages.

- Our method, by contrast, can solve substitution ciphers from different languages without explicit language identification.

- In this work, we present an end-to-end decipherment model that is capable of solving simple substitution ciphers without the need for explicit language identification.

- • We conduct extensive testing of the proposed method in different realistic decipherment conditions; different cipher lengths, no-space ciphers, and ciphers with noise, and demonstrate that our model is robust to these conditions.




# [Learning Faithful Representations of Causal Graphs](https://aclanthology.org/2021.acl-long.69/)
- Our key contributions are: • We define a faithfulness property for word embeddings over a causal graph, that captures geometric properties of the causal graph, beyond the direct link prediction by ensuring global proximity preservation.

- We show that the faithfulness of text embeddings to a causal graph is important for causal inferencealigned downstream tasks.

- 3 Learning Faithful Embeddings

- In addition to the causal graph link prediction task, we now present how the faithfulness properties are incorporated through regularization constraints.




# [SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues](https://aclanthology.org/2021.acl-long.54/)
- Overall, we make the following contributions: (i) We propose to model and infer social relations and individual's attributes jointly with SocAoG for the consistency of attributes and social relations among a group.

- MCMC is proposed to parse the relation graph incrementally, enabling the dynamic inference upon any incoming utterance.

- Experiments show that our model outperforms state-of-the-art methods; case studies and ablation studies are provided for analysis.

- We case study the dynamic inference in the next subsection.




# [Metaphor Generation with Conceptual Mappings](https://aclanthology.org/2021.acl-long.524/)
- We propose a novel framework for metaphor generation informed by conceptual metaphor theory.

- In summary, we have shown two methods for incorporating knowledge of conceptual metaphor theory in metaphor generation.

- • A thorough evaluation using both automatic and human evaluations (Section 5).

- We then apply transformations between domains to literal verbs to generate metaphors grounded in conceptual metaphor theory.




# [Refining Sample Embeddings with Relation Prototypes to Enhance Continual Relation Extraction](https://aclanthology.org/2021.acl-long.20/)
- The comparison experiments show that our model significantly outperforms current state-of-the-art CRE models.

- (3) Our extensive experiments upon two RE benchmark datasets justify our model's remarkable superiority over the state-of-the-art CRE models and less dependence on memory size.

- We introduce the following state-of-the-art CRE baselines to be compared with our model in our experiments.

- Our contributions in this paper are summarized as follows: (1) We propose a novel CRE model which achieves enhanced performance through refining sample embeddings with relation prototypes and is effective in avoiding catastrophic forgetting.




# [Select, Extract and Generate: Neural Keyphrase Generation with Layer-wise Coverage Attention](https://aclanthology.org/2021.acl-long.111/)
- The objective of the sentence-selector is to predict the salient sentences in a document, as described in Task 1.

- We evaluate SEG-Net on five benchmarks from scientific articles and two benchmarks from web documents to demonstrate its effectiveness over the state-of-the-art neural generative methods.

- This paper presents SEG-Net, a keyphrase generation model that identifies the salient sentences in a target document to utilize maximal information for keyphrase prediction.

- Our proposed model, SEG-Net jointly learns to extract and generate present and absent keyphrases from the salient sentences in a target document.




# [HERALD: An Annotation Efficient Method to Detect User Disengagement in Social Conversations](https://aclanthology.org/2021.acl-long.283/)
- Our experiments show that HERALD achieves 86% accuracy in user disengagement detection in two dialog corpora.

- Unlike the existing work, we leverage weak supervision to improve annotation efficiency for detecting user disengagement in social conversations.

- We propose a two-stage pipeline HER-ALD to automatically label and denoise training data and, at the same time, build a user disengagement detector.

- To improve annotation efficiency, we reframe the training data annotation process as a denoising problem.




# [A Sequence-to-Sequence Approach to Dialogue State Tracking](https://aclanthology.org/2021.acl-long.135/)
- (3) Scalable, the model can deal with categorical and non-categorical slots and unseen schemas.

- We have proposed a new approach to dialogue state tracking.

- Our approach Seq2Seq-DU formalizes dialogue state tracking as a sequence to sequence problem using BERT and pointer generation.

- A task-oriented dialogue system usually consists of several modules: natural language understanding (NLU), dialogue state tracking (DST), dialogue policy (Policy), and natural language generation (NLG).




# [Lexical Semantic Change Discovery](https://aclanthology.org/2021.acl-long.543/)
- We define the task of lexical semantic change discovery as follows.

- Increasing the threshold on the predicted words improves the F 0.5 for both the type-based and token-based approach.

- Instead, we are interested in the discovery of changing words from the full vocabulary of the corpus.

- We then tune the threshold to find the best-performing type-and token-based approach (Schlechtweg et al., 2018).




# [LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding](https://aclanthology.org/2021.acl-long.201/)
- • In addition to the masked visual-language model, we add text-image alignment and textimage matching as the new pre-training strategies to enforce the alignment among different modalities.

- Meanwhile, a spatial-aware self-attention mechanism is integrated into the Transformer architecture.

- In this paper, we present a multi-modal pre-training approach for visually-rich document understanding tasks, aka LayoutLMv2.

- We also introduce a spatial-aware self-attention mechanism to the model architecture for better modeling the document layout.




# [Fast and Accurate Neural Machine Translation with Translation Memory](https://aclanthology.org/2021.acl-long.246/)
- In this paper, we present a fast and accurate approach for TM-based NMT which can be applied to general translation tasks besides TM-specialized tasks.

- This paper presents a simple TM-based NMT model that employs a single bilingual sentence as its TM and thus is fast in training and inference.

- We first design a light-weight TM-based NMT model for efficiency: its TM includes a single bilingual sentence and we explore variant ways to encode the TM.

- Experiments on TM-specialized tasks demonstrate its superiority over strong baselines in terms of running time and BLEU.




# [Leveraging Type Descriptions for Zero-shot Named Entity Recognition and Classification](https://aclanthology.org/2021.acl-long.120/)
- This paper is the first to study zero-shot NERC, by leveraging entity type descriptions.

- This paper explored the task of zero-shot NERC with entity type descriptions to transfer knowledge from observed to unseen classes.

- We addressed the zero-shot NERC specific challenge that the not-an-entity class is not well defined by proposing a multiclass architecture that uses class-aware encoding to model the negative class.

- Regarding the mentioned causes for the low zero-shot NERC




# [Measuring and Increasing Context Usage in Context-Aware Machine Translation](https://aclanthology.org/2021.acl-long.505/)
- Context-aware Machine Translation

- We introduce a new, architecture-agnostic, metric to measure how context-aware machine translation models are using context and propose a simple regularization technique to increase context usage by these models.

- Specifically, we introduce a simple but effective variation of word dropout (Sennrich et al., 2016a) for context-aware machine translation, dubbed COWORD dropout ( §4).

- We release a software package to encourage the use of this metric in future context-aware machine translation research.




# [Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model](https://aclanthology.org/2021.acl-long.141/)
- • We propose a new way to generate weak labels for ultra-fine entity typing.

- First, we obtain weak ultra-fine entity typing labels from a BERT masked language model.

- In this work, we propose a new approach to automatically generate ultra-fine entity typing labels.

- Many different approaches have been proposed to improve fine-grained entity typing performance.




# [MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation Understanding](https://aclanthology.org/2021.acl-long.285/)
- (3) Our proposed MPC-BERT achieves new state-ofthe-art performance on all three downstream tasks at two benchmarks.

- Experimental results on three downstream tasks show that MPC-BERT outperforms previous methods by large margins and achieves new state-of-the-art performance on two benchmarks.

- In this paper, we present MPC-BERT, a pre-trained language model with five self-supervised tasks for MPC understanding.

- Our goal is to build a pre-trained language model for universal MPC understanding.




# [Dual Slot Selector via Local Reliability Verification for Dialogue State Tracking](https://aclanthology.org/2021.acl-long.12/)
- Nevertheless, existing approaches generally predict the dialogue state at every turn from scratch.

- • Empirical results show that our model achieves state-of-the-art performance with significant improvements.

- Joint accuracy refers to the accuracy of the dialogue state in each turn.

- The results outperform the previous state-of-the-art by +2.54%, +5.43%, and +6.34%, respectively.




# [I like fish , especially dolphins : * Addressing Contradictions in Dialogue Modeling](https://aclanthology.org/2021.acl-long.134/)
- We formalize dialogue contradiction detection as a supervised classification task.

- We introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and humanbot contradictory dialogues.

- We further propose a structured utterance-based approach where utterances are paired before being fed into Transformer NLI models to tackle the dialogue contradiction detection task.

- We hope future work on dialogue contradiction detection could explore pretraining models on more dialogue-focused corpora.




# [Explainable Prediction of Text Complexity: The Missing Preliminaries for Text Simplification](https://aclanthology.org/2021.acl-long.88/)
- Using complexity prediction as a preliminary step reduces the error of the state-of-the-art text simplification models by a large margin.

- We demonstrate that by simply applying explainable complexity prediction as a preliminary step, the out-of-sample text simplification performance of the state-of-the-art, black-box models can be improved by a large margin.

- Such a comparison demonstrates whether adding complexity prediction as a preliminary step is beneficial to a text simplification process when a state-of-the-art, end-to-end simplifier is already in place.

- All these qualitative and quantitative results suggest that the state-of-the-art black-box models tend to oversimplify and distort the meanings of outof-sample input that is already simple.




# [Contrastive Learning for Many-to-many Multilingual Neural Machine Translation](https://aclanthology.org/2021.acl-long.21/)
- Therefore, mRASP2 has a great potential to serve many-to-many translations, including both English-centric and non-English directions.

- Simple yet effective, mRASP2 achieves consistent translation performance improvements for both English-centric and non-English directions on a wide range of benchmarks.

- We demonstrate that contrastive learning can significantly improve zero-shot machine translation directions.

- In this paper, we propose a multilingual COntrastive Learning framework for Translation (mCOLT or mRASP2) to reduce the representation gap of different languages, as shown in Figure 1.




# [Language Grounding Through Neuro-Symbolic Interaction in a 3D World](https://aclanthology.org/2021.acl-long.159/)
- We learn the dynamics model through interaction.

- After pretraining our physical dynamics model, we integrate it with a Transformer Language Model (LM).

- We factorize an embodied agent into an explicit model of world dynamics, and a model of language form.

- We integrate our dynamics model with a pretrained language model, giving us a joint model of linguistic form and meaning.




# [BinaryBERT: Pushing the Limit of BERT Quantization](https://aclanthology.org/2021.acl-long.334/)
- Therefore, BinaryBERT retains the good performance of the ternary model, and can be further refined on the new architecture.

- We thus propose a ternary weight splitting that splits a trained ternary BERT to initialize BinaryBERT, followed by fine-tuning for further refinement.

- Given the challenging loss landscape of binary BERT, we propose ternary weight splitting (TWS) that exploits the flatness of ternary loss landscape as the optimization proxy of the binary model.

- Empirical results show that our approach significantly outperforms vanilla binary training, achieving stateof-the-art performance on BERT compression.




# [From Discourse to Narrative: Knowledge Projection for Event Relation Extraction](https://aclanthology.org/2021.acl-long.60/)
- Specifically, we design Multi-tier Knowledge Projection Network (MKPNet), which can leverage multi-tier discourse knowledge effectively for event relation extraction.

- The main contributions of this paper are: • We propose a new knowledge projection paradigm, which can effectively leverage the commonalities between discourses and narratives for event relation extraction.

- Specifically, we propose Multi-tier Knowledge Projection Network (MKPNet) which can effectively leverage multi-tier discourse knowledge for implicit event relation extraction.

- In this paper, we propose a knowledge projection paradigm for event relation extraction and Multitier Knowledge Projection Network (MKPNet) is designed to leverage multi-tier discourse knowledge.




# [UnitedQA: A Hybrid Approach for Open Domain Question Answering](https://aclanthology.org/2021.acl-long.240/)
- In this study, we propose a hybrid model for opendomain QA, called UnitedQA, which combines the strengths of extractive and generative readers.

- The significant improvement brought by our proposed hybrid approach indicates the benefit of combining extractive and generative readers for open-domain QA.

- We hypothesize that extractive and generative readers adopt different answer inference strategies, thus a hybrid extractive/generative reader can be a better option for open-domain QA tasks.

- Then, we compare the prediction errors made by extractive and generative models, respectively.




# [READONCE Transformers: Reusable Representations of Text for Transformers](https://aclanthology.org/2021.acl-long.554/)
- This work introduced READONCE Transformers, a novel approach for using large scale transformerbased language models to both build and consume reusable document representations.

- Further, can we extend text-to-text transformer architectures to consume such representa-tions in conjunction with text?

- , we also evaluate the ability of READONCE Transformers to handle long documents on the NarrativeQA dataset.

- Our model exhibits a speedup of 2x-5x in training time compared to the different BART architectures (Figure 5).




# [How is BERT surprised? Layerwise detection of linguistic anomalies](https://aclanthology.org/2021.acl-long.325/)
- We use Gaussian models to characterize outof-domain embeddings at intermediate layers of Transformer language models.

- We apply our model to test sentences drawn from BLiMP and 7 psycholinguistics studies, exhibiting morphosyntactic, semantic, and commonsense anomalies.

- It has been proposed that there are different types of linguistic anomalies.

- In this paper, we introduce a new tool to probe for surprisal at intermediate layers of BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019), formulating the problem as density estimation.




# [Semantic Representation for Dialogue Modeling](https://aclanthology.org/2021.acl-long.342/)
- Figure 2 illustrates our method for constructing a dialogue-level AMR graph from multiple utterancelevel AMRs.

- We investigated the feasibility of using AMRs for dialogue modeling, describing an algorithm to construct dialogue-level AMRs automatically and exploiting two ways to incorporate AMRs into neural dialogue systems.

- We consider two main ways of making use of dialogue-level AMRs.

- We focus on creating conversation-level AMRs to facilitate information exchange more effectively for dialogue modeling.




# [What is Your Article Based On? Inferring Fine-grained Provenance](https://aclanthology.org/2021.acl-long.458/)
- Therefore, we propose a rank-aware multi-head cross-attention to relieve this problem.

- Therefore, we propose to learn a query generator, which is different with previous works.

- Therefore, we propose to develop a query generator to generate the possible metadata of the target source article as new search keywords, so that the search engine is more likely to recall source articles.

- Setup To conduct an isolated evaluation of the ILP based inference




# [Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment](https://aclanthology.org/2021.acl-long.67/)
- We show that retrieval-based bitext mining and contextual word alignment achieves even better performance.

- We present a direct and effective framework for BLI with unsupervised bitext mining and word alignment, which sets a new state of the art on the task.

- Word alignment.

- We show that simply pipelining recent algorithms for unsupervised bitext mining (Tran et al., 2020) and unsupervised word alignment (Sabet et al., 2020) significantly improves bilingual lexicon induction (BLI) quality, and that further gains are possible by learning to filter the resulting lexical entries.




# [LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking](https://aclanthology.org/2021.acl-long.64/)
- We introduced LNN-EL, a neuro-symbolic approach for entity linking on short text.

- • We propose, to the best of our knowledge, the first neuro-symbolic method for entity linking (coined "LNN-EL") that provides a principled approach to learning EL rules.

- Entity Linking Models.

- Entity Linking.




# [From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via Synchronous Semantic Decoding](https://aclanthology.org/2021.acl-long.397/)
- We propose an unsupervised semantic parsing method -Synchronous Semantic Decoding, which leverages paraphrasing and grammar-constrained decoding to simultaneously resolve the semantic gap and the structure gap.

- The semantic gap and the structure gap are simultaneously resolved by jointly leveraging paraphrasing and grammar-constrained decoding.

- The main contributions of this paper are: • We propose an unsupervised semantic parsing method -Synchronous Semantic De-coding , which can simultaneously resolve the semantic gap and the structure gap by jointly leveraging paraphrasing and grammar-constrained semantic decoding.

- Paraphrasing in Semantic Parsing.




# [Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders](https://aclanthology.org/2021.acl-long.204/)
- This demonstrates the superiority of stacked acoustic and textual encoding for the speech translation task.

- We propose a Stacked Acoustic-and-Textual Encoding (SATE) method to cascade the ASR encoder and the MT encoder.

- This inspires us to propose a Stacked Acoustic-and-Textual Encoding method, which is straightforward to incorporate the pre-trained models into ST.

- We believe that we are the first to present an end-to-end system that can beat the strong cascaded system in unrestricted speech translation tasks.




# [A Unified Generative Framework for Various NER Subtasks](https://aclanthology.org/2021.acl-long.451/)
- Our contribution can be summarized as follows: • We propose a novel and simple generative solution to solve the flat NER, nested NER, and discontinuous NER subtasks in a unified framework, in which NER subtasks are formulated as an entity span sequence generation problem.

- the larger probability that it can be recalled for the flat NER and discontinuous NER.

- Since flat, continuous and discontinuous entities can all be represented as entity pointer index sequences, this formulation can tackle all the three kinds of NER subtasks in a unified way.

- In this paper, we propose using a novel and simple sequence-to-sequence (Seq2Seq) framework with the pointer mechanism (Vinyals et al., 2015) to generate the entity sequence directly.




# [Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter](https://aclanthology.org/2021.acl-long.454/)
- • BERT. Directly fine-tuning a pre-trained Chinese BERT on Chinese sequence labeling tasks.

- Our work is related to existing neural methods using lexicon features and pre-trained models to improve Chinese sequence labeling.

- In this paper, we proposed a novel method to integrate lexicon features and BERT for Chinese sequence labeling, which directly injects lexicon information between Transformer layers in BERT using a Lexicon Adapter.

- Inspired by the work about BERT Adapter (Houlsby et al., 2019;Bapna and Firat, 2019;Wang et al., 2020), we propose Lexicon Enhanced BERT (LEBERT) to integrate lexicon information between Transformer layers of BERT directly.




# [COSY: COunterfactual SYntax for Cross-Lingual Understanding](https://aclanthology.org/2021.acl-long.48/)
- Comparison with the State of the Art.

- Contributions: 1) we develop a syntax-aware network that incorporates transferable syntax in language models; 2) we propose a novel counterfactual training method that addresses the technical challenge of emphasizing syntax; and 3) extensive experiments on three benchmarks demonstrate the effectiveness of our method for cross-lingual tasks.

- Cross-lingual Transfer.

- In this section, we evaluate our COSY method for cross-lingual understanding under both zero-shot and few-shot settings.




# [MECT: Multi-Metadata Embedding based Cross-Transformer for Chinese Named Entity Recognition](https://aclanthology.org/2021.acl-long.121/)
- By combining the radical information, we propose a Multi-metadata Embedding based Cross-Transformer (MECT).

- The main contributions of the proposed method include: • The use of multi-metadata feature embedding of Chinese characters in Chinese NER.

- • A novel two-stream model that combines the radicals, characters and words of Chinese characters to improve the performance of the proposed MECT method.

- The proposed method uses multi-metadata embedding that fuses the information of radicals, characters and words through a Cross-Transformer network.




# [CDRNN: Discovering Complex Dynamics in Human Language Processing](https://aclanthology.org/2021.acl-long.288/)
- Predictors may thus coordinate in a non-linear, non-additive, and time-varying manner.

- In so doing, CDRNN provides detailed estimates of human language processing dynamics that are difficult to obtain using other measures.

- This study proposed and evaluated CDRNN, a deep neural extension of continuous-time deconvolutional regression that relaxes implausible simplifying assumptions made by widely used regression techniques in psycholinguistics.

- Continuous-time deconvolutional regression (CDR) is a recently proposed technique to address delayed effects in measures of human cognition




# [AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models](https://aclanthology.org/2021.acl-long.400/)
- B Search Space of Architecture Hyper-parameters.

- We propose an effective and efficient method Au-toTinyBERT to search for the optimal architecture hyper-parameters of efficient PLMs.

- We demonstrate the effectiveness of one-shot learning by comparing the performance of one-shot model and stand-alone trained model on the given architectures.

- In the following sections, we first introduce the search space, which is the basis for the one-shot learning and search process.




# [Lexicon Learning for Few-Shot Neural Sequence Modeling](https://aclanthology.org/2021.acl-long.382/)
- We have described a lexical translation mechanism for representing token-level translation rules in neural sequence models.

- In summary, this work: • Introduces a new, lexicon-based output mechanism for neural encoder-decoder models.

- We investigate the effectiveness of the lexical translation mechanism on sequence-to-sequence models for four tasks, three focused on compositional generalization and one on low-resource machine translation.

- While the lexical translation mechanism is quite general, we focus here on its ability to improve few-shot learning in sequence-to-sequence models.




# [Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization](https://aclanthology.org/2021.acl-long.117/)
- Besides, combining all three annotations, our summarizer can achieve new state-of-the-art performance on the SAMSum dataset.

- Extensive experimental results show that our method can obtain consistent and remarkable improvements over strong baselines on both datasets and achieves new stateof-the-art performance on the SAMSum dataset.

- We investigate to use DialoGPT as unsupervised annotators for dialogue summarization, including keywords extraction, redundancy detection and topic segmentation.

- To alleviate the above problem, we explore the pre-trained language model as an unsupervised annotator to automatically provide annotations for the dialogue.




# [LeeBERT: Learned Early Exit for BERT with Cross-Level Optimization](https://aclanthology.org/2021.acl-long.231/)
- • We propose a novel cross-level optimization (CLO) algorithm to learn the loss term weights better.

- Built upon previous literature , we propose a novel cross-level optimization (CLO) algorithm to solve the bilevel optimization better.

- Our contributions are integrated into our Lee-BERT framework, which can be summarized as follows: • We propose a novel training method for early exiting PLMs to ask each exit to learn from each other.

- A Derivation of our cross-level optimization algorithm.




# [Align Voting Behavior with Public Statements for Legislator Representation Learning](https://aclanthology.org/2021.acl-long.99/)
- (2) Hashtag usage prediction.

- In practice, we build a heterogeneous graph to bridge the voting behavior and public statements of legislators.

- Given representation of legislators and legislation, the roll call vote prediction comes out to be a classification task.

- In this paper, we take the first step to align voting behavior with statements on Twitter to jointly learn representation of legislators.




# [Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker](https://aclanthology.org/2021.acl-long.274/)
- We summarize our contributions as follows: • We construct a heterogeneous graph interaction network for document-level EE.

- GIT uses a heterogeneous graph interaction network to model global interactions among sentences and entity mentions.

- On the effect of heterogeneous graph interaction network.

- To tackle the challenges, we introduce Heterogeneous Graph-based Interaction Model with a Tracker (GIT).




# [TicketTalk: Toward human-level performance with end-to-end, transaction-based dialog systems](https://aclanthology.org/2021.acl-long.55/)
- In this section, we show how our end-to-end approach to transaction-based dialog systems produces verbal responses and predicts API calls with near human-level quality and accuracy.

- Dataset size and pre-training are key factors in creating models for end-to-end dialog systems.

- We have described an end-to-end dialog system approach that shows promising potential for transaction-based dialog applications.

- In this work we promote an end-to-end approach to single-domain, transaction-based dialog systems and describe how we overcome both data and grounding challenges described above.




# [Learning to Ask Conversational Questions by Optimizing Levenshtein Distance](https://aclanthology.org/2021.acl-long.438/)
- We present an alternative solution, a Reinforcement Iterative Sequence Editing (RISE) framework for the optimization of MLD.

- To train RISE, we have devised an Iterative Reinforce Training (IRT) algorithm with a novel Dynamic Programming based Sampling (DPS) process.

- In this paper, we have proposed a minimum Levenshtein distance (MLD) based Reinforcement Iterative Sequence Editing (RISE) framework for Conversational Question Simplification (CQS).

- To this end, we devise an Iterative Reinforce Training (IRT) algorithm that allows RISE to do some exploration itself.




# [Improving Zero-Shot Translation by Disentangling Positional Information](https://aclanthology.org/2021.acl-long.101/)
- Our contributions are as follow: • We show that the positional correspondence to input tokens hinders zero-shot translation.

- Our approach substantially improves zero-shot translation quality, as summarized in Table 3.

- Here we extend the challenge of zero-shot translation by integrating a new language.

- With this simple modification, we achieve improvements up to 18.5 BLEU points on zero-shot translation.




# [Assessing Emoji Use in Modern Text Processing Tools](https://aclanthology.org/2021.acl-long.110/)
- In this study, we assessed how well prominent NLP tools cope with text containing emoji characters.

- We consider a set of popular NLP tools and empirically assess to what extent they support emojis across a set of standard tasks, encompassing tokenization, part-of-speech tagging, dependency parsing, and sentiment analysis.

- Owoputi et al. (2013) proposed an improved part-of-speech tagging model for online conversational text based on word clusters.

- Table 3 reports the results of our part-of-speech tagging experiments.




# [A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition](https://aclanthology.org/2021.acl-long.372/)
- In this work, we also aim to design a competitive model for both overlapped and discontinuous NER.

- The results show that our model is highly competitive to the state-of-the-art models for overlapped or discontinuous NER.

- Muis and Lu (2016) present a hypergraph model that is capable of handling both overlapped and discontinuous entities.

- We define two relations for our goal: Overlapping or Succession, which are used for overlapped and discontinuous entities, respectively.




# [Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning](https://aclanthology.org/2021.acl-long.142/)
- With the external contexts, our models with CL outperform previous state-of-the-art approaches on most of the datasets.

- To improve the robustness of the models when no external contexts are available, we propose Cooperative Learning.

- 2. We propose Cooperative Learning to jointly improve the accuracy of both input views in a unified model.

- Besides, we also compare our approaches with previous state-of-the-art approaches over entity-level F1 scores 6 .




# [Nested Named Entity Recognition via Explicitly Excluding the Influence of the Best Path](https://aclanthology.org/2021.acl-long.275/)
- We empirically demonstrate that extracting the innermost entities first results in better performance.

- In this paper, we proposed a simple and effective method for nested named entity recognition by explicitly excluding the influence of the best path through selecting and removing chunks at each level to build different potential functions.

- Besides, we found the innermost-first encoding scheme works better than the conventional outermost-first encoding scheme.

- We conduct experiments on three nested named entity recognition datasets in English, i.e., ACE2004 (Doddington et al., 2004), ACE2005 (Walker et al., 2006) and GENIA (Kim et al., 2003).




# [On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation](https://aclanthology.org/2021.acl-long.172/)
- Adapter-based tuning is more robust to overfitting.

- • Adapter-based tuning demonstrates higher stability and better generalization ability.

- Adapter-based tuning is more stable across a wider range of learning rates.

- Adapter-based tuning alone without TAPT even outperforms fine-tuning with TAPT.




# [A Semantic-based Method for Unsupervised Commonsense Question Answering](https://aclanthology.org/2021.acl-long.237/)
- Our contributions in this paper are summarized as follows: • We propose a semantic-based question answering model (SEQA) for robust commonsense question answering in an unsupervised setting.

- We present a semantic-based question answering method, SEQA, which can answer commonsense questions more accurately and robustly in an unsupervised setting.

- Hence we propose a new method for unsupervised commonsense question answering, which achieves better results and performs more robustly.

- Previous work has explored pre-trained language models (LMs) for unsupervised commonsense question answering.




# [Transferable Dialogue Systems and User Simulators](https://aclanthology.org/2021.acl-long.13/)
- Our contributions can be summarised as follows: • Novel contributions in joint optimisation of a fully text-to-text dialogue system with a matched user simulator on complex, multidomain human-human dialogues.

- We propose a novel joint learning framework of training both the DS and the US for complex multidomain dialogues.

- 1 Through the pre-training on complex multi-domain datasets, two agents are able to interact using natural language, and further create more diverse and rich dialogues.

- In this section, we demonstrate the capability of transfer learning of the proposed framework under two low-resource setups: Domain Adaptation and Single-to-Multiple Domain Transfer.




# [Towards Argument Mining for Social Good: A Survey](https://aclanthology.org/2021.acl-long.107/)
- 2 We conclude the survey by defining the conceptual coordinates and the practical challenges of (semi-)automatic moderation, a highly integrative application of AM for Social Good which represents a natural testbed for the integrated definition of quality discussed above.

- We present an interdisciplinary formulation of the notion of argument quality, which is more apt to work with heterogeneous data and platforms, such as discussion forums and social media.

- Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

- Furthermore, there is a growing research interest in other aspects of AM, such as argument quality.




# [CogAlign: Learning to Align Textual Neural Representations to Cognitive Language Processing Signals](https://aclanthology.org/2021.acl-long.291/)
- Further experiments exhibit that CogAlign is able to transfer cognitive information from Zuco to other datasets without cognitive processing signals.

- In a nutshell, our contributions are listed as follows: • We present CogAlign that learns to align neural representations of natural language to cognitive processing signals at both word and sentence level.

- • We propose a text-aware attention mechanism that extracts useful cognitive information via a compatibility matrix.

- In addition, Co-gAlign with both cognitive processing signals obtains new state-of-the-art performance in all NLP tasks.




# [IrEne: Interpretable Energy Prediction for Transformers](https://aclanthology.org/2021.acl-long.167/)
- This work focused on inference energy predictions of Transformers on a target hardware device.

- The result is that IrEne can predict not only the inference energy consumption of the entire model, but also of its components, making the energy prediction highly interpretable.

- We ask how we can build an energy prediction method that is accurate, interpretable, and extensible.

- First, we frame the problem of interpretable energy prediction over a model tree abstraction.




# [Argument Pair Extraction via Attention-guided Multi-Layer Multi-Cross Encoding](https://aclanthology.org/2021.acl-long.496/)
- To overcome this limitation, we propose an attention-guided multi-layer multi-cross (MLMC) encoding mechanism.

- Figure 2 shows our proposed attention-guided multi-layer multi-cross (MLMC) encoding based model.

- In addition, we also design an auxiliary attention loss to guide each argument to refer to its paired arguments.

- In this paper, we adopt the table-filling approach for modeling the sentence-level correlation between two passages, and propose the attention-guided multi-layer multi-cross (MLMC) encoding scheme for the argument pair extraction (APE) task.




# [Personalized Transformer for Explainable Recommendation](https://aclanthology.org/2021.acl-long.383/)
- In summary, our key contributions are: • We propose PETER that makes recommendation and generates explanation simultaneously based on user and item IDs for explainable recommendation.

- Meanwhile, we demonstrate that conducting recommendation task on the same model is also feasible, so we name it PETER, which stands for PErsonalized Transformer for Explainable Recommendation.

- The following models leverage only user and item IDs to generate explanations (without feature).

- We propose a simple and effective solution to address the personalized generation problem of Transformer, unleashing its language modeling power to generate explanations for recommender systems.




# [Are Pre-trained Convolutions Better than Pre-trained Transformers?](https://aclanthology.org/2021.acl-long.335/)
- Hence, the benefits achieved by pre-training are not exclusive to Transformer models.

- What are the benefits of pre-trained convolutions over Transformers?

- Are only Transformers able to capitalize on the benefits of pre-training?

- Convolutional models outperform Transformers both in non-pretrained and pre-trained setups.




# [DynaSent: A Dynamic Benchmark for Sentiment Analysis](https://aclanthology.org/2021.acl-long.186/)
- ing version 1 of the DynaSent dataset for Englishlanguage ternary (positive/negative/neutral) sentiment analysis.

- Thus, with only two rounds collected, DynaSent is already a substantial new resource for sentiment analysis.

- We presented DynaSent, as the first stage in an ongoing effort to create a dynamic benchmark for sentiment analysis.

- Figure 1: The DynaSent dataset creation process.




# [Dependency-driven Relation Extraction with Attentive Graph Convolutional Networks](https://aclanthology.org/2021.acl-long.344/)
- State-of-the-art performance is observed on both datasets.

- In this paper, we propose a dependency-driven neural approach for RE, where attentive graph neural network (A-GCN) is proposed to distinguish the important contextual information for this task.

- Experimental results and analyses on two English benchmark datasets for relation extraction demonstrate the effectiveness of our approach, especially for entities with long word-sequence distances, where state-of-theart performance is obtained on both datasets.

- In this paper, we propose A-GCN to leverage dependency information for relation extraction, where an attention mechanism is applied to dependency connections to applying weighting on both connections and types so as to better distin-guish the important dependency information and leverage them accordingly.




# [Check It Again: Progressive Visual Question Answering via Visual Entailment](https://aclanthology.org/2021.acl-long.317/)
- In this paper, we propose a select-and-rerank (SAR) progressive framework based on Visual Entailment.

- Our method establishes a new state-of-the-art accuracy of 66.73% with an improvement of 7.55% on the previous best.

- To summarize, our contributions are as follows: 1. We propose a select-and-rerank progressive framework to tackle the language priors problem, and empirically investigate a range of design choices for each module of this framework.

- From the results on VQA-CP v2 shown in Table 1, we can observe that: (1) Top20-SAR+LMH establishes a new state-of-the-art accuracy of 66.73% on VQA-CP v2, beating the previous bestperforming method CL by 7.55%.




# [Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization](https://aclanthology.org/2021.acl-long.510/)
- 6 Multi-task Learning Experiments

- Structured Lottery Tickets.

- Moreover, our experiments show that the phase transition phenomenon is task and model dependent.

- Based on the observation, we further propose a tickets sharing strategy to improve multi-task fine-tuning.




# [Implicit Representations of Meaning in Neural Language Models](https://aclanthology.org/2021.acl-long.143/)
- Even when trained only on language data, NLMs encode simple representations of meaning.

- encodings of entities and situations must begin with a formal framework for representing them.

- Formal models of situations (built, like (a )-(b ), from logical representations of entities and their attributes) are central to linguistic theories of meaning.

- In experiments on two domains, internal representations of text produced by two pretrained language models can be mapped, using a linear probe, to representations of the state of the world described by the text.




# [Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features](https://aclanthology.org/2021.acl-long.58/)
- In this paper, we investigate how to design knowledge grounded dialogue systems that are less prone to including hallucinations or subjective information.

- We demonstrate that this controllable dialogue system is able to produce responses that are perceived by humans to be more objective and faithful to document-based evidence.

- Our goal is to design a dialogue model that is more faithful and objective in how it relays evidence.

- Knowledge-Grounded Dialogue




# [Exploiting Language Relatedness for Low Web-Resource Language Model Adaptation: An Indic Languages Study](https://aclanthology.org/2021.acl-long.105/)
- RelateLM exploits relatedness along two dimensions -script relatedness through transliteration, and sentence structure relatedness through pseudo translation.

- Together, our experiments establish that using a related language as pivot, along with data augmentation through transliteration and bilingual dictionary-based pseudo translation, can be an effective way of adapting an LM for LRLs, and that this is more effective than direct training or pivoting through English.

- In this paper, we make the following contributions: • We address the problem of adding a Low Web-Resource Language (LRL) to an existing pretrained LM, especially when monolingual corpora in the LRL is limited.

- Our proposed approach, consists of three steps, viz., Transliteration to RPL's script, Pseudo translation, and Adaptation through Pre-training.




# [StereoRel: Relational Triple Extraction from a Stereoscopic Perspective](https://aclanthology.org/2021.acl-long.375/)
- Furthermore, we propose a novel model for relational triple extraction, which can simultaneously handle the above issues, named StereoRel.

- Correspondingly, the proposed method leverages three decoders to extract relational triples in a unified model.

- This work has the following main contributions: • We provide a revealing insight into relational triple extraction from a stereoscopic perspective, where the occurrence of several challenging issues and shortcomings of existing methods are rationalized.

- • We propose a novel StereoRel model for relational triple extraction, which can simultaneously reduce information loss, avoid error propagation and not ignore the interaction between entity and relation.




# [Stance Detection in COVID-19 Tweets](https://aclanthology.org/2021.acl-long.127/)
- • We also establish baselines for self-training and domain adaptation approaches that use unlabeled data from the current task, or labeled data from a related task, to complement for limited labeled data for the current task.

- • We establish baseline results using state-ofthe-art supervised stance detection models, including transformer-based models.

- In summary, the contributions of this work are as follows: • We construct a COVID-19-Stance dataset that consists of 6,133 tweets covering user's stance towards four targets relevant to COVID-19 health mandates.

- We provide a comprehensive set of baseline results for the newly constructed COVID-19-Stance dataset, including results with established supervised baselines for stance detection tasks, and also baselines that employ approaches for handling small amounts of labeled data, including self-training and domain adaptation approaches.




# [MLBiNet: A Cross-Sentence Collective Event Detection Network](https://aclanthology.org/2021.acl-long.373/)
- Thus, we devise a bidirectional decoder to model event interdependency within a sentence.

- Firstly, a bidirectional decoder is proposed to explicitly model the sentence-level event inter-dependency, and event relevant information within a sentence is aggregated by an information aggregation module.

- Our contributions are summarized as follows: • We propose a novel bidirectional decoder model to explicitly capture bidirectional event inter-dependency within a sentence, alleviating long-range forgetting problem of traditional tagging structure; • We propose a model called MLBiNet to propagate semantic and event inter-dependency information across sentences and detect multiple events collectively; • We achieve the best performance (F 1 value) on ACE 2005 corpus, surpassing the state-ofthe-art by 1.9 points.

- This paper presents a novel Multi-Layer Bidirectional Network (MLBiNet) to propagate documentlevel semantic and event inter-dependency information for event detection task.




# [DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations](https://aclanthology.org/2021.acl-long.72/)
- In this paper, we proposed a self-supervised objective for learning universal sentence embeddings.

- Our primary contributions are: • We propose a self-supervised sentence-level objective that can be used alongside MLM to pretrain transformer-based language models, inducing generalized embeddings for sentence-and paragraph-length text without any labelled data (subsection 5.1).

- Our objective does not require labelled training data and is applicable to any text encoder.

- • We demonstrate that the quality of the learned embeddings scale with model and data size.




# [Neural Bi-Lexicalized PCFG Induction](https://aclanthology.org/2021.acl-long.209/)
- To model bilexical dependencies and meanwhile reduce complexities, we draw inspiration from the canonical polyadic decomposition (CPD) (Kolda and Bader, 2009) and propose a latent-variable based neural parameterization of L-PCFGs.

- Zhu et al. (2020) propose neural L-PCFGs for unsupervised joint parsing.

- Our main goal is to find a parameterization that removes the implausible independence assumptions of Zhu et al. (2020) while decreases the complexities of the original L-PCFGs.

- Bilexical dependencies, which have been shown useful in unsupervised dependency parsing (Han et al., 2017;Yang et al., 2020), are thus ignored.




# [Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision](https://aclanthology.org/2021.acl-long.390/)
- MetaAdaptRank significantly improves the few-shot ranking accuracy of Neu-IR models across all benchmarks.

- Specifically, CTSyncSup directly improves the few-shot ranking accuracy of BERT rankers by 3% on all benchmarks.

- This paper presents MetaAdaptRank, a domain adaption method for few-shot Neu-IR with contrastive weak data synthesis and meta-reweighted data selection.

- MetaAdaptRank transfers the relevance supervision signals from source domains to few-shot target domains in a zero-shot way.




# [Improving Formality Style Transfer with Context-Aware Rule Injection](https://aclanthology.org/2021.acl-long.124/)
- In this work, we proposed the Context-Aware Rule Injection(CARI), an innovative method for formality style transfer (FST) by injecting multiple rules into an end-to-end BERT-based encoder and decoder model.

- In this work, we propose Context-Aware Rule Injection (CARI), an end-to-end BERT-based encoder and decoder model that is able to learn to select optimal rules based on context.

- Rule-based Formality Style Transfer In the past few years

- Our contributions are as follows: 1. We propose a new method, CARI, to integrate rules for pre-trained language models.




# [What Ingredients Make for an Effective Crowdsourcing Protocol for Difficult NLU Data Collection Tasks?](https://aclanthology.org/2021.acl-long.98/)
- However, we find that training workers using an iterative feedback and requalification protocol is an effective strategy for collecting high-quality QA data.

- We also find that the JUSTIFICATION intervention is ineffective as a stand-alone method for increasing NLU data quality.

- Our results suggest that asking workers to write justifications is not a helpful stand-alone strategy for improving NLU dataset difficulty, at least in the absence of explicit incentives for workers to write high-quality justifications.

- For the EXPERT and CROWD protocols, we train work-ers using an iterative process of collecting data, sending feedback, and qualifying high performing workers to subsequent rounds.




# [PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction](https://aclanthology.org/2021.acl-long.486/)
- This experiment proves that the Global Correspondence component is effective and greatly outperforms the heuristic nearest neighbor principle in the subject-object alignment task.

- The main contributions of this paper are as follows: 1. We tackle the relational triple extraction task from a novel perspective which decomposes the task into three subtasks: Relation Judgement, Entity Extraction and Subject-object Alignment, and previous works are compared on the basis of the proposed paradigm as shown in Table 1.

- In this paper, we presented a brand-new perspective and introduced a novel joint relational extraction framework based on Potential Relation and Global Correspondence, which greatly alleviates the problems of redundant relation judgement, poor generalization of span-based extraction and inefficient subject-object alignment.

- This task is related to the Global Correspondence component, and we just evaluate the entity pair in a triple and ignore the relation.




# [MASK-ALIGN: Self-Supervised Neural Word Alignment](https://aclanthology.org/2021.acl-long.369/)
- We also introduce an attention variant called leaky attention to reduce the high attention weights on specific tokens such as periods.

- Experiments show that MASK-ALIGN achieves new stateof-the-art results without using the guided alignment loss.

- We propose to explicitly model the NULL token with an attention variant, namely leaky attention.

- By encouraging agreement between two directional models both for training and inference, our method consistently outperforms the state-of-the-art on four language pairs without using guided alignment loss.




# [A Gradually Soft Multi-Task and Data-Augmented Approach to Medical Question Understanding](https://aclanthology.org/2021.acl-long.119/)
- We propose a novel multi-task learning approach for medical question understanding.

- In this paper, we introduce a novel, gradually soft multi-task and data-augmented approach to medical question understanding.

- Finally, we describe our gradually soft parameter-sharing scheme.

- We consider the multi-task learning of medical question summarization and medical RQE.




# [Benchmarking Scalable Methods for Streaming Cross Document Entity Coreference](https://aclanthology.org/2021.acl-long.364/)
- Feature-Based Encoders Existing models for streaming cross-document coreference exclusively make use of feature-based mention encoders.

- Our results show that the relative performance of different mention encoders and clustering algorithms varies across different domains.

- We also consider a hybrid encoder which combines feature-based and neural mention encoders.

- In cases where existing approaches perform well, we also find that better performance can be obtained by using a combination of neural and feature-based mention encoders.




# [Cross-modal Memory Networks for Radiology Report Generation](https://aclanthology.org/2021.acl-long.459/)
- In this paper, we propose an effective yet simple approach to radiology report generation enhanced by cross-modal memory networks (CMN), which is designed to facilitate the interactions across modalities (i.e., images and texts).

- Experimental results on two benchmark datasets demonstrate the effectiveness of our model, which achieves the state-of-the-art performance.

- In this paper, we propose to generate radiology reports with cross-modal memory networks, where a memory matrix is employed to record the alignment and interaction between images and texts, with memory querying and responding performed to obtain the shared information across modalities.

- First, cross-modal memory shows its effectiveness in this task, where our model outper-forms COATT, although both of them improve the report generation by the alignment of visual and textual features.




# [Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT Knowledge Distillation](https://aclanthology.org/2021.acl-long.228/)
- • We propose an efficient KD paradigm based on the empirical findings.

- Based on the second finding, we further propose an efficient paradigm to distill HSK.

- Based on this finding, we propose a new paradigm to improve the training efficiency in BERT KD, which does not require loading the teacher model during training.

- We study the three dimensions separately and compare a variety of strategies to extract the crucial knowledge.




# [Exploring Discourse Structures for Argument Impact Classification](https://aclanthology.org/2021.acl-long.306/)
- This indicates that a sequence of discourse re-lations is one of the essential factors for identifying the persuasive power of an argument.

- In this paper, we explicitly investigate how discourse structures influence the impact and the persuasiveness of an argument claim.

- Discourse relations, such as Restatement and Instantiation, among arguments reveal logical structures of a debate conversation.

- 2. We propose a new model called DISCOC to utilize attentions to imitate recurrent networks for sentence-level contextual representation learning.




# [Factoring Statutory Reasoning as Language Understanding Challenges](https://aclanthology.org/2021.acl-long.213/)
- Our contribution enables finer-grained scoring and debugging of models for statutory reasoning, which facilitates incremental progress and identification of performance bottlenecks.

- Here, taking inspiration from Prolog programs, we introduce a novel paradigm, by breaking statutory reasoning down into a sequence of tasks.

- Holzenberger et al. (2020) introduced SARA, a benchmark for the task of statutory reasoning, as well as two different approaches to solving this problem.

- Taking inspiration from the structure of Prolog programs, we re-frame statutory reasoning as a sequence of four tasks, prompting us to introduce a novel extension of the SARA dataset (Section 2), referred to as SARA v2.




# [Diverse Pretrained Context Encodings Improve Document Translation](https://aclanthology.org/2021.acl-long.104/)
- Our architecture is designed to incorporate multiple sources of external embeddings into a pretrained sequence-to-sequence transformer model.

- Earlier work in document machine translation exploits the context by taking a concatenated string of adjacent source sentences as the input of neural sequence-to-sequence models ( Scherrer, 2017).

- This work is closely related to two lines of research: document-level neural machine translation and representation learning via language modeling.

- Our key architectural innovation in this paper is an architecture for two-staged training that enables jointly conditioning on multiple context types, including both the source and target language context.




# [Counterfactual Inference for Text Classification Debiasing](https://aclanthology.org/2021.acl-long.422/)
- We have designed a counterfactual framework for text classification debiasing.

- Inspired by the success of counterfactual inference in mitigating biases in computer vision (Niu et al., 2021;Wang et al., 2020;Tang et al., 2020;Yang et al., 2020;Goyal et al., 2017), we propose a counterfactual-inference-based text-classification debiasing framework (CORSAIR), which is able to make unbiased decisions with biased observations.

- Extensive experiments demonstrated the framework's good effectiveness, generalizability and fairness.

- Inspired by this, we propose a novel modelagnostic paradigm (CORSAIR), which adopts factual learning before mitigating the negative influence of the dataset biases in inference (i.e., after training), without the need of employing data manipulations or designing balancing mechanisms.




# [NeuralWOZ: Learning to Collect Task-Oriented Dialogue via Model-Based Simulation](https://aclanthology.org/2021.acl-long.287/)
- We propose NeuralWOZ, a novel dialogue collection framework, and we show our method achieves state-of-the-art performance on zero-shot domain transfer task.

- To support development of scalable dialogue systems, we propose NeuralWOZ, a model-based dialogue collection framework.

- Our contributions are as follows: • NeuralWOZ, a novel method for generating dialogue corpus using goal instruction and knowledge base information • New state-of-the-art performance on the zeroshot domain transfer task • Analysis results highlighting the potential synergy of using the data generated from Neural-WOZ together with human-annotated data 2 Related Works

- Our method achieves new state-of-the-art of zeroshot domain transfer learning for dialogue state tracking on the MultiWOZ 2.1 dataset (Table 1).




# [Breaking Down Walls of Text: How Can NLP Benefit Consumer Privacy?](https://aclanthology.org/2021.acl-long.319/)
- In this article, we summarize how language technologies can help overcome this challenge and support the development of solutions that assist customers, technology providers and regulators.

- Automated processing of privacy policies opens the door to a number of scenarios where language technologies can be developed to support users in the context of different tasks.

- Thus, an opportunity exists for language technologies to bridge this gap by processing privacy policies to meet the needs of Internet and mobile users.

- Potentially, early successes of language technologies in compliance systems can be extended to analyzing a specified nutrition label, policy and application code.




# [Bridge-Based Active Domain Adaptation for Aspect Term Extraction](https://aclanthology.org/2021.acl-long.27/)
- In this paper, we propose a novel active domain adaptation method.

- In this paper, we propose a novel active domain adaptation method for aspect term extraction.

- This proves the effectiveness of our proposed active domain adaptation strategy.

- We then illustrate how to construct syntactic and semantic bridges.




# [FORECASTQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data](https://aclanthology.org/2021.acl-long.357/)
- FORECASTQA is a question answering task whose goal is to test a machine's forecasting ability.

- In support of the proposed FORECASTQA formulation, we construct a dataset of 10,392 yes-no and multiple-choice questions.

- To utilize this form of data for forecasting, we proposed a question-answering task that requires forecasting skills to solve FORECASTQA, and provided the accompanying dataset.

- Specifically, we formulate the forecasting problem as a multiple-choice Question Answering (QA) task, where the input is a news corpus, questions, choices and timestamps associated with each question, and the output is one of the given choices per question.




# [Selecting Informative Contexts Improves Language Model Fine-tuning](https://aclanthology.org/2021.acl-long.87/)
- Several methods have recently been proposed to improve language model fine-tuning performance.

- Algorithm 1 summarizes IGF with a secondary learner for language model fine-tuning.

- The instability of language model fine-tuning has previously been investigated by others.

- This suggests that IGF could be used as a more energy efficient alternative to standard language model fine-tuning.




# [Towards User-Driven Neural Machine Translation](https://aclanthology.org/2021.acl-long.310/)
- • We propose a novel framework for user-driven NMT based on cache module and contrastive learning, which is able to model user traits in zero-shot scenarios.

- Different from them, user-driven NMT can generate personalized translations for these unseen users in a zero-shot manner.

- Furthermore, we contribute UDT-Corpus, which is the first Chinese-English parallel corpus annotated with user behavior.

- To summarize, major contributions of our work are four-fold: • We introduce and explore user-driven NMT task that leverages user behavior to enhance translation model.




# [VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words](https://aclanthology.org/2021.acl-long.389/)
- In this work, we propose VisualSparta, a simple yet effective text-to-image retrieval model that outperforms all existing query-agnostic retrieval models in both accuracy and speed.

- Existing text-to-image retrieval models can be broadly divided into two categories: query-agnostic and query-dependent models.

- Since VisualSparta can be fit into an inverted-index architecture

- We compare both recall and speed performance with the current state-of-the-art retrieval model in text-to-image search.




# [Using Meta-Knowledge Mined from Identifiers to Improve Intent Recognition in Conversational Systems](https://aclanthology.org/2021.acl-long.545/)
- But, more important, the results seem to support our claim that meta-knowledge embedded in the output layer of our neuro-symbolic algorithms can improve intent recognition performance in practical systems.

- This paper focuses on the algorithms to use the meta-knowledge and on evaluating their impact on the accuracy of intent recognition.

- The results of the experiments indicate that the intent proto-taxonomies embedded by those developers can indeed be used by many workspaces to improve accuracy in intent recognition, notably in OOS detection.

- Those accuracy improvements were achieved without any change in the training set but simply by incorporating the meta-knowledge into intent recognition.




# [Diversifying Dialog Generation via Adaptive Label Smoothing](https://aclanthology.org/2021.acl-long.272/)
- 2. We introduce a light-weight bi-directional decoder that can produce context-aware supervision signals for non-target words.

- We address the low-diversity issue of neural dialogue models by introducing an adaptive label smoothing approach, AdaLabel.

- To address the above issue, we propose an Adaptive Label smoothing (AdaLabel) method that can dynamically estimate a soft target distribution at each time step for different contexts.

- We demonstrate the hard target, label smoothing, and Adaptive Label Smoothing approach when learning to predict the next word ("human").




# [Accelerating BERT Inference for Sequence Labeling via Early-Exit](https://aclanthology.org/2021.acl-long.16/)
- Sentence-Level Early-Exit (SENTEE) is a simple extension for sequential labeling tasks based on existing early-exit approaches.

- Thus, we proposed a TOKen-level Early-Exit (TOKEE) that allows part of tokens that get confident predictions to exit earlier.

- First, we proposed the SENTence-level Early-Exit (SEN-TEE), which is a simple extension of existing earlyexit methods.

- Thus, to further accelerate the inference for sequence labeling tasks, we propose a token-level early-exit (TOKEE) method that allows simple tokens with confident predictions to exit early.




# [Novel Slot Detection: A Benchmark for Discovering Unknown Slot Types in the Task-Oriented Dialogue System](https://aclanthology.org/2021.acl-long.270/)
- Our contributions are three-fold: (1) We introduce a Novel Slot Detection (NSD) task in the task-oriented dialogue system.

- (2) We construct two public NSD datasets and establish a benchmark for future work.

- In this paper, we first introduce a new and important task, Novel Slot Detection (NSD), in the task-oriented dialogue system (Section 2.2).

- In this paper, we defined a new task, Novel Slot Detection(NSD), then provide two public datasets and establish a benchmark for it.




# [Privacy at Scale: Introducing the PrivaSeer Corpus of Web Privacy Policies](https://aclanthology.org/2021.acl-long.532/)
- We evaluated PrivBERT on the data practice classification and the question answering tasks and achieved state of the art results.

- Subsequently, we pretrain PrivBERT, a transformerbased language model, using the corpus and evaluate it on data practice classification and question answering tasks.

- The PrivaSeer Corpus consists of 1,005,380 privacy policies from 995,475 different web domains.

- To satisfy the need for a much larger corpus of privacy policies, we introduce the PrivaSeer Corpus of 1,005,380 English language website privacy policies.




# [GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation](https://aclanthology.org/2021.acl-long.370/)
- Our contributions are two-fold: • We propose the task of general word-level autocompletion for CAT, and construct the first public benchmark to facilitate research in this topic.

- Second, almost no public benchmarks are available for the autocompletion task of CAT.

- We propose a General Word-Level Autocomple-tioN (GWLAN) task for computer-aided translation (CAT).

- This motivates us to propose a general word-level autocompletion task for CAT.




# [LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations](https://aclanthology.org/2021.acl-long.198/)
- Our main contributions are summarized as follows: • We propose to model the 1-hop edge features with a line graph in text-to-SQL.

- To address the above limitations, we propose a Line Graph Enhanced Text-to-SQL model (LGESQL), which explicitly considers the topological structure of edges.

- Our proposed line graph enhanced text-to-SQL (LGESQL) model achieves state-of-the-art results in all configurations at the time of writing.

- In this work, we utilize the line graph to update the edge features in the heterogeneous graph for the text-to-SQL task.




# [Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation](https://aclanthology.org/2021.acl-long.266/)
- Contributions Our main contributions are: • We show the effectiveness of rejuvenating lowfrequency information by pretraining NAT models from raw data.

- By observing these outputs, we found a large amount of translation errors on low-frequency words, most of which are domain-specific terminologies.

- Inspired by this finding, we propose reverse KD to recall more alignments for low-frequency target words ( §2.3).

- • We provide a quantitative analysis of bilingual links to demonstrate the necessity to improve low-frequency alignment by leveraging both KD and reverse KD.




# [Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text](https://aclanthology.org/2021.acl-long.498/)
- A third contribution is a novel evaluation method for text simplification.

- Our main contribution is the Keep it Simple (KiS) procedure, a novel unsupervised method for text simplification.

- We optimize a three-component reward: fluency, salience and simplicity.

- Based on the assumption that simplified text should enable faster reading with better understanding, we propose a realistic Text Comprehension task.




# [A Bidirectional Transformer Based Alignment Model for Unsupervised Word Alignment](https://aclanthology.org/2021.acl-long.24/)
- We present a bidirectional Transformer based alignment (BTBA) model for unsupervised learning of the word alignment task.

- We propose a bidirectional Transformer based alignment (BTBA) model for unsupervised learning of the word alignment task.

- This paper presents a novel BTBA model for unsupervised learning of the word alignment task.

- We further fine-tune the target-to-source attention in the BTBA model to obtain better alignments using a full context based optimization method and selfsupervised training.




# [Multi-stage Pre-training over Simplified Multimodal Pre-training Models](https://aclanthology.org/2021.acl-long.199/)
- Specifically, we propose a new Multi-stage Pretraining (MSP) method.

- The pre-training process is divided into three stages based on different granularities of text-image correspondence from token, phrase to sentence.

- Experimental results show that our method achieves comparable performance to the original LXMERT model in downstream tasks.

- In this paper, inspired by the idea of curriculum learning, we propose a MSP method, which uses information at different granularities from word, phrase to sentence in both texts and images to pre-train a model in stages, we also design pretraining tasks suitable for each stage of pre-training, IFRS task for word-based pre-training, TITP task for phrase-based pretraining, and TITS task for sentence-based pretraining.




# [Learning Syntactic Dense Embedding with Correlation Graph for Automatic Readability Assessment](https://aclanthology.org/2021.acl-long.235/)
- (2) We verify that the correlation relationships among linguistic features could be utilized to learn syntactic dense embeddings.

- We prove that complementing semantic dense embeddings with syntactic dense embeddings learned with correlation graph of linguistic features can produce better-informed representations for readability assessment.

- Table 1 shows three pairs of linguistic features for Chinese readability assessment.

- (3) We propose a Dual-channel neural network model (i.e., Dual-Model) to combine the syntactic dense embeddings and the BERT semantic dense embeddings for readability predictions.




# [DYPLOC: Dynamic Planning of Content Using Mixed Language Models for Text Generation](https://aclanthology.org/2021.acl-long.501/)
- them for long-form opinion text generation poses distinct challenges.

- Opinion Text Generation.

- Our main contributions are summarized as below: • We present a dynamic content planning generation framework, which is directly built on top of BART.

- • We construct two opinion text generation datasets with content plans that capture prominent entities and concepts.




# [GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling](https://aclanthology.org/2021.acl-long.15/)
- A global intent-slot graph interaction layer is further introduced to perform sentence-level intent-slot interaction.

- Their models adopt the autoregressive architecture for joint multiple intent detection and slot filling.

- Graph Interaction Layer Instead of using the whole global-locally graph interaction layer for slot filling

- In our work, we apply a global-locally graph interaction network to model the slot dependency and interaction between the multiple intents and slots.




# [Poisoning Knowledge Graph Embeddings via Relation Inference Patterns](https://aclanthology.org/2021.acl-long.147/)
- We propose data poisoning attacks against KGE models based on inference patterns like symmetry, inversion and composition.

- We study the problem of generating data poisoning attacks on KGE models.

- We study the adversarial vulnerabilities of KGE models through data poisoning attacks.

- Poisoning Attacks on KGE models: We study poisoning attacks for the task of link prediction using KGE models.




# [Integrating Semantics and Neighborhood Information with Graph-Driven Generative Models for Document Retrieval](https://aclanthology.org/2021.acl-long.174/)
- Extensive experimental results on three public datasets demonstrate that the proposed method can outperform state-of-the-art methods, indicating the effectiveness of the proposed framework in unifying the semantic and neighborhood information for document hashing.

- In this section, we present a more effective framework to unify the semantic and neighborhood information for the task of document hashing.

- Obviously, the KL divergence is decomposed into the terms involving singleton and pairwise variables, which can be calculated efficiently.

- Specifically, we applied a graph-induced Gaussian prior to model the two types of information in a unified framework.




# [Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion](https://aclanthology.org/2021.acl-long.534/)
- Our contributions can be summarized as follows: • We summarize three principles of KGC: inferential ability, assumptions and patterns, and construct a rule-guided dataset.

- We highlighted three principles for KGC datasets: inferential ability, assumptions, and patterns, and contribute a large-scale dataset InferWiki.

- We can see that all of the baseline models perform worse than those under the closed-world assumption.

- Different from existing datasets, InferWiki aims to include positive, negative, and unknown testing triples, to evaluate the model under two types of assumptions: open-world assumption and closedworld assumption.




# [Self-Supervised Multimodal Opinion Summarization](https://aclanthology.org/2021.acl-long.33/)
- We proposed the first self-supervised multimodal opinion summarization framework.

- This study proposes a self-supervised multimodal opinion summarization framework called MultimodalSum by extending the existing selfsupervised opinion summarization framework, as shown in Figure 1.

- Our contributions can be summarized as follows: • this study is the first work on self-supervised multimodal opinion summarization; • we propose a multimodal training pipeline to resolve the heterogeneity between input modalities; • we verify the effectiveness of our model framework and model training pipeline through various experiments on Yelp and Amazon datasets.

- The goal of the self-supervised multimodal opinion summarization is to generate a pseudo sum-mary from multimodal data.




# [E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning](https://aclanthology.org/2021.acl-long.42/)
- In this paper, we propose a new end-to-end paradigm for pixel-level vision-language pretraining, to jointly learn visual representation, and semantic alignments between image and text.

- We further incorporate the tasks of object detection and image captioning into pre-training with a unified Transformer encoder-decoder architecture for enhancing visual learning.

- We make the following major contributions in this paper: • We propose the first end-to-end vision-language pre-trained model for both V+L understanding and generation, namely E2E-VLP, which can achieve comparable or superior performance with faster online inference speedup.

- • We enhance cross-modal feature fusion by visual learning of object detection and image caption, which has empirically shown to be effective for vision-language pre-training.




# [Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks](https://aclanthology.org/2021.acl-long.456/)
- Different from previous works, we design a quasi dual learning method between symbolic grounded equation generation and problem's part-of-speech generation to enhance the understanding ability by easing the difficulty of generating problems from symbolic equations.

- Finally, we also propose a novel duality exploiting task that exploits the quasi duality between symbolic grounded equation generation and the problem's part-of-speech generation to enhance the understanding ability of our solver.

- Therefore, we propose a duality exploiting task to enhance the understanding ability of our solver by exploiting the quasi duality between symbolic grounded equation generation and the problem's part-of-speech generation.

- Third, we propose program consistency checker to compute the semantic loss between the predicted program and ground-truth equation to ensure reasonable equation mapping.




# [Improving Paraphrase Detection with the Adversarial Paraphrasing Task](https://aclanthology.org/2021.acl-long.552/)
- In the remainder of this paper, we apply the adversarial paradigm to the problem of paraphrase detection, and demonstrate the following novel contributions: • We use the adversarial paradigm to create a new benchmark examining whether paraphrase detection models are assessing the meaning equivalence of sentences rather than being over-reliant on word-level measures.

- • We create an additional dataset by training a paraphrase generation model to perform our adversarial task, creating another large dataset that further improves the paraphrase detection models' performance.

- Do human-generated adversarial paraphrases improve paraphrase detection?

- We call this the Adversarial Paraphrasing Task (APT).




# [Syntax-Enhanced Pre-trained Model](https://aclanthology.org/2021.acl-long.420/)
- (2) We propose a syntax-aware attention layer and a pre-training task for infusing syntactic information into the pre-trained model.

- To inject syntactic information, we introduce a syntax-aware attention layer and a newly designed pre-training task are proposed.

- In this paper, we seek to enhance pre-trained models with syntax of text.

- To address this, we conduct a large-scale study on injecting automatically produced syntax of text in both the pre-training and fine-tuning stages.




# [Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?](https://aclanthology.org/2021.acl-long.224/)
- , the performance gap between the two paradigms has gradually decreased.

- Although BLEU scores are not strictly comparable across languages, we can safely consider all our models as state-of-the-art.

- The ASR model was trained with the goal of achieving state-of-the-art performance.

- Systems' behavior is analysed from different perspectives, by exploiting high-quality post-edits and annotations by professionals.




# [Control Image Captioning Spatially and Temporally](https://aclanthology.org/2021.acl-long.157/)
- We propose a novel caption generation model with contrastive constraints and attention guidance called LoopCAG to control the captioning process spatially and temporally.

- Our contribution can be summarized as: 1) We propose a novel model LoopCAG, which learns the caption tokens' spatial grounding through attention guidance and temporal localization between trace input and the caption sentences through contrastive constraints in an end-to-end loop manner among the three modalities(vision, language, and traces).

- 3) We intensively study the controllability and explainability of trace-controlled image captioning.

- Composing the above together, We propose a novel trace-controlled image captioning model called LoopCAG and demonstrate its superior capability on captioning quality and flexible controllability.




# [PLOTCODER: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context](https://aclanthology.org/2021.acl-long.169/)
- In this section, we present PLOTCODER, a hierarchical model architecture for synthesizing visualization code from natural language and code context.

- We evaluate PLOTCODER's ability to synthesize visualization programs using Jupyter notebooks of homework assignments or exam solutions.

- In this paper, we conduct the first study of visualization code synthesis from natural language and programmatic context.

- We describe PLOTCODER, a model architecture that includes an encoder that links the natural language description and code context, and a hierarchical program decoder that synthesizes plotted data from the code context and dataframe items.




# [Learning to Explain: Generating Stable Explanations Fast](https://aclanthology.org/2021.acl-long.415/)
- Moreover, our L2E approach produces explanations between 5 and 7.5 × 10 4 times faster than the six baselines, making it suitable for long documents and very large black-box models.

- We start by investigating the faithfulness of an explanation model to the black-box model f θ θ θ .

- We have presented a Learning to Explain (L2E) approach to learn the commonalities of the explanation generation processes across different examples.

- In this paper, we present a learning to explain (L2E) approach that efficiently learns the commonalities of the explanation process across different examples.




# [Cross-Lingual Abstractive Summarization with Limited Parallel Resources](https://aclanthology.org/2021.acl-long.538/)
- We name our model Multi-task Cross-Lingual Abstractive Summarization (MCLAS) under limited resources.

- Therefore, in this paper, we will develop a new model for cross-lingual abstractive summarization under limited supervision.

- In this paper, we propose a novel multi-task learning framework MCLAS to achieve cross-lingual abstractive summarization with limited parallel resources.

- To our best knowledge, cross-lingual summarization under low-resource settings has not been well investigated and explored.




# [DynaEval: Unifying Turn and Dialogue Level Evaluation](https://aclanthology.org/2021.acl-long.441/)
- DynaEval serves as a unified framework for both turn and dialogue level evaluation in open-domain dialogue.

- (3) Empirical results show that DynaEval outperforms the stateof-the-art dialogue coherence model and strongly correlates with human judgements at both turn and dialogue level.

- Hence, a unified framework, which holistically models the entire dialogue, is highly sought after.

- 2.1 Open-ended Dialogue Evaluation Turn-Level Evaluation The current trend for automatic dialogue evaluation is shifting towards the reference-free paradigm.




# [CTFN: Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network](https://aclanthology.org/2021.acl-long.412/)
- Importantly, the cyclic consistency constraint is presented to improve the translation performance.

- Based on CTFN, a hierarchical architecture is further established to exploit multiple bi-direction translations, leading to double multimodal fusing embeddings compared with traditional translation methods.

- On the basis of CTFN, a hierarchical architecture is established to exploit multiple bi-direction translations, leading to double multimodal fusing embeddings (Figure 4).

- In this paper, we present a novel hierarchical multimodal fusion architecture using coupled-translation fusion network (CTFN).




# [Controversy and Conformity: from Generalized to Personalized Aggressiveness Detection](https://aclanthology.org/2021.acl-long.460/)
- The gain is greater for more controversial documents.

- Just a few documents are able to capture individual user beliefs, the more so, the more controversial documents they relate to.

- In this paper, we present novel methods of personalized aggressive content detection based on the representation of user opinion about aggressive texts.

- It is clearly visible that the nonpersonalized method is completely lost for the most controversial documents.




# [Structural Guidance for Transformer Language Models](https://aclanthology.org/2021.acl-long.289/)
- We show evidence that generative structural supervision indeed induces more robust and human-like linguistic generalization in Transformer language models and explore the different trade-offs involved in the presented methods.

- Our work explores two forms of syntactic supervision as structural guidance for Transformer language models.

- Here we explore two major classes of structural guidance for Transformer language models based on joint modeling of language and constituency parses.

- This work hypothesizes that the Transformer language model may benefit from explicit generative structural supervision to systematically generalize syntactic knowledge.




# [Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition](https://aclanthology.org/2021.acl-long.432/)
- (2) We propose a novel method for crowdsourcing learning.

- In addition, the supervised learning with a very small scale of expert annotations can boost the performance significantly.

- In addition, we introduce the unsupervised and supervised settings for crowdsourcing learning which are directly borrowed from the domain adaptation.

- We suggest a state-of-the-art representation learning model that can effectively capture annotator(domain)-aware features.




# [Better than Average: Paired Evaluation of NLP Systems](https://aclanthology.org/2021.acl-long.179/)
- The choice of aggregation mechanism matters in real evaluation setups, and we therefore recommend BT as a robust aggregation mechanism.

- By re-evaluating 296 real NLP evaluation setups across four tasks and 18 evaluation metrics, different aggregation mechanisms yield different conclusions as to which systems are SotA in about 30% of the setups (Sec. 5).

- We can conclude that the choice of aggregation mechanism has a real impact on evaluation outcome.

- We performed a critical assessment of the standard NLP evaluation methodology based on averaged scores, which ignores the natural instance-level pairing of evaluation scores when comparing systems.




# [Factuality Assessment as Modal Dependency Parsing](https://aclanthology.org/2021.acl-long.122/)
- • We develop a joint modal dependency parsing model that extracts events, conceivers and parses a document into its modal dependency structure.

- We evaluate our joint model against the pipeline model.

- The main contributions of this work are as follows: • We construct a large corpus annotated with modal dependency structures via crowdsourcing.

- In addition, we evaluate the joint model against the pipeline model, and show the advantage of the joint model in overall end-to-end modal dependency parsing performance.




# [ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information](https://aclanthology.org/2021.acl-long.161/)
- In this work, we propose ChineseBERT, a model that incorporates the glyph and pinyin information of Chinese characters into the process of largescale pretraining.

- The glyph embedding is based on different fonts of a Chinese character, being able to capture character semantics from the visual surface character forms.

- We hypothesize glyph and pinyin embeddings also serve as strong regularization over text semantics, which means that the proposed ChineseBERT model is able to perform better with less training data.

- Due to the additional consideration of glyph and pinyin, the proposed cannot be directly initialized using a vanilla BERT model, as the model structures are different.




# [A DQN-based Approach to Finding Precise Evidences for Fact Verification](https://aclanthology.org/2021.acl-long.83/)
- Comparison on retrieval of precise evidences.

- Thus, a post-processing strategy is needed to tackle the label bias on Q-values.

- Existing methods for FV do not target the retrieval of precise evidences.

- Inspired by the strong exploration ability of the Deep Q-learning Network (DQN) (Mnih et al., 2015), we develop a DQN-based approach to retrieval of precise evidences.




# [Reservoir Transformers](https://aclanthology.org/2021.acl-long.331/)
- Our contributions are as follows: • We introduce a area under the convergence curve metric for measuring performanceefficiency trade-offs, and show that replacing regular transformer layers with reservoir layers leads to improvements.

- We introduce "reservoir transformers", wherein fixed random reservoir layers are interspersed with regular updateable transformer layers.

- • We show that the addition of reservoir layers leads to improved test set generalization on a variety of tasks in a variety of settings.

- This work explores inserting random non-linear transformations, or what we call reservoir layers, into transformer networks.




# [ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning](https://aclanthology.org/2021.acl-long.260/)
- In this paper, we present ERICA, a general framework for PLMs to improve entity and relation understanding via contrastive learning.

- The experimental results show that ERICA improves the performance of typical PLMs (BERT and RoBERTa) and outperforms baselines, especially under lowresource settings, which demonstrates that ERICA effectively improves PLMs' entity and relation understanding and captures the in-text relational facts.

- The experimental results show that ERICA outperforms all baselines, especially under low-resource settings, which means ERICA helps PLMs better capture the in-text relational facts and synthesize information about entities and their relations.

- We demonstrate the effectiveness of our method on several language understanding tasks, including relation extraction, entity typing and question answering.




# [Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search](https://aclanthology.org/2021.acl-long.508/)
- unless for the proposed Drop-and-Restore

- Second, we design Drop-and-Restore process that makes PoWER-BERT applicable beyond classification, which enables PoWER-BERT to be applicable to a wider range of NLP tasks such as span-based question answering.

- Learning We train a Length-Adaptive Transformer with LengthDrop probability and Layer-Drop probability both set to 0.2.

- In short, we train a Length-Adaptive Transformer once with LengthDrop and Drop-and-Restore, and use it with an automatically determined length configuration for inference with any target computational budget, on both sequencelevel and token-level tasks.




# [TEXT2EVENT: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction](https://aclanthology.org/2021.acl-long.217/)
- 2. We design an effective sequence-to-structure architecture, which is enhanced with a constrained decoding algorithm for event knowledge injection during inference and a curriculum learning algorithm for efficient model learning.

- Concretely, we propose an effective sequence-to-structure network for event extraction, which is further enhanced by a constrained decoding algorithm for event knowledge injection during inference and a curriculum learning algorithm for efficient model learning.

- In summary, the contributions are as follows: 1. We propose a new paradigm for event extraction --sequence-to-structure generation, which can directly extract events from the text in an end-to-end manner.

- In this paper, we propose TEXT2EVENT, a sequence-to-structure generation paradigm for event extraction.




# [Learning Relation Alignment for Calibrated Cross-modal Retrieval](https://aclanthology.org/2021.acl-long.43/)
- Finally, we propose a metric named Intra-modal Self-attention Distance (ISD) to quantify the relation consistency.

- Accordingly, we propose a new regularized training method called Inter-modal Alignment on Intra-modal Selfattentions (IAIS) to calibrate two intra-modal attention distributions mutually via inter-modal alignment, which helps learn better contextualized representations for image-text pairs.

- , we propose a metric called Intra-modal Self-attention Distance with annotation (ISDa) to quantify their semantic gap at the relation level.

- Furthermore, we present a regularized training method IAIS to calibrate intra-modal selfattentions mutually by minimizing the ISD metric.




# [Self-Attention Networks Can Process Bounded Hierarchical Languages](https://aclanthology.org/2021.acl-long.292/)
- In this paper, we theoretically and experimentally demonstrate that self-attention networks can process bounded hierarchical languages Dyck k,D , even with a memory advantage over recurrent networks, despite performing distributed processing of sequences without explicit recursive elements.

- In particular, we prove that self-attention networks can both recognize and generate Dyck k,D , with two conceptually simple yet different constructions (Figure 1).

- Corollary A.2. ∀k, D ∈ N + , there exists a Dlayer hard-attention network that can generate Dyck k,D .

- So, what can self-attention networks tell us about natural language and recursion?




# [Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task](https://aclanthology.org/2021.acl-long.328/)
- 2. A parameter sharing and initialization strategy are proposed to encourage information sharing between tasks.

- Finally, an online knowledge distillation learning is introduced for MTL in order to enhance knowledge transfer from the MT to the ST task.

- 3. Cross-attentive regularization and online knowledge distillation are proposed to reduce the model representation difference between different modalities and enhance the knowledge transfer from the MT task to the ST task.

- Sharing more parameters is helpful to transfer knowledge to the primary ST task.




# [A Pre-training Strategy for Zero-Resource Response Selection in Knowledge-Grounded Conversations](https://aclanthology.org/2021.acl-long.343/)
- Our contributions are summarized as follows: • To the best of our knowledge, this is the first exploration of knowledge-grounded response selection under the zero-resource setting.

- • We achieve a comparable performance of response selection with several existing models learned from crowd-sourced training sets.

- In particular, we propose decomposing the training of the knowledge-grounded response selection into three tasks and joint train all tasks in a unified pre-trained language model.

- We first describe a standard knowledge-grounded response selection task such as Wizard-of-Wikipedia.




# [Point, Disambiguate and Copy: Incorporating Bilingual Dictionaries for Neural Machine Translation](https://aclanthology.org/2021.acl-long.307/)
- Copier couples Pointer and Disambiguator based on a hierarchical copy mechanism seamlessly integrated with Transformer, thereby building a sophisticated endto-end architecture.

- (3) The above two steps are then systematically integrated based on a hierarchical copy mechanism.

- To address the above problems, we propose a novel neural architecture consisting of three novel components: Pointer, Disambiguator, and Copier, to effectively incorporate bilingual dictionaries into NMT models in an end-to-end manner.

- Finally, Copier connects the outputs of Pointer and Disambiguator via a hierarchical copy operation.




# [Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction](https://aclanthology.org/2021.acl-long.367/)
- • We propose a dual-channel span pruning strategy by incorporating explicit supervision from the ATE and OTE tasks to ease the high computational cost caused by span enumeration and maximize the chances of pairing valid target and opinion candidates together.

- Hence, we propose to use a dual-channel pruning strategy which results in two separate pruned pools of aspects and opinions.

- We employ the ABSA subtasks of ATE and OTE to guide our dual-channel span pruning strategy through the scores of the predicted opinion and target span.

- This result indicates that our end-to-end approach can effectively encode the interaction between target and opinion spans, and also alleviates the error propagation.




# [Hierarchical Context-aware Network for Dense Video Event Captioning](https://aclanthology.org/2021.acl-long.156/)
- Our contributions can be summarized as: 1) We propose a hierarchical context-aware model for dense video event captioning to capture video-level context.

- Compared with these works, we are the first to implement a novel video-level hierarchical context-aware network for dense video event captioning.

- In this paper, we propose a novel hierarchical context-aware model for dense video event captioning (HCN) to capture both the local and global context simultaneously.

- In this paper, we propose a novel hierarchical context-aware network to encode both the local and global context of long videos.




# [A Survey of Race, Racism, and Anti-Racism in NLP](https://aclanthology.org/2021.acl-long.149/)
- However, questions of race and racial bias have been minimally explored in NLP literature.

- In this work, we conduct a comprehensive survey of how NLP literature and research practices engage with race.

- We first examine 79 papers from the ACL Anthology that mention the words 'race', 'racial', or 'racism' and highlight examples of how racial biases manifest at all stages of NLP model pipelines ( §3).

- The papers we surveyed suggest that research on race in NLP has used a very limited range of data sets, which fails to account for the multidimensionality of race and simplifications inherent in classification.




# [Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases](https://aclanthology.org/2021.acl-long.146/)
- These findings strongly question previous conclusions that current MLMs could serve as reliable factual knowledge bases.

- To this end, this paper conducts a thorough study on whether MLMs could be reliable factual knowledge bases.

- All the above findings demonstrate that current MLMs are not reliable in factual knowledge extraction.

- Our findings strongly question the conclusions of previous literatures, and demonstrate that current MLMs can not serve as reliable knowledge bases when using prompt-based retrieval paradigm.




# [SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis](https://aclanthology.org/2021.acl-long.175/)
- Future work could also focus on optimal weighting between semantics and style.

- Our analysis leads us to two caption evaluation metrics that capture separate dimensions of caption quality and a fused metric.

- In order to provide caption-level insight as well, we combine SPURTS, SPARCS, and our grammar outlier penalty into one metric -SeMantic and linguistic UndeRstanding Fusion (SMURF) -which rewards captions based on semantics and fluency.

- Both BERT and RoBERTa have achieved state-of-the-art results in various language understanding tasks.




# [An In-depth Study on Internal Structure of Chinese Words](https://aclanthology.org/2021.acl-long.452/)
- Utilizing word-internal structure.

- Third, we propose word-internal structure as a new task, and present benchmark results using a popular dependency parser.

- This paper presents a thorough study on internal structures of Chinese words.

- Annotating word-internal structure.




# [AGGGEN: Ordering and Aggregating while Generating](https://aclanthology.org/2021.acl-long.113/)
- Our contributions are as follows: • We present a novel interpretable architecture for jointly learning to plan and generate based on modelling ordering and aggregation by aligning facts in the target text to input representations with an HMM and Transformer encoder-decoder.

- The model is trained end-to-end and all intermediate steps are learned in a unified framework.

- Recent neural data-to-text systems generate text "end-to-end" (E2E) by learning an implicit mapping between input representations (e.g. RDF triples) and target texts.

- We demonstrate this for two data-to-text generation tasks: the E2E NLG (Novikova et al., 2017) and the WebNLG Challenge (Gardent et al., 2017a).




# [BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?](https://aclanthology.org/2021.acl-long.280/)
- In this paper, we have presented an extensive analysis of the ability of language models to identify analogies.

- The aim of this paper is to analyze the ability of pre-trained LMs to recognize analogies.

- On the other hand, when carefully tuned, some language models are able to achieve state-of-the-art results.

- To directly answer the question posed in the title, our conclusion is that language models can identify analogies to a certain extent, but not all language models are able to achieve a meaningful improvement over word embeddings (whose limitations in analogy tasks are well documented).




# [Including Signed Languages in Natural Language Processing](https://aclanthology.org/2021.acl-long.570/)
- We urge the inclusion of signed languages in NLP.

- What is Good Signed Language Data? For SLP models to be deployable, they must be developed using data that represents the real world ac-curately.

- However, they only capture one aspect of signed languages.

- We survey common SLP tasks and limitations of current methods by drawing on linguistic theories of signed languages.




# [TWAG: A Topic-guided Wikipedia Abstract Generator](https://aclanthology.org/2021.acl-long.356/)
- We propose a novel two-stage Topic-guided Wikipedia Abstract Generation model (TWAG).

- In conclusion, the contributions of our work are as follows: • We propose TWAG, a two-stage neural abstractive Wikipedia abstract generation model utilizing the topic information in Wikipedia, which is capable of generating comprehensive abstracts.

- However, these models are not suitable for Wikipedia abstract generation.

- In this paper, we propose a novel topic-guided abstractive summarization model TWAG for generating Wikipedia abstracts.




# [Probabilistic, Structure-Aware Algorithms for Improved Variety, Accuracy, and Coverage of AMR Alignments](https://aclanthology.org/2021.acl-long.257/)
- Contributions are as follows: • A novel all-inclusive formulation of AMR alignment in terms of mappings between spans and connected subgraphs, including spans aligned to multiple subgraphs; mappings between spans and inter-subgraph edges; and characterization of reentrancies.

- • An algorithm combining rules and EM to align English sentences to AMRs without supervision ( §5), achieving higher coverage and quality than existing AMR aligners ( §7).

- TAMR (Tuned Abstract Meaning Representation; Liu et al., 2018) uses the JAMR alignment rules, along with two others, to produce a set of candidate alignments for the sentence.

- This formulation lends itself to unsupervised learning of alignment models.




# [Joint Models for Answer Verification in Question Answering Systems](https://aclanthology.org/2021.acl-long.252/)
- The results show that our models can outperform the state of the art.

- We have proposed new joint models for AS2.

- ASR establishes the new state of the art on WikiQA with an MAP of 92.80 vs. 92.00.

- Garg et al. (2020) proposed the TANDA approach based on pre-trained Transformer models, obtaining impressive improvement over the state of the art for AS2, measured on the two most used datasets, WikiQA (Yang et al., 2015) and TREC-QA (Wang et al., 2007).




# [Beyond Offline Mapping: Learning Cross-lingual Word Embeddings through Context Anchoring](https://aclanthology.org/2021.acl-long.506/)
- We achieve this by combining cross-lingual anchoring with self-learning and iterative restarts.

- Self-learning.

- So as to understand the role of self-learning and the iterative restarts in our approach, we perform an ablation study and report our results in Table 6.

- This suggests that both the self-learning and the iterative restarts are helpful to make the method more robust to a weak initialization, and have a minor impact otherwise.




# [CoRI: Collective Relation Integration with Data Augmentation for Open Information Extraction](https://aclanthology.org/2021.acl-long.363/)
- To address the issue, we propose a collective approach CoRI, which achieves collective relation integration via two stages: candidate generation and collective inference.

- In this paper, we proposed CoRI, a collective inference approach to relation integration.

- Definition 1 (Relation Integration).

- To alleviate the incoherent prediction issue of local approaches, we propose Collective Relation Integration (CoRI) that exploits the dependency of predictions between adjacent entity pairs to enforce global coherence.




# [Multi-hop Graph Convolutional Network with High-order Chebyshev Approximation for Text Reasoning](https://aclanthology.org/2021.acl-long.513/)
- This study proposes a multi-hop graph convolutional network on high-order dynamic Chebyshev approximation (HDGCN) for text reasoning.

- The main contributions of this paper are listed below: • To improve the efficiency and performance of multi-hop reasoning in spectral graph convolution, we propose a novel graph convolutional network with high-order dynamic Chebyshev Approximation (HDGCN).

- To improve the efficiency and performance of multi-hop graph reasoning in spectral graph convolution, we proposed a new graph convolutional network with high-order dynamic Chebyshev approximation (HDGCN).

- This way have the hidden pairwise interactions to improve the multi-hop graph reasoning in high-order Chebyshev polynomials.




# [Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks](https://aclanthology.org/2021.acl-long.47/)
- We propose a parameter-efficient method for multi-task fine-tuning.

- In summary, we make the following contributions: (1) We propose a parameter-efficient method for multitask fine-tuning based on hypernetworks and adapter layers.

- (3) We provide empirical results on GLUE demonstrating the effectiveness of the proposed method on multi-task learning.

- Extensive experiments show that our method obtains strong improvement over multi-task learning on the GLUE benchmark, and substantially improves the in-domain task generalization.




# [Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates](https://aclanthology.org/2021.acl-long.515/)
- Experiments on the GLUE benchmark showed that ensemble can be effective in reducing the regression when updating to homogeneous models.

- Our main contributions are as follows: • We provide empirical evidence to show that the model update regression occurs across text classification tasks in NLP; • We formulate the regression-free model updates into a constrained optimization problem, and further reduce into a relaxed form which can be approximately optimized through knowledge distillation training method; • We also explore the model ensemble as another method to reduce regression, and analyzed its efficacy; • We analyze the source of the regressions in NLP tasks through linguistic behavioural testing, compare reduction in both distillation and ensemble methods.

- Table 2 shows the efficacy of distillation method and model ensemble on reducing NLP classification task model update regressions.

- Here we include model ensemble as an alternative approach to reduce regression, with further analysis on how ensemble reduces regression in Section 5.1.




# [Best of Both Worlds: Making High Accuracy Non-incremental Transformer-based Disfluency Detection Incremental](https://aclanthology.org/2021.acl-long.286/)
- The results on ASR transcripts are also state-of-the-art.

- Our system is competitive at reparadnum word detection and achieves state-of-the-art results in edit term detection.

- Here we describe the different strategies we used to modify the training and live decoding methods of non-incremental models to detect speech disfluencies word-by-word incrementally.

- This performance degrades to 0.678 on raw transcripts but is a state-of-the-art result for this setting.




# [To POS Tag or Not to POS Tag: The Impact of POS Tags on Morphological Learning in Low-Resource Settings](https://aclanthology.org/2021.acl-long.78/)
- The overall impact of POS tags is not significant.

- For both tasks the impact made by the presence or absence of POS tags is minimal.

- The impact of POS tags on computational morphology may hold implications for linguistic theory as well.

- We conclude that the presence or absence of POS tags does not have a significant impact on two morphological learning tasks: segmentation and glossing, or reinflection.




# [Label-Specific Dual Graph Neural Network for Multi-Label Text Classification](https://aclanthology.org/2021.acl-long.298/)
- The contributions of this paper are as follows: • We propose a novel label-specific dual graph neural network (LDGN), which incorporates category information to extract label-specific components from documents, and explores the interactions among these components.

- Thus, our goal is to find a way to explore the complete and adaptive interactions among labelspecific semantic components more accurately.

- Thus, our goal is to find a way to explore the complete and adaptive interactions among label-specific semantic components more accurately.

- The outstanding results confirm the effectiveness of label-specific semantic interaction learning with dual graph neural network, which include global statistical patterns and local dynamic relations.




# [UnNatural Language Inference](https://aclanthology.org/2021.acl-long.569/)
- We show that state-of-the-art models do not rely on sentence structure the way we think they should: NLI models (Transformer-based models, RNNs, and ConvNets) are largely insensitive to permutations of word order that corrupt the original syntax.

- In (a), we investigate the state-of-the-art pre-trained models such as RoBERTa-Large (Liu et al., 2019), BART-Large (Lewis et al., 2020) and DistilBERT .

- We find, based on a suite of permutation metrics, that they are not.

- In case of MNLI, for example, the current state-of-the-art of 90.5% can be increased to 98.7% merely by permuting the word order of test set examples.




# [A Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy](https://aclanthology.org/2021.acl-long.34/)
- In this paper, we propose a novel training-free and reference-free summarization evaluation metric consisting of a relevance score and a redundancy score.

- Our metric is composed of a centrality-weighted relevance score and a self-referenced redundancy score.

- To solve the above limitations, based on SU-PERT, we propose a novel training-free and reference-free metric for both multiple and single document summarization evaluation.

- Our final evaluation score of a summary consists of an averaged centrality-weighted relevance score and a self-referenced redundancy score.




# [Towards Table-to-Text Generation with Numerical Reasoning](https://aclanthology.org/2021.acl-long.115/)
- • We introduce a new dataset for table-totext generation focusing on numerical reasoning.

- We proposed numericNLG, a new dataset for tableto-text generation using a table and its corresponding description from scientific papers, focusing on numerical-reasoning texts.

- Recent research on the table-to-text generation task is starting to generate text with more reasoning.

- is a sequence-to-sequence model with attention and a copy mechanism.




# [Fine-grained Information Extraction from Biomedical Literature based on Knowledge-enriched Abstract Meaning Representation](https://aclanthology.org/2021.acl-long.489/)
- We develop a new ontology for entities and events with a large corpus from COVID-19 research papers, which is specifically annotated by medical professionals and can serve as a new benchmark for the biomedical IE community.

- To tackle these two challenges, we propose a novel framework for biomedical IE that integrates Abstract Meaning Representation (AMR) (Banarescu et al., 2013) and external knowledge graphs.

- In this paper, we propose a novel biomedical Information Extraction framework to effectively tackle two unique challenges for scientific domain IE: complex sentence structure and unexplained concepts.

- The major contributions of this paper are summarized as follows. • We are the first to enrich the AMR graph with the external knowledge and use a graph neural network to incorporate the fine-grained edge features.




# [Neural Stylistic Response Generation with Disentangled Latent Variables](https://aclanthology.org/2021.acl-long.339/)
- Our contributions are listed below: • We propose a unified framework to simultaneously improve style intensity and maintain content relevance for neural stylistic response generation.

- Experimental results show that our proposed approach improves style intensity and maintains content relevance.

- We thus disentangle the content and style by diluting sentence-level information in the style representation.

- We propose a uniform framework to simultaneously improve the style intensity and maintain the content relevance for neural stylistic response generation.




# [BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised Named Entity Recognition](https://aclanthology.org/2021.acl-long.482/)
- We propose the conditional hidden Markov model (CHMM) to infer true NER labels from multi-source weak annotations.

- Our contributions include: • A multi-source label aggregator CHMM with token-wise transition and emission probabilities for aggregating multiple sets of NER labels from different weak labeling sources.

- The conditional hidden Markov model is an HMM variant for multi-source label denoising.

- We benchmark CHMM-ALT on four datasets against state-of-the-art weakly supervised NER baselines, including both distant learning models and multi-source label aggregation models.




# [Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning](https://aclanthology.org/2021.acl-long.93/)
- This result strengthens our claim that competition between learned linguistic constraints can obscure underlying linguistic knowledge in model behavior.

- Other linguistic processes influence pronouns in Spanish and Italian, and we showed that competition between multiple distinct constraints affects model behavior.

- Using fine-tuning to demote pro drop, we are able to uncover otherwise dormant IC knowledge in Spanish and Italian.

- As such, our results indicate that non-human like behavior can be driven by failure either to learn the underlying linguistic constraints or to learn the relevant constraint ranking.




# [Syntopical Graphs for Computational Argumentation Tasks](https://aclanthology.org/2021.acl-long.126/)
- 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection.

- The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph.

- Nuclear Energy We introduce the idea of a syntopical graph, a data structure that represents the context of claims.

- In this paper, we have introduced a data structure, the syntopical graph, which provides context for claims in collections.




# [ADEPT: An Adjective-Dependent Plausibility Task](https://aclanthology.org/2021.acl-long.553/)
- We present a new large-scale corpus and task, ADEPT, for assessing semantic plausibility.

- Finally, our task provides deeper insight into the effects of various classes of adjectives on event plausibility, and suggests that rules based solely on the adjective or its denotation do not suffice in determining the correct plausibility readings of events.

- We introduce a novel plausibility task: Using automated mechanisms to extract, filter and construct natural sentences, we create ADEPT-a large human-labeled semantic plausibility task consisting of 16 thousand pairs of sentences that differ only by one adjective added to a noun, and designed to resist the statistical correlations that might underpin modern distributional lexical semantics.

- We develop ADEPT, a semantic plausibility task that features over 16 thousand instances consisting of two sentences, where the second sentence differs from the first only by the inclusion of an adjectival modifier.




# [A Dataset and Baselines for Multilingual Reply Suggestion](https://aclanthology.org/2021.acl-long.97/)
- In summary, we present MRS, a multilingual reply suggestion dataset.

- We present MRS, a multilingual dataset for reply suggestion.

- MRS is also a useful benchmark for future research in reply suggestion and cross-lingual generalization.

- After presenting the dataset, we explain how we use MRS to compare reply suggestion models.




# [Language Embeddings for Typology and Cross-lingual Transfer Learning](https://aclanthology.org/2021.acl-long.560/)
- In this paper, following the finding that structural similarity is critical in multilingual language models (K et al., 2020), we generate language embeddings from a denoising autoencoder objective and demonstrate that they can be effectively used in cross-lingual zero-shot learning.

- We train our model on the English MultiNLI dataset, and directly evaluate the trained model on the other languages without language-specific fine-tuning, in a zero-shot cross-lingual setting.

- In addition, to address the question of whether the learned language embeddings can help in downstream language tasks, we plug-in the language embeddings to cross-lingual dependency parsing and natural language inference (XNLI, in a zero-shot learning setting, obtaining performance improvements.

- We report results with Eng. language embeddings.




# [From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text](https://aclanthology.org/2021.acl-long.245/)
- Below, we summarize our main contributions: 1. We propose a state-of-the-art translation model that generates Hindi-English CS text starting from monolingual Hindi text.

- 2. We introduce a new Hindi-English CS text corpus in this work.

- GLUECoS (Khanuja et al., 2020) is an evaluation benchmark spanning six natural language tasks for code-switched English-Hindi and English-Spanish data.

- 3. We use sentences generated from our model to train language models for Hindi-English CS text and show significant improvements in perplexity compared to other approaches.




# [Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP](https://aclanthology.org/2021.acl-long.345/)
- In this paper we introduce AmbER sets, a benchmark for evaluating the entity disambiguation capabilities of retrievers across multiple NLP tasks.

- We create a broad range of AmbER sets, covering many entity types, with input queries for three open-domain NLP tasks: fact checking, slot filling, and question answering.

- In this work, we create AmbER sets for three tasks: fact checking, slot filling, and question answering (Table 2).

- To answer this question, we create two collections of AmbER sets.




# [ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic](https://aclanthology.org/2021.acl-long.551/)
- Our models are publicly available.

- Our models establish new stateof-the-art (SOTA) on the majority of tasks, across all cluster tasks.

- We presented our efforts to develop two powerful Transformer-based language models for Arabic.

- For evaluation, we also introduce a novel ARabic natural Language Understanding Evaluation benchmark (ARLUE).




# [Generating Relevant and Coherent Dialogue Responses using Self-separated Conditional Variational AutoEncoders](https://aclanthology.org/2021.acl-long.437/)
- To address these drawbacks, we propose a novel model, namely Self-Separated Conditional Variational Autoencoder (SepaCVAE).

- Open-domain dialogue generation is a challenging task in natural language processing.

- In a word, the evaluation results illustrate the effectiveness of SepaCVAE in terms of improving the relevance and coherence of responses.

- Thus this approach sacrifices too much relevance and coherence for diversity and informativeness.




# [Supporting Land Reuse of Former Open Pit Mining Sites using Text Classification and Active Learning](https://aclanthology.org/2021.acl-long.320/)
- We used active learning for multi-label text classification to extract restrictions and topics from unstructured text in legacy documents and visualized the results using a GIS.

- We address this issue by demonstrating and evaluating a workflow consisting of optical character recognition (OCR), text classification and active learning, whose results are then visualized by a Geographic Information System (GIS).

- We evaluate multi-label active learning performed by three human annotators, who each train a sentence classification model for classifying restrictions and topics, resulting in two runs per person.

- In this work, we have presented and evaluated a system which automates information requests related to the post-management of former open pit mines by leveraging unstructured and geospatial data.




# [H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences](https://aclanthology.org/2021.acl-long.294/)
- We have proposed a new Transformer attention using the inductive bias inspired by the H-Matrix.

- In our case, it also serves to highlight the effectiveness of the inductive bias inspired by the H-Matrix method, as well as the capability of our hierarchical attention to handle long sequences.

- However, the standard attention mechanism of the Transformer has a run time and memory usage that scales quadratically with sequence length.

- The hierarchical attention proposed in this paper is inspired by these Multilevel Methods in general and the H-Matrix in particular.




# [SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining](https://aclanthology.org/2021.acl-long.457/)
- In this paper, we introduce SMedBERT, a KE-PLM pre-trained over large-scale medical corpora and medical KGs.

- To the best of our knowledge, SMedBERT is the first PLM with structured semantics knowledge injected in the medical domain.

- (1) Rich semantic information from neighboring structures of linked-entities, such as entity types and relations, are highly useful for medical text understanding.

- (2) Mention-neighbor hybrid attention aims to infuse the structured semantics knowledge into encoder layers, which includes type attention, node attention and gated position infusion module.




# [Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact Verification](https://aclanthology.org/2021.acl-long.128/)
- We have presented a novel topic-aware evidence reasoning and stance-aware aggregation model for fact verification.

- The main contributions are listed as follows: • We propose a novel topic-aware evidence reasoning and stance-aware aggregation approach, which is, to our best knowledge, the first attempt of jointly exploiting semantic interaction and topical consistency to learn latent evidence representation for fact verification.

- Specifically, to incorporate the topical coherence among multiple pieces of evidence into our model, we disregard the order of evidence and treat each evidence independently.

- Therefore, two kinds of topical relationship are considered: 1) topical coherence among multiple pieces of evidence (T C ee ); 2) topical consistency between the claim and each evidence (T C ce ).




# [How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models](https://aclanthology.org/2021.acl-long.243/)
- 1) We systematically compare monolingual with multilingual pretrained language models for 9 typologically diverse languages on 5 structurally different tasks.

- This implies that both the data size and the tokenizer are among the main driving forces of downstream task performance.

- Further, we have disentangled the impact of pretrained corpora size from the influence of the tokenizers on the downstream task performance.

- We have conducted the first comprehensive empirical investigation concerning the monolingual performance of monolingual and multilingual language models (LMs).




# [What Context Features Can Transformer Language Models Use?](https://aclanthology.org/2021.acl-long.70/)
- We have investigated the extent to which transformer models can use structural and lexical information in long-range contexts for English language modeling.

- This transformation removes significant information in both mid-and long-range conditions (55% and 69% Figure 3: Effect of word identity on usable information.

- Notably, the shuf. within trigrams (14% and 41%) and the shuf. trigrams within sent. (16% and 35%) ablations both remove relatively little usable information in both the mid-and long-range conditions.

- In this section, we attempt to determine what information in transformer LM contexts is usable by measuring ablated information (Eq. ( 9)).




# [Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification](https://aclanthology.org/2021.acl-long.40/)
- Finally, we showed that attention-based explanations with TaSc outperform other interpretability techniques.

- This demonstrates the efficacy of TaSc in providing more faithful attention-based explanations than strong baselines without TaSc (Nguyen, 2018;Atanasova et al., 2020).

- Motivated by this, we aim to improve the effectiveness of neural models in providing more faithful attention-based explanations for text classification, by introducing noncontextualised information in the model.

- However, our main aim is not to improve predictive performance but the faithfulness of attention-based explanations, which we illustrate below.




# [Generation-Augmented Retrieval for Open-Domain Question Answering](https://aclanthology.org/2021.acl-long.316/)
- 3 Generation-Augmented Retrieval

- (1) We propose Generation-Augmented Retrieval (GAR), which augments queries with heuristically discovered relevant contexts through text generation without external supervision or time-consuming downstream feedback.

- In this work, we propose Generation-Augmented Retrieval and demonstrate that the relevant contexts generated by PLMs without external supervision can significantly enrich query semantics and improve retrieval accuracy.

- In this paper, we propose Generation-Augmented Retrieval (GAR), which augments a query through text generation of a pre-trained language model (PLM).




# [Discriminative Reranking for Neural Machine Translation](https://aclanthology.org/2021.acl-long.563/)
- First, we notice that all methods improve over the beam search output with gains ranging from 1.0 to 4.1 BLEU.

- Indeed, recent generative reranking approaches applied to NMT, such as Noisy-Channel Decoding (NCD, which leverages a pre-trained language model and a backward model, show strong improvements over beam search outputs, as demonstrated in recent WMT evaluations .

- Our method is inspired by the seminal work of and who introduced and popularized discriminative reranking to SMT.

- In this paper, we explore whether training large transformer models using the reranking objective can further improve performance.




# [Compare to The Knowledge: Graph Neural Fake News Detection with External Knowledge](https://aclanthology.org/2021.acl-long.62/)
- We construct a directed heterogeneous document graph incorporating topics and entities.

- Based on the directed heterogeneous document graph, we develop a heterogeneous graph attention network to learn topic-enriched news representations and contextual entity representations.

- In this paper, we propose a novel end-to-end graph neural model CompareNet which compares the news to the external knowledge for fake news detection.

- 1 https://en.wikipedia.org/wiki/Mammography 2 https://github.com/ytc272098215/FakeNewsDetection In summary, our main contributions include: 1) In this paper, we propose a novel end-to-end graph neural model CompareNet which compares the news to the external knowledge through entities for fake news detection.




# [Lightweight Cross-Lingual Sentence Representation Learning](https://aclanthology.org/2021.acl-long.226/)
- Concerning the training tasks, we propose a novel cross-lingual language model, which combines SMLM and XTR.

- We perform cross-lingual sentence representation learning by a lightweight dual-transformer framework.

- In this paper, we presented a lightweight dualtransformer based cross-lingual sentence representation learning method.

- Moreover, we add a fullyconnected layer before computing the loss of the cross-lingual language model inspired by .




# [PLOME: Pre-training with Misspelled Knowledge for Chinese Spelling Correction](https://aclanthology.org/2021.acl-long.233/)
- We propose PLOME, a pre-trained masked language model with misspelled knowledge for CSC.

- In this paper, we propose PLOME, a Pre-trained masked Language mOdel with Misspelled knowl-edgE, for Chinese spelling correction.

- We summarize our contributions as follows: (1) PLOME is the first task-specific language model designed for Chinese spelling correction.

- To the best of our knowledge, PLOME is the first task-specific language model for CSC, which jointly learns semantics and misspelled knowledge




# [The Possible, the Plausible, and the Desirable: Event-Based Modality Detection for Language Processing](https://aclanthology.org/2021.acl-long.77/)
- First, we change the modal sense labels to be intuitive and self-explanatory.

- 1229 of the modal triggers are modal verbs.

- Next, we aim to jointly predict the modal triggers and their modified events.

- How should we label these fine-grained modal senses?




# [PairRE: Knowledge Graph Embeddings via Paired Relation Vectors](https://aclanthology.org/2021.acl-long.336/)
- To overcome the problem of modeling 1-to-N/Nto-1/N-to-N complex relations and enrich the capabilities for different relation patterns, we propose a model with paired vectors for each relation.

- We also find the paired relation vectors enable an adaptive adjustment of the margin in loss function, which alleviates the modeling problem for complex relations.

- Here we present PairRE, an embedding method that is capable of encoding complex relations and multiple relation patterns simultaneously.

- The recent state-ofthe-art RotatE shows promising results to encode symmetry/antisymmetry, inverse and composition relations.




# [Tree-Structured Topic Modeling with Nonparametric Neural Variational Inference](https://aclanthology.org/2021.acl-long.182/)
- By connecting the network layers with dependency matrices, the model is able to extract an explainable tree-structured hierarchy.

- In (Griffiths et al., 2004), a tree-structured topic model called hLDA was first proposed by introducing a nested Chinese restaurant process (nCRP).

- The results indicate that our model is able to learn a reasonable tree-structured topic hierarchy with low redundancy.

- To address these limitations, we propose a novel nonparametric neural method to generate tree-structured topic hierarchies, namely nonparametric Tree-Structured Neural Topic Model (nTSNTM) 1 .




# [Psycholinguistic Tripartite Graph Network for Personality Detection](https://aclanthology.org/2021.acl-long.326/)
- • We propose a novel tripartite graph network, TrigNet, with a flow GAT to reduce the computational cost in graph learning.

- In this work, we proposed a novel psycholinguistic knowledge-based tripartite graph network, TrigNet, for personality detection.

- Our contributions are summarized as follows: • This is the first effort to use a tripartite graph to explicitly introduce psycholinguistic knowledge for personality detection, providing a new perspective of using domain knowledge.

- Figure 2 presents the overall architecture of the proposed TrigNet, which consists of a tripartite graph network and a BERT-based graph initializer.




# [Locate and Label: A Two-stage Identifier for Nested Named Entity Recognition](https://aclanthology.org/2021.acl-long.216/)
- We denote them as span proposals and contextual spans, respectively.

- Our main contributions are as follow: • Inspired by the two-stage detector popular in object detection, we propose a novel twostage identifier for NER of locating entities first and labeling them later.

- • We make effective use of boundary information.

- The regressor locates entities by adjusting the boundaries of span proposals to improve the quality of candidate spans.




# [REDDITBIAS: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models](https://aclanthology.org/2021.acl-long.151/)
- The contributions of this work are threefold: 1) we construct REDDITBIAS, a resource for multi-dimensional bias evaluation and mitigation dedicated to conversational AI.

- Consisting of manuallyannotated biased comments from Reddit, REDDIT-BIAS is the first real-world resource dedicated to multi-dimensional analysis (gender, race, religion, queerness) of biases in dialog models.

- We presented REDDITBIAS, a comprehensive resource for bias evaluation and debiasing of conversational LMs.

- Here, we provide (1) a brief overview of bias measures and mitigation methods and their usage in (2) language generation and, specifically, in (3) dialog.




# [Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting](https://aclanthology.org/2021.acl-long.465/)
- We explored the task of difficulty-controllable question generation, with question difficulty redefined as the inference steps required to answer it.

- for transfer learning in question generation.

- In summary, our contributions are as follows: • To the best of our knowledge, this is the first work of difficulty-controllable question generation, with question difficulty defined as the inference steps to answer it; • We propose a novel framework that achieves DCQG through step-by-step rewriting under the guidance of an extracted reasoning chain; • We build a dataset that can facilitate training of rewriting questions into more complex ones, paired with constructed context graphs and the underlying reasoning chain of the question.

- In this work, we redefine the difficulty level of a question as the number of inference steps required to answer it, which reflects the requirements on reasoning and cognitive abilities (Pan et al., 2019).




# [Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution](https://aclanthology.org/2021.acl-long.377/)
- In this work, we present invisible textual backdoors that are activated by a learnable combination of word substitution, in the hope of drawing attention to the security threats faced by NLP models.

- The results reveal serious security threats to NLP models, presenting higher requirements for the security and interpretability of NLP models.

- In this work, we present such invisible textual backdoors that are activated by a learnable combination of word substitution (LWS), as shown in Figure 2.

- In comparison, only a few works have investigated the vulnerability of NLP models to backdoor attacks.




# [KACE: Generating Knowledge-Aware Contrastive Explanations for Natural Language Inference](https://aclanthology.org/2021.acl-long.196/)
- The contributions of this paper are as follows: • We introduce a novel knowledge-aware contrastive explanation generation framework (KACE) for natural language inference tasks.

- In this paper, we use knowledgeaware pre-trained language model to generate contrastive explanation.

- Moreover, we train an NLI model enhanced with contrastive explanations and achieve the new stateof-art performance on SNLI.

- In this paper, we focus on knowledge-aware contrastive explanation generation for NLI.




# [LEXFIT: Lexical Fine-Tuning of Pretrained Language Models](https://aclanthology.org/2021.acl-long.410/)
- We can expose this knowledge by rewiring their parameters through lexical fine-tuning, and turn the LMs into universal (decontextualized) word encoders.

- We proposed LEXFIT, a lexical fine-tuning procedure which transforms pretrained LMs such as BERT into effective decontextualized word encoders through dual-encoder architectures.

- Lexical Fine-Tuning Objectives.

- We also probe what amount of lexical knowledge is required to turn BERTs into effective decontextualized word encoders by running tests with reduced lexical sets P sampled from the full set.




# [GTM: A Generative Triple-Wise Model for Conversational Question Generation](https://aclanthology.org/2021.acl-long.271/)
- We propose a generative triple-wise model for generating appropriate questions in open-domain conversations, named GTM.

- There are one-to-many mappings in both PQ and QA pairs.

- The main contribution is threefold: • To generate coherent and informative questions in the CQG task, we propose a generative triple-wise model that models the semantic relationship of a triple in three levels: PQA, PQ, and QA.

- Third, higher distinct values illustrate that one-to-many mappings in PQ and QA pairs make the generated responses more diverse.




# [Prevent the Language Model from being Overconfident in Neural Machine Translation](https://aclanthology.org/2021.acl-long.268/)
- Accordingly, we propose a Margin-based Token-level Objective (MTO) to maximize the Margin.

- Then we propose Margin-based Token-level and Sentence-level objectives to maximize the Margin.

- Then we put forward the token-level (Section 3.2) and sentencelevel (Section 3.3) optimization objectives to maximize the Margin.

- Therefore, based on the MTO, we further propose a Margin-based Sentence-level Objective (MSO) by adding a dynamic weight function to alleviate the negative effect of these "dirty data".




# [Structural Pre-training for Dialogue Comprehension](https://aclanthology.org/2021.acl-long.399/)
- In this work, we present SPIDER (Structural Pre-traIned DialoguE Reader), a structural language modeling method to capture dialogue exclusive features.

- In this paper, we focus on the task-related adaptation of the pre-trained language models and propose SPIDER (Structural Pre-traIned DialoguE Reader), a structural language modeling method to capture dialogue exclusive features.

- Then, we will introduce our designed language modeling objectives for dialogue scenarios, including utterance order restoration (UOR) and sentence backbone regularization (SBR).

- This section presents our proposed method SPI-DER (Structural Pre-traIned DialoguE Reader).




# [Reasoning over Entity-Action-Location Graph for Procedural Text Understanding](https://aclanthology.org/2021.acl-long.396/)
- A general framework to systematically model the rich types of relations among entities, actions, and locations is essential to procedural text understanding.

- Thus, it is highly valuable for both state and location tracking of entities.

- However, these methods can not systematically capture the relations among entities, actions, and locations, and entity-action and entity-entity relations are ignored.

- Finally, the prediction module leverages the graph-based representations to predict the state and location.




# [Cross-language Sentence Selection via Data Augmentation and Rationale Training](https://aclanthology.org/2021.acl-long.300/)
- We (i) propose a data augmentation and negative sampling scheme to create a synthetic training set of cross-lingual query-sentence pairs with binary relevance judgements, and (ii) demonstrate the effectiveness of a Supervised Embedding-based Cross-Lingual Relevance (SECLR) model trained on this data for low-resource sentence selection tasks on text and speech.

- We use a simple data augmentation and negative sampling scheme to generate a labeled dataset of relevant and irrelevant pairs of queries and sentences from these noisy parallel corpora.

- Since this work targets cross-language sentence selection in a low-resource setting, we perform a training data ablation study to understand how training data size affects effectiveness.

- In this work, we presented a supervised crosslingual embedding-based query relevance model, SECLR, for cross-language sentence selection and also applied a rationale training objective to further increase model performance.




# [Unified Dual-view Cognitive Model for Interpretable Claim Verification](https://aclanthology.org/2021.acl-long.5/)
- To address the deficiencies, we propose a unified Dual-view model based on Collective and Individual Cognition (CICD) for interpretable claim verification, which focuses on discovering global evidence and local key evidence, respectively, and then strengthens the consistent shared evidence between the both.

- To weaken the bias of individual cognition view and strengthen the consistent shared evidence between global and local evidence, we project inconsistent loss to suppress the divergence.

- Our contributions are summarized: • A novel framework integrating interdisciplinary knowledge on interpretable claim verification is explored, which discovers global and local evidence from the perspectives of collective and individual cognition to interpret verified results. • Proposed CED captures word-level (individual) and sentence-level (holistic) opinions, and reasonably adjusts the proportion between them, which generates global evidence of the view of all users.

- In this paper, we proposed a unified dual-view model based on the perspectives of collective and individual cognition for interpretable claim verification, which constructed collective cognition view-based encoder-decoder module to generate global evidence and designed individual cognition view-based selected interaction module to explore local key evidence segments.




# [A Neural Transition-based Joint Model for Disease Named Entity Recognition and Normalization](https://aclanthology.org/2021.acl-long.219/)
- We transform the end-to-end disease recognition and normalization task as an action sequence prediction task.

- We define the end-to-end disease recognition and normalization task as follows.

- In this work, we propose a neural transition-based joint model for disease named entity recognition and normalization.

- In this work, we propose a neural joint model to alleviate these two issues.




# [Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering](https://aclanthology.org/2021.acl-long.481/)
- In this paper, we proposed a Motion-Appearance Synergistic Networks to fuse and create a synergy between motion and appearance features.

- First, we propose Motion-Appearance Synergistic Networks (MASN) for video question answering based on three modules, the motion module, the appearance module, and the motionappearance fusion module.

- Finally, the Motion-Appearance Fusion module modulates the amount of motion and appearance information utilized and integrates them based on question context.

- Synergistic Networks (MASN) for video question answering which consist of three kinds of modules: the motion module, the appearance module, and the motion-appearance fusion module.




# [XLPT-AMR: Cross-Lingual Pre-Training via Multi-Task Learning for Zero-Shot AMR Parsing and Text Generation](https://aclanthology.org/2021.acl-long.73/)
- • We propose an effective cross-lingual pretraining approach for zero-shot AMR parsing and AMR-to-text generation.

- In this paper we proposed a cross-lingual pretraining approach via multi-task learning for zeroshot AMR parsing and AMR-to-text generation.

- Performance of English AMR parsing and AMR-to-Text generation.

- In this paper we build strong cross-lingual pre-trained models for both AMR parsing and AMR-to-text generation.




# [MultiMET: A Multimodal Dataset for Metaphor Understanding](https://aclanthology.org/2021.acl-long.249/)
- It also offers a set of baseline results of various tasks and shows the importance of combining multimodal cues for metaphor understanding.

- • We propose three tasks to evaluate finegrained multimodal metaphor understanding abilities, including metaphor detection, sentiment analysis, and intent detection in multimodal metaphor.

- This paper presents the creation of a novel resource, a large-scale multimodal metaphor dataset, MultiMET, with manual fine-gained annotation for metaphor understanding and research.

- Our main contributions are as follows: • We create a novel multimodal dataset consisting of 10,437 text-image pair samples from a range of resources including social media (Twitter and Facebook), and advertisements.




# [Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators](https://aclanthology.org/2021.acl-long.418/)
- First, we introduce a new fine-tuning strategy that only focuses on the parameters of auxiliary tensors, so the number of fine-tuning parameters can be largely reduced.

- If this could be achieved, we can derive a lighter network meanwhile reduce the parameters to be fine-tuned.

- If this approach was feasible, this will largely reduce the parameters to be fine-tuned.

- So far, most of pre-trained language models (PLM) are developed based on stacked Transformer layers (Vaswani et al., 2017).




# [Data Augmentation with Adversarial Training for Cross-Lingual NLI](https://aclanthology.org/2021.acl-long.401/)
- Cross-lingual Inference Classification.

- Data Augmentation.

- In this paper, we propose a novel data augmentation scheme to synthesize controllable and much less noisy data for cross-lingual NLI.

- To address this, this paper proposes a novel data augmentation strategy with label rectification to build synthetic examples, outperforming even models trained with larger amounts of ground-truth data.




# [Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical Core-Fringe Approach](https://aclanthology.org/2021.acl-long.282/)
- Second, to leverage the graph and support fine-grained domains without relying on domain-specific corpora, we propose hierarchical core-fringe learning, which learns the domain relevance of core and fringe terms jointly in a semi-supervised manner contextualized in the hierarchy of the domain.

- Hierarchical Positive-Unlabeled Learning.

- In this section, we aim to design learning methods to learn the fine-grained domain relevance of core and fringe terms jointly.

- In this way, core and fringe terms help each other, and the domain relevance is learned jointly.




# [Importance-based Neuron Allocation for Multilingual Neural Machine Translation](https://aclanthology.org/2021.acl-long.445/)
- These language-specific neurons are important for preserving the language-specific knowledge.

- The experiments prove that our method can get superior translation results with better general and language-specific knowledge.

- Our contributions can be summarized as follows: • We propose a method that can improve the translation performance of the MNMT model without introducing any specialized modules or adding new parameters.

- We divide neurons to general neurons and language-specific neurons to retain general knowledge and capture language-specific knowledge without model capacity incremental and specialized design.




# [Engage the Public: Poll Question Generation for Social Media Posts](https://aclanthology.org/2021.acl-long.3/)
- We have presented a novel task to generate social media poll questions.

- The topic representations are then incorporated into a sequence-to-sequence (S2S) architecture to decode poll questions word by word.

- Human evaluation further demonstrates our models' capability to generate poll questions relevant to the source post, fluent in language, and particularly engaging to draw user attentions for discussions.

- This suggests the sparsity of social media data.




# [Probing Toxic Content in Large Pre-Trained Language Models](https://aclanthology.org/2021.acl-long.329/)
- In this paper, we present a methodology to probe toxic content in pre-trained language models using commonsense patterns.

- The human annotations confirm the existence of toxicity in English, French, and Arabic PTLMS and show that, despite their imperfections, the classifiers can be used as toxicity pointers.

- Finally, we compare counts of potentially incoherent associations produced by various PTLMs in English, French and Arabic.

- For the sake of normalizing English, French, and Arabic patterns 2 , we do not consider the pronoun they.




# [Multi-View Cross-Lingual Structured Prediction with Minimum Supervision](https://aclanthology.org/2021.acl-long.207/)
- We propose a novel multi-view framework to selectively transfer knowledge from multiple sources by utilizing a small amount of labeled dataset.

- The contributions of this work are: 1. We propose to leverage a small number of target labeled data to better aggregate multiple source models.

- We propose a novel multi-view framework to achieve a good trade-off between the two views.

- Why the Multi-View Framework Works?




# [Automated Generation of Storytelling Vocabulary from Photographs for Use in AAC](https://aclanthology.org/2021.acl-long.108/)
- Figure 1: An AAC app design demonstrating how context-related vocabulary generated by our method might be presented for use in subsequent conversations.

- The similar performance across photos with different levels of contextual information (RQ2) suggests that our method is robust to variations in the input photograph.

- The design space for generating AAC storytelling vocabulary directly from photographs is vast and under explored.

- Our method generates a rank of key words and short narrative phrases from a single 4 input photo for scaffolding storytelling.




# [Towards Propagation Uncertainty: Edge-enhanced Bayesian Graph Convolutional Networks for Rumor Detection](https://aclanthology.org/2021.acl-long.297/)
- • We design a new edge-wise consistency training framework to optimize the model with unlabeled latent relations.

- The main contributions of this work are summarized as follows: • We propose novel Edge-enhanced Bayesian Graph Convolutional Networks (EBGCN) to handle the uncertainty in a probability manner.

- Besides, we design an edge-wise consistency training framework incorporating unsupervised relation learning to enforce the consistency on latent relations.

- Moreover, due to the unavailable of missing or inaccurate relations for training the proposed model, we design a new edge-wise consistency training framework.




# [Mid-Air Hand Gestures for Post-Editing of Machine Translation](https://aclanthology.org/2021.acl-long.527/)
- Guided by a gesture elicitation study with 14 freelance translators, we develop a prototype supporting midair hand gestures for cursor placement, text selection, deletion, and reordering.

- Due to the COVID-19 pandemic we conducted an online GES.

- In addition, the gesture pointing (where a participant points with the index finger to place the cursor on the item) was highly preferred for single item selection.

- In summary, the study has shown positive attitudes towards using mid-air hand gestures in combination with the keyboard for specific PE tasks.




# [A Sweet Rabbit Hole by DARCY: Using Honeypots to Detect Universal Trigger's Adversarial Attacks](https://aclanthology.org/2021.acl-long.296/)
- Robustness to Varying Attacks.

- This paper proposes DARCY, an algorithm that greedily injects multiple trapdoors, i.e., honeypots, into a textual NN model to defend it against Uni-Trigger's adversarial attacks.

- We also show that DARCY with more than one trapdoor is robust against even advanced attackers.

- • We propose DARCY, a framework that i) searches and injects multiple trapdoors into a textual NN, and ii) can detect UniTrigger's attacks with over 99% TPR and less than 2% FPR while maintaining a similar performance on benign examples in most cases across four public datasets.




# [Lower Perplexity is Not Always Human-Like](https://aclanthology.org/2021.acl-long.405/)
- (2) The effect of surprisals for modeling human reading behavior was calculated using a linear mixedeffects regression

- We found that syntactic category is the most influential factor for modeling gaze duration, at least in this experiment.

- Notably, surprisal was effective for gaze duration modeling in all the Japanese LMs.

- Settings: In a preliminary experiment, we observed that the syntactic category (similar to partof-speech) was the most dominant linguistic factor for explaining the difference in human gaze duration in Japanese sentences (see Appendix D).




# [Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering](https://aclanthology.org/2021.acl-long.564/)
- Active Learning.

- Active Learning with Global Reasoning.

- We evaluate the 8 active learning strategies across the 5 models described in the previous section.

- The previous section shows that active learning fails to improve over random acquisition on VQA across models and datasets.




# [Evaluation of Thematic Coherence in Microblogs](https://aclanthology.org/2021.acl-long.530/)
- • We provide guidelines for the annotation of thematic coherence in microblog clusters and construct a dataset of clusters annotated for thematic coherence spanning two different domains (political tweets and COVID-19 related tweets).

- The main contributions of this paper are: • We define the task of assessing thematic coherence in microblogs and use it as the basis for creating microblog clusters (Sec. 3).

- Here we present (a) the creation of a corpus of topic clusters of tweets C and (b) the annotation process for thematic coherence.

- We have found that TGMs correlate much better with human judgement of thematic coherence compared to metrics employed in topic model evaluation.




# [Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers](https://aclanthology.org/2021.acl-long.566/)
- In this paper, we propose the first large-scale meta-evaluation of MT in which we manually annotated 769 research papers published from 2010 to 2020.

- The accumulation of these pitfalls and the concerning trends we observed lead us to propose a guideline for automatic MT evaluation.

- For the latter, we also propose a simple scoring method for the meta-evaluation of MT. Note that the proposed guideline and scoring method only cover the aspects discussed in this paper.

- Our meta-evaluation identified pitfalls in the MT evaluation in most of the annotated papers.




# [Rewriter-Evaluator Architecture for Neural Machine Translation](https://aclanthology.org/2021.acl-long.443/)
- We present prioritized gradient descent (PGD) to train the proposed architecture.

- To address this problem, we introduce a novel architecture, Rewriter-Evaluator.

- We also propose prioritized gradient descent (PGD) that facilitates training the rewriter and the evaluator both jointly and efficiently.

- Firstly, Rewriter-Evaluator significantly improves the translation quality of NMT models.




# [On Compositional Generalization of Neural Machine Translation](https://aclanthology.org/2021.acl-long.368/)
- Compositional Generalization.

- In this paper, we study compositional generalization in the context of machine translation.

- Our work is in line but we discuss robustness from the perspective of compositional generalization.

- In contrast to these studies, we quantitatively measure compositionality of NMT under compound translation error rate.




# [Question Answering Over Temporal Knowledge Graphs](https://aclanthology.org/2021.acl-long.520/)
- In this paper we propose CRONQUESTIONS, a new dataset for Temporal KGQA.

- Temporal Knowledge Graphs (Temporal KGs) are multi-relational graph where each edge is associated with a time duration.

- We also propose a new method, CRONKGQA, that is able to leverage Temporal KG Embeddings to perform TKGQA.

- 3 CRONQUESTIONS: The new Temporal KGQA dataset CRONQUESTIONS, our Temporal KGQA dataset consists of two parts: a KG with temporal annotations, and a set of natural language questions requiring temporal reasoning.




# [KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers](https://aclanthology.org/2021.acl-long.176/)
- In this paper, we propose a few-shot evaluation to inspire future research of practical text-to-SQL parsers.

- KaggleDBQA provides two resources to facilitate real-world applications of text-to-SQL parsing.

- Each database has associated plain-text documentation that can assist text-to-SQL parsing.

- We encourage adopting this regime for established text-to-SQL benchmarks.




# [InfoSurgeon: Cross-Media Fine-grained Information Consistency Checking for Fake News Detection](https://aclanthology.org/2021.acl-long.133/)
- In this paper, we propose a new task: finegrained, knowledge element-level cross-media information consistency checking.

- Fake News Detection.

- 2019. Learning hierarchical discourse-level structure for fake news detection.

- 3 Fake News Detection




# [Modularized Interaction Network for Named Entity Recognition](https://aclanthology.org/2021.acl-long.17/)
- In this paper, we have proposed a novel Modularized Interaction Network (MIN) model for the NER task.

- The proposed MIN model utilizes both segment-level information and word-level dependencies, and incorporates an interaction mechanism to support information sharing between boundary detection and type prediction to enhance the performance for the NER task.

- In summary, the main contributions of this paper include: • We propose a novel Modularized Interaction Network (MIN) model which utilizes both the segment-level information from segmentbased models and word-level dependencies from sequence labeling-based models in order to enhance the performance of the NER task.

- This section presents our proposed Modularized Interaction Network (MIN) for NER.




# [Annotating Online Misogyny](https://aclanthology.org/2021.acl-long.247/)
- In this work, we have documented the construction of a dataset for training systems for automatic detection of online misogyny.

- 3. Dataset: We present a new, annotated corpus of Danish social media posts, Bajer, 1 annotated for misogyny, including analysis of class balance, word frequencies, Inter-Annotator Agreement (IAA), annotation errors, and classification baseline.

- 2. Model: We present a taxonomy and annotation codebook grounded in previous research on automatic detection of misogyny as well as social science terminology.

- This paper investigates the research question: How might we design a comprehensive annotation process which results in high quality data for automatically detecting misogyny?




# [Structurizing Misinformation Stories via Rationalizing Fact-Checks](https://aclanthology.org/2021.acl-long.51/)
- We identify ten types of misinformation stories, a preview of which are shown in Figure 1.

- Structure of misinformation stories.

- With a large corpus of fact-checks, these phrases would accumulate and reveal prevalent types of misinformation stories.

- In this paper, we identify ten prevalent misinformation types with rationalized models on fact-checks and analyze their evolution over the last ten years and between notable events.




# [COVID-Fact: Fact Extraction and Verification of Real-World Claims on COVID-19 Pandemic](https://aclanthology.org/2021.acl-long.165/)
- We release a dataset of 4,086 claims concerning the COVID-19 pandemic, together with supporting and refuting evidence.

- We propose a novel semi-automatic method to build a fact-checking dataset for COVID-19 (COVID-Fact) with the goal of facilitating all the above tasks.

- • Automatic generation of counter-claims (Section 2.2).

- We propose an approach to generate counter-claims automatically (see Table 1 for examples).




# [CCMatrix: Mining Billions of High-Quality Parallel Sentences on the Web](https://aclanthology.org/2021.acl-long.507/)
- Our contributions are: • development of a new highly efficient and parallelized processing pipeline to confront the substantial computational challenge; • unprecedented size: 10.8 billion mined parallel sentences in 90 different languages; • all these resources are freely available; • we demonstrate the quality of our mined data on a variety of machine translation benchmarks, such as TED, WMT, and WAT, achieving highly competitive results.

- This procedure yields 10.8 billion parallel sentences, out of which only 2.9 billions are aligned with English.

- We show that margin-based mining in a joint multilingual sentence embedding space can be scaled to monolingual texts of more than 71 billion unique sentences in 90 languages, including several low resource languages.

- We leverage massively multilingual sentence embeddings and a margin-based criterion to mine parallel sentences.




# [TGEA: An Error-Annotated Dataset and Benchmark Tasks for Text Generation from Pretrained Language Models](https://aclanthology.org/2021.acl-long.469/)
- • TGEA, to the best of our knowledge, is the first dataset built on machine-generated texts from state-of-the-art pretrained language models with rich annotations.

- we use the state-of-the-art BERT-GEC model (Kaneko et al., 2020) as the baseline for this task, which is an encoder-decoder model using representations learned by PLMs as additional inputs.

- (2) Erroneous and associated span detection.

- We also experiment zero-shot generation on the test set.




# [SemFace: Pre-training Encoder and Decoder with a Semantic Interface for Neural Machine Translation](https://aclanthology.org/2021.acl-long.348/)
- The key point is to use a semantic interface to connect the pre-trained encoder and decoder.

- We propose two types of semantic interfaces, namely CL-SemFace and VQ-SemFace.

- We propose SemFace, a better pre-training method for neural machine translation.

- Our contributions are listed as follows: • To the best of our knowledge, this is the first work to investigate and define a semantic interface between encoder and decoder for the MT pre-train-finetune framework.




# [Mitigating Bias in Session-based Cyberbullying Detection: A Non-Compromising Approach](https://aclanthology.org/2021.acl-long.168/)
- To address these challenges, we propose a context-aware and model-agnostic debiasing training framework for cyberbullying detection.

- In this work, we examined unintended biases in datasets for session-based cyberbullying detection.

- This paper aims to mitigate the unintended bias in cyberbullying detection in social media sessions.

- To alleviate these unintended biases, we propose an effective debiasing strategy by leveraging techniques in RL.




# [Continuous Language Generative Flow](https://aclanthology.org/2021.acl-long.355/)
- Question Generation Model.

- We have proposed a language generative flow model with non-autoregressive and autoregressive variants.

- We generally follow previous work on evaluation metrics across density estimation, question generation, and question answering augmentation.

- In addition to improving language generation quality, we also use the proposed autoregressive flow model for data augmentation.




# [CoSQA: 20,000+ Web Queries for Code Search and Question Answering](https://aclanthology.org/2021.acl-long.442/)
- We also propose a novel code contrastive learning method, named CoCLR, to incorporate artificially generated instances into training.

- We demonstrate that CoSQA is an ideal dataset for code question answering and code search.

- We perform experiments on the task of querycode matching on two tasks: code question answering and code search.

- Furthermore, to better leverage the CoSQA dataset for querycode matching, we propose a code contrastive learning method (CoCLR) to produce more artificially generated instances for training.




# [Hate Speech Detection based on Sentiment Knowledge Sharing](https://aclanthology.org/2021.acl-long.556/)
- To overcome the weaknesses of previous works, we propose a hate speech detection framework based on sentiment knowledge sharing (SKS) 1 .

- Hate speech detection and sentiment analysis are highly correlated, so that sentiment knowledge sharing can improve the performance of hate speech detection.

- SKS is our proposed model which detects hate speech based on sentiment knowledge sharing.

- We show that sentiment knowledge sharing improves system performance over the baselines and advances hate speech detection.




# [Cascaded Head-colliding Attention](https://aclanthology.org/2021.acl-long.45/)
- We propose cascaded head-colliding attention (CODA, Figure 1b).

- CODA explicit models of the interactions among attention heads through a hierarchical variational distribution.

- Vanilla vs. head-colliding attention

- (b) Right: PGM diagram of a 3-layer cascaded head-colliding attention (CODA).




# [Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation](https://aclanthology.org/2021.acl-long.387/)
- MedWriter introduces a novel hierarchical retrieval mechanism working with a hierarchical language decoder to automatically learn the dynamic report and sentence templates from the data for generating accurate and professional medical reports.

- On top of the retrieval modules, we design a new multi-query attention mechanism to fuse the retrieved information for medical report generation.

- Different from these models, MedWriter is able to automatically learn both report-level and sentence-level templates from the data, which significantly enhances the model applicability.

- To sum up, our contributions are: • To the best of our knowledge, we are the first to model the memory retrieval mechanism in both report and sentence levels.




# [Introducing Orthogonal Constraint in Structural Probes](https://aclanthology.org/2021.acl-long.36/)
- Orthogonal Structural Probes are less vulnerable to memorization.

- We introduce orthogonality to structural probes.

- 2. Can we find other phenomena such as lexical hypernymy and a word's absolute position in a sentence using Orthogonal Structural Probe? How vulnerable are the probes to memorizing random data?

- We have expanded structural probing to new types of auxiliary tasks and introduced a new setting, Orthogonal Structural Probe, in which probes can be optimized jointly.




# [Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning](https://aclanthology.org/2021.acl-long.568/)
- Finally, we connect intrinsic dimensional with low dimensional task representations and compression-based generalization bounds to provide intrinsic-dimension-based generalization bounds independent of the full parameter count, further justifying why these methods generalize so well in practice across tasks.

- • Lastly, we show that compression based generalization bounds can be applied to our intrinsic dimension framework to provide generalization bounds for large pre-trained models independent of the pre-trained model parameter count.

- A lower intrinsic dimension is strongly correlated with better evaluation performance.

- 4 Intrinsic Dimensionality of Common NLP Tasks




# [Evidence-based Factual Error Correction](https://aclanthology.org/2021.acl-long.256/)
- In this paper, we propose Factual Error Correction, as an explainable alternative for fact verification.

- In this paper, we demonstrated that the task can be performed with distant supervision in the form of claims labeled by evidence supporting or refuting them.

- A challenge for factual error correction is the lack of datasets consisting of claims paired with their corrections.

- To address this data scarcity, we make use of distant supervision to incorporate retrieved evidence into generating the corrections.




# [Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy](https://aclanthology.org/2021.acl-long.281/)
- This paper has presented a systematic study of word meaning representation in context.

- We use this resource to perform a systematic evaluation of contextualized word meaning representations.

- We have created a new resource to investigate how vector models represent word meanings in context.

- The results suggest that the best monolingual models based on Transformers (Vaswani et al., 2017) can identify homonyms having different meanings adequately.




# [Learning Language Specific Sub-network for Multilingual Machine Translation](https://aclanthology.org/2021.acl-long.25/)
- • LaSS shows its strong generalization performance at easy adaptation to new language pairs and zero-shot translation.

- Secondly, we show that LaSS can also boost performance in zero-shot translation scenario, obtaining performance gains by up to 26.5 BLEU.

- Besides, LaSS can boost zero-shot translation by up to 26.5 BLEU.

- In this paper, we propose to learn Language-Specific Sub-network (LaSS) for multilingual NMT.




# [Search from History and Reason for Future: Two-stage Reasoning on Temporal Knowledge Graphs](https://aclanthology.org/2021.acl-long.365/)
- Thus, we propose a new model called CluSTeR, consisting of two stages, Clue Searching (Stage 1) and Temporal Reasoning (Stage 2).

- As illustrated in Figure 2, the model consists of two stages, clue searching and temporal reasoning.

- In general, this paper makes the following contributions: • We formulate the TKG reasoning task from the view of human cognition and propose a two-stage model, CluSTeR, which is mainly composed of a RL-based clue searching stage and a GCN-based temporal reasoning stage.

-  for the temporal reasoning task addressed in this paper.




# [Span-based Semantic Parsing for Compositional Generalization](https://aclanthology.org/2021.acl-long.74/)
- We define span-based semantic parsing as follows.

- We now present our experimental evaluation, which demonstrates the advantage of span-based parsing for compositional generalization.

- Our code and data are available at https:// github.com/jonathanherzig/span-based-sp.

- More importantly, we show that this approach leads to dramatic gains in compositional generalization compared to autoregressive parsers.




# [Multi-perspective Coherent Reasoning for Helpfulness Prediction of Multimodal Reviews](https://aclanthology.org/2021.acl-long.461/)
- We design a novel Multi-perspective Coherent Reasoning method (denoted as MCR) to tackle the MRHP task.

- (2) We propose a multi-perspective coherent reasoning method for the MRHP task to conduct joint reasoning over texts and images from both the product and the review, and aggregate the signals to predict the helpfulness of multimodal reviews.

- The improvements of the intra-modal and inter-modal coherent reasoning in the product-review coherent reasoning module are also significant.

- We propose a multi-perspective coherent reasoning (MCR) method to solve MRHP task, which fully explores the product-review coherence and intra-review coherence from both textual and visual modalities.




# [xMoCo: Cross Momentum Contrastive Learning for Open-Domain Question Answering](https://aclanthology.org/2021.acl-long.477/)
- xMoCo jointly optimizes question-to-passage and passage-to-question matching, enabling using separate encoders for questions and passages, while efficiently maintains a large pool of negative samples like the original MoCo.

- To summarize, the main contributions of this work are as follows: • We proposes a new momentum contrastive learning method, Cross Momentum Contrast (xMoCo), which can learn question-passage matching where questions and passages require different encoders.

- To solve the problems mentioned above, we propose a new momentum contrastive learning method, for passages.

- To solve this problem, we propose a new contrastive learning method called Cross Momentum Contrastive Learning (xMoCo).




# [Modeling Fine-Grained Entity Types with Box Embeddings](https://aclanthology.org/2021.acl-long.160/)
- In this paper, we investigated a box-based model for fine-grained entity typing.

- Our box-based model outperforms the vector-based model on two benchmarks, Ultra-fine Entity Typing and OntoNotes, achieving state-ofthe-art-performance.

- Our box-based model achieves better accuracy compared to the vector-based model on all supertypes.

- However, the box-based model achieves slightly lower total error.




# [DESCGEN: A Distantly Supervised Dataset for Generating Abstractive Entity Descriptions](https://aclanthology.org/2021.acl-long.35/)
- In summary, our contributions include: • We propose a new dataset DESCGEN that includes challenging, abstractive entity summaries.

- DESCGEN contains about 37K entity descriptions from Wikipedia and Fandom.

- DESCGEN contains 37K entity descriptions extracted from Wikipedia and Fandom 2 .

- In summary, our analysis suggests there is room for improvement in extractive content selection and abstractive generation, particularly for new and emerging entities from less popular domains.




# [Structured Sentiment Analysis as Dependency Graph Parsing](https://aclanthology.org/2021.acl-long.263/)
- Moreover, we cast sentiment analysis as a dependency graph parsing problem, where the sentiment expression is the root node, and the other elements have arcs which model the relationships between them.

- In this paper, we have proposed a dependency graph parsing approach to structured sentiment analysis and shown that these models outperform state-of-the-art sequence labeling models on five benchmark datasets.

- As such, we propose a unified approach to structured sentiment which jointly predicts all elements of an opinion tuple and their relations.

- We further propose methods to inject linguistic structure into the sentiment graphs using syntactic dependencies.




# [WARP: Word-level Adversarial ReProgramming](https://aclanthology.org/2021.acl-long.381/)
- The approach based on frozen features does not require storing task-specific language models.

- The method outperforms existing methods with significantly more trainable parameters on GLUE benchmark tasks and shows an impressive performance in a few-shot setting on two SuperGLUE tasks.

- Every input sentence can be concatenated with task-specific pretrained prompts in advance.

- In contrast, our method does not inject taskspecific knowledge inside the body of the pretrained language model.




# [RADDLE: An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems](https://aclanthology.org/2021.acl-long.341/)
- We introduce RADDLE, a platform and collection of resources for evaluating and analyzing taskoriented dialog systems.

- We confirm (1) the utility of grounded pre-training and transfer learning methods in dialog systems: pre-training improves generalization in a limited data setting, and (2) adversarial training improves robustness, but still leaves room for improvement.

- To better understand the challenges posed by RADDLE, we conduct experiments with simple baselines and state-of-the-art task-oriented dialog models.

- In our experiments, SOLOIST Adv employs adversarial training in both task-specific pre-training and fine-tuning stages.




# [ERNIE-DOC: A Retrospective Long-Document Modeling Transformer](https://aclanthology.org/2021.acl-long.227/)
- In this paper, we proposed ERNIE-DOC, a document-level language pretraining model based on the Recurrence Transformers paradigm.

- Additionally, ERNIE-DOC is pretrained with a document-aware segment-reordering objective to explicitly learn the relationship among segments of a long context.

- Moreover, we introduce a segment-reordering objective to pretrain a document-level model.

- Benefitting from the much larger effective context length provided by the enhanced recurrence mechanism, the goal of the segment-reordering objective is to predict the correct order for the permuted set of segments of a long document, to explicitly learn the relationships among segments.




# [TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling](https://aclanthology.org/2021.acl-long.293/)
- Our main contributions are to: (1) present a new, flexible approach to few-shot style transfer, (2) use sentence adjacency as a means for inducing text style representations, (3) reframe style transfer as "targeted restyling" directional operations in style space, (4) introduce "tunable inference" for finergrained control of transfers, (5) show the effectiveness of "noisy" back-translation training, and (6) illustrate few-shot generalization to a range of style attributes including dialect, emotiveness, formality, politeness, and sentiment.

- Supervised style transfer has seen limited research due to the difficulty of obtaining parallel data.

- As mentioned at the outset, recent work on text style transfer falls into three classes: supervised, "unsupervised", and few-shot.

- An advantage of few-shot style transfer is that, in theory, a single model can perform transfer along any "dimension" of style given only a few exemplars, without the need for additional training.




# [BanditMTL: Bandit-based Multi-task Learning for Text Classification](https://aclanthology.org/2021.acl-long.428/)
- Moreover, BanditMTL can be seen to outperform all baselines and achieve state-of-the-art performance.

- It fills the task variance regularization gap in the field of MTL and achieves state-of-the-art performance in real-world text classification applications.

- In this paper, we propose a task-varianceregularized multi-task learning algorithm based on mirror gradient ascent-descent, dubbed Ban-ditMTL.

- The results demonstrate that applying variance regularization can improve the performance of a MTL model; moreover, BanditMTL is found to outperform several state-of-the-art multitask text classification methods.




# [A Cognitive Regularizer for Language Modeling](https://aclanthology.org/2021.acl-long.404/)
- Evidence for the UID hypothesis.

- In this paper, we propose a new experimental paradigm that uses modern-day NLP models to test the UID hypothesis.

- Moreover, observing lower perplexity in language models trained with this regularization would imply that the concept of UID is a good inductive bias for language modeling, thereby providing a new type of evidence for the UID hypothesis at scale.

- These results both provide a new form of evidence for the UID hypothesis and build on prior work exploring UID in modern-day NLP models.




# [Weight Distillation: Transferring the Knowledge in Neural Network Parameters](https://aclanthology.org/2021.acl-long.162/)
- In this work, we propose weight distillation to transfer knowledge in the parameters of the teacher network to the student network.

- It transfers the knowledge in parameters of the teacher network to the student network via a parameter generator.

- Our experiments on three machine translation tasks show that weight distillation consistently outperforms knowledge distillation by producing a faster and better student network.

- This fact implies that the knowledge in parameters is complementary to KD but missed.




# [ExCAR: Event Graph Knowledge Enhanced Explainable Causal Reasoning](https://aclanthology.org/2021.acl-long.183/)
- To fully exploit the potential of the evidence information, we present an Event graph knowledge enhanced explainable CAusal Reasoning (ExCAR) framework.

- We devise a novel explainable causal reasoning framework ExCAR.

- Given an event pair, ExCAR is able to obtain logical rules from a large-scale causal event graph to provide insight to inference results.

- Then ExCAR conducts causal reasoning based on the logical rules using a Conditional Markov Neural Logic Network.




# [ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences](https://aclanthology.org/2021.acl-long.303/)
- Atomic clauses are fundamental text units for understanding complex sentences.

- We presented a new task to decompose complex sentences into simple ones, along with DeSSE, a new dataset designed for this task.

- Building on these observations, we propose a neural model that learns to Accept, Break, Copy or Drop elements of a special-purpose sentence graph that represents word adjacency and grammatical dependencies, so the model can learn based on both kinds of graph proximity.

- We formulate the problem of converting complex sentences into covering sets of simple sentences as a graph segmentation problem.




# [End-to-End AMR Coreference Resolution](https://aclanthology.org/2021.acl-long.324/)
- We investigated a novel end-to-end multi-sentence AMR coreference resolution model using a graph neural network.

- We consider coreference resolution as the prerequisite for creating multi-sentence AMRs, proposing the first end-to-end model for this task.

- We propose an AMR coreference resolution model by extending an end-to-end text-based coreference resolution model (Lee et al., 2017).

- This verifies the effectiveness of the end-to-end framework.




# [Unsupervised Neural Machine Translation for Low-Resource Domains via Meta-Learning](https://aclanthology.org/2021.acl-long.225/)
- Overall, our contributions can be summarized as follows: • We apply a meta-learning approach for UNMT.

- We utilize the metalearning approach to address a low-resource challenge for unsupervised machine translation.

- Therefore, we introduce a new meta-learning approach for UNMT, called MetaUMT, for low-resource domains by defining each task as a domain.

- Moreover, we introduce an improved method called MetaGUMT, which enhances cross-domain generalization and maintains high-resource domain knowledge.




# [OoMMix: Out-of-manifold Regularization in Contextual Embedding Space for Text Classification](https://aclanthology.org/2021.acl-long.49/)
- In this work, we propose a novel approach to discovering and leveraging the out-of-manifold for contextual embedding regularization.

- In the end, the fine-tuning on the synthesized out-of-manifold embeddings tightly regularizes the contextual embedding space of BERT.

- In this section, we propose a novel mixup approach, termed as OoMMix, to regularize the outof-manifold in contextual embedding space for text classification.

- Finally, we provide in-depth analyses on our approach to further validate the effect of out-of-manifold regularization.




# [Language Model Evaluation Beyond Perplexity](https://aclanthology.org/2021.acl-long.414/)
- That is, we pose the question: Do neural language models exhibit the statistical tendencies of human language?

- Simulating Corpora from Language Models.

- In this work, we present a framework for determining the linguistic properties learned by language models through analysis of statistical trends in generated text.

- Language models are probability distributions over natural language sentences.




# [StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling](https://aclanthology.org/2021.acl-long.559/)
- Incorporating the new parsing mechanism, the dependency-constrained self-attention, and the Transformer architecture, we introduce a new model named StructFormer.

- First, we introduce a new neural model, StructFormer, that is able to simultaneously induce both dependency structure and constituency structure.

- Based on the framework, we propose StructFormer, a new unsupervised parsing algorithm that does unsupervised dependency and constituency parsing at the same time.

- We evaluate the proposed model on three tasks: Masked Language Modeling, Unsupervised Constituency Parsing and Unsupervised Dependency Parsing.




# [Vocabulary Learning via Optimal Transport for Neural Machine Translation](https://aclanthology.org/2021.acl-long.571/)
- On English-German translation, VOLT only takes 30 GPU hours to find vocabularies, while the traditional BPE-Search solution takes 384 GPU hours.

- To address the above problems, we propose a VOcabulary Learning approach via optimal Transport, VOLT for short.

- We evaluate our approach on multiple machine translation tasks, including WMT-14 English-German translation, TED bilingual translation, and TED multilingual translation.

- Thus, trial training is required to find the optimal size, which brings high computation costs.




# [Unsupervised Extractive Summarization-Based Representations for Accurate and Explainable Collaborative Filtering](https://aclanthology.org/2021.acl-long.232/)
- Nevertheless, to the best of our knowledge, there does not appear to be a comprehensive set of criteria that assesses the real-life explainability of explanations.

- The strength of ESCOFILT lies in the fact that it uniquely unifies representation and explanation.

- Our proposed summary-level explanation closely resembles real-life explanations, wherein the explanation text is derived from multiple reviews.

- While most recommender models address this via attention mechanisms, our proposed model solves this by unifying representation and explanation in the form of extractive summaries.




# [N -ary Constituent Tree Parsing with Recursive Semi-Markov Model](https://aclanthology.org/2021.acl-long.205/)
- (1) We propose a novel graph-based framework, recursive semi-Markov model, for n-ary constituent tree parsing, which can model the dependencies of sibling nodes.

- 3 The Recursive Semi-Markov Model

- The whole framework is a 1-order semi-Markov model.

- In this paper, a recursive semi-Markov model is proposed for n-ary constituent tree parsing, with the advantage of modeling the sibling relations within n-ary node.




# [Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks](https://aclanthology.org/2021.acl-long.28/)
- We propose a Multi-channel Graph Neural Networks model with Sentiment-awareness (MGNNS) for multimodal sentiment analysis that consists of three stages.

- Our main contributions are summarized as follows: • We propose a novel MGNNS framework that models the global characteristics of the dataset to handle the multimodal sentiment detection task.

- As far as we know, this is the first application of graph neural networks in image-text multimodal sentiment analysis.

- We focus on multimodal sentiment detection for image-text pairs in social media posts.




# [Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech](https://aclanthology.org/2021.acl-long.250/)
- Hybrid models for data collection.

- In this paper we presented a novel HITL methodology for data collection based on an author-reviewer framework.

- This data collection session lasted roughly one month.

- To the best of our knowledge, this is the first multi-target expert-based HS/CN dataset constructed through a semi-automatic mechanism and can be downloaded at the following link: https://github.com/marcoguerini/CONAN.




# [CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction](https://aclanthology.org/2021.acl-long.483/)
- • We propose a novel contrastive instance learning method to boost the DSRE model performances under the MIL framework.

- Thus, we propose a contrastive instance learning method CIL to boost the MIL model performances.

- In this paper, we go beyond typical MIL framework and propose a novel Contrastive Instance Learning (CIL) framework.

- Furthermore, the ablation study shows the rationality of our proposed positive/negative pair construction strategy.




# [Towards Quantifiable Dialogue Coherence Evaluation](https://aclanthology.org/2021.acl-long.211/)
- To address the above limitations, we propose a novel dialogue coherence metric training framework, named as Quantifiable Dialogue Coherence Evaluation (QuantiDCE).

- To the best of our knowledge, it is the first attempt to consider the quantifiable problem for dialogue coherence evaluation.

- To summarize our contributions: 1) We propose QuantiDCE, a novel quantifiable training framework for dialogue coherence evaluation, which aims to align the automatic scores with the actual human rating standards via MLR pre-training and KD fine-tuning.

- To overcome this issue, a novel knowledge distillation (KD) regularization is introduced for retaining the knowledge learned at the MLR pre-training stage.




# [UXLA: A Robust Unsupervised Data Augmentation Framework for Zero-Resource Cross-Lingual NLP](https://aclanthology.org/2021.acl-long.154/)
- We propose a novel data augmentation framework, UXLA, for zero-resource cross-lingual task adaptation.

- It performs simultaneous self-training with data augmentation and unsupervised sample selection.

- Motivated by this, we present UXLA, our unsupervised data augmentation framework for zero-resource cross-lingual task adaptation.

- We consider three tasks in the zero-resource crosslingual transfer setting.




# [Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition](https://aclanthology.org/2021.acl-long.487/)
- To handle the issues, we propose the Mining Undefined Classes from Other-class (MUCO) model to leverage the rich semantics to improve few-shot NER.

- In this paper, we propose Mining Undefined Classes from Other-class (MUCO) to utilize the rich semantics in O class to improve few-shot NER.

- • We propose a novel zero-shot classification method for undefined class detection.

- Our contributions can be summarized as follows: • We propose a novel approach MUCO to leverage rich semantics in O class to improve fewshot NER.




# [RepSum: Unsupervised Dialogue Summarization based on Replacement Strategy](https://aclanthology.org/2021.acl-long.471/)
- Our main contributions are as follows: • We propose RepSum, an unsupervised (or self-supervised) strategy for dialogue summarization, which roots from the hypothesis that a superior summary approximates a replacement of the original dialogue for completing other tasks.

- This work investigates the problem of unsupervised dialogue summarization.

- Based on the RepSum strategy, we propose the corresponding model and employ it to both extractive and abstractive summarization.

- RepSum is employed on both extractive and abstractive-based models via a self-supervision from two auxiliary tasks.




# [Robustifying Multi-hop Question Answering through Pseudo-Evidentiality Training](https://aclanthology.org/2021.acl-long.476/)
- This section includes the results of our model for multi-hop reasoning.

- Since we do not use human-annotations, we aim to generate "pseudo-evidentiality" annotation.

- Our focus is to identify and alleviate reasoning shortcuts in multi-hop QA, without evidence annotations.

- In this paper, we propose a new approach to train multi-hop QA models, not to take reasoning shortcuts of guessing right answers without sufficient evidences.




# [A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections](https://aclanthology.org/2021.acl-long.301/)
- We propose an architecture for joint document and snippet ranking, i.e., stages (ii) and (iii), which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents.

- We modified Natural Questions to fit the BIOASQ setting.

- ( 5) We provide a modified version of the Natural Questions dataset, suitable for document and snippet retrieval.

- We make our code and the modified Natural Questions publicly available.




# [Robustness Testing of Language Understanding in Task-Oriented Dialog](https://aclanthology.org/2021.acl-long.192/)
- In this paper, we present a systematic robustness evaluation of language understanding (LU) in taskoriented dialog from three aspects: language variety, speech characteristics, and noise perturbation.

- Robustness in LU has always been a challenge in task-oriented dialog.

- This paper aims to provide an automatic way to test the LU robustness in task-oriented dialog.

- Ori. of-the-art model which was further pre-trained on task-oriented dialog data, has comparable performance with BERT.




# [End-to-End Training of Neural Retrievers for Open-Domain Question Answering](https://aclanthology.org/2021.acl-long.519/)
- We then present two approaches for end-to-end training of the reader and retriever components in OpenQA.

- • Our end-to-end training approach obtains new state-of-the-art performance on retrieval accuracy.

- The task of open-domain question answering (OpenQA) consists of finding answers to the information-seeking questions using a large knowledge source such as Wikipedia.

- • We scale up end-to-end training to large models and show consistent gains in performance.




# [Maria: A Visual Experience Powered Conversational Agent](https://aclanthology.org/2021.acl-long.435/)
- ( 4) ReCoSa: A hierarchical transformer-based model ) that achieves the state-of-the-art performance on benchmarks of dialog generation.

- Specifically, we present Maria, a neural conversational agent powered by visual world experiences which are retrieved from a pre-built image index, e.g., the Open Images Dataset (Kuznetsova et al., 2018).

- Overall, the contributions of this paper are summarized as follows: • We explore the task of image-grounded dialog generation under a fully open-ended setting where no specific image-dialog pairs are assumed available, i.e., zero-resource imagegrounded conversation.

- The experimental results on Reddit Conversation Corpus (Dziri et al., 2019a) demonstrate that Maria significantly outperforms previous state-of-the-art methods, and can generate informative responses with visual commonsense of our physical world.




# [Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data](https://aclanthology.org/2021.acl-long.66/)
- We presented NMT-Adapt, a novel approach for neural machine translation of low-resource languages which assumes zero parallel data or bilingual lexicon in the low-resource language.

- We introduce NMT-Adapt, a zero resource technique that does not need parallel data of any kind on the low resource language.

- We first evaluate performance of translating into the low-resource language.

- We present results of applying NMT-Adapt to lowresource language translation.




# [Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains](https://aclanthology.org/2021.acl-long.236/)
- The meta-teacher is jointly trained with multi-domain datasets to acquire the instance-level and feature-level metaknowledge.

- Learning Instance-level Transferable Knowledge.

- • We propose the Meta-KD framework to address the task.

- In this paper, we propose the Meta-Knowledge Distillation (Meta-KD) framework, which facilities cross-domain KD.




# [Detecting Propaganda Techniques in Memes](https://aclanthology.org/2021.acl-long.516/)
- • We develop a multi-modal annotation schema, and we create and release a new dataset for the task, consisting of 950 memes, which we manually annotate with 22 propaganda techniques.

- We have proposed a new multi-class multi-label multimodal task: detecting the type of propaganda techniques used in memes.

- We further created and released a corpus of 950 memes annotated with 22 propaganda techniques, which can appear in the text, in the image, or in both.

- Our contributions can be summarized as follows: • We formulate a new multimodal task: propaganda detection in memes, and we discuss how it relates and differs from previous work.




# [PROTAUGMENT: Unsupervised diverse short-texts paraphrasing for intent detection meta-learning](https://aclanthology.org/2021.acl-long.191/)
- Paired with paraphrasing user utterances and its consistency loss incorporated in Prototypical net-works, our model is the best method for intent detection meta-learning on 4 public datasets, with neither extra labeling efforts nor domain-specific conditional language model fine-tuning.

- In this work, we proposed PROTAUGMENT, an architecture for meta-learning for the problem of classifying user-generated short-texts (intents).

- The measured diversity strongly correlates with the average accuracy of the intent detection task (Table 4).

- As a consequence, recent research (Snell et al., 2017;Ren et al., 2018) considers few-shot intent detection as a meta-learning problem: the model is trained to classify user utterances from a consecutive set of small tasks named episodes.




# [A Hierarchical VAE for Calibrating Attributes while Generating Text using Normalizing Flow](https://aclanthology.org/2021.acl-long.187/)
- We demonstrate the effectiveness of CTVAE to generate controlled text by fine tuning two different attributes namely sentiment and formality.

- Automatic generation of content with fine regulation of attributes like sentiment and style is extremely beneficial in this context.

- We propose a hierarchical model using Variational Autoencoders (Kingma and Welling, 2013) to achieve fine grained control over attribute space while maintaining the quality of the generated sentences.

- State-of-the-art methods for style transfer are categorized as supervised and unsupervised techniques.




# [Knowledge-Enriched Event Causality Identification via Latent Structure Induction Networks](https://aclanthology.org/2021.acl-long.376/)
- • To exploit the descriptive knowledge, we devise a descriptive graph induction module.

- Specifically, we devise a Descriptive Graph Induction module to make use of the descriptive knowledge.

- Our contributions are summarized as follows: • We propose a novel Latent Structure Induction Network (LSIN) to leverage the external structural knowledge.

- Meanwhile, we propose a Relational Graph Induction module to leverage the relational knowledge.




# [CLEVE: Contrastive Pre-training for Event Extraction](https://aclanthology.org/2021.acl-long.491/)
- In this paper, we propose CLEVE, a contrastive pre-training framework for event extraction to utilize the rich event knowledge lying in large unsupervised data.

- CLEVE consists of two components, including a text encoder to learn event semantics and a graph encoder to learn event structure information.

- Considering the fact that the AMR structures of large-scale unsupervised data can be easily obtained with automatic parsers (Wang et al., 2015), we propose CLEVE, an event-oriented contrastive pre-training framework utilizing AMR structures to build self-supervision signals.

- We design trigger-argument pair discrimination as our contrastive pre-training task for event semantic pre-training.




# [Hierarchy-aware Label Semantics Matching Network for Hierarchical Text Classification](https://aclanthology.org/2021.acl-long.337/)
- In this paper, we formulate the interaction between text and label as a semantic matching problem and propose a Hierarchy-aware Label Semantics Matching Network (HiMatch).

- 2. We propose a hierarchy-aware label semantics matching network (HiMatch), in which we introduce a joint embedding loss and a matching learn-ing loss to learn the text-label semantics matching relationship in a hierarchy-aware manner.

- In this section, we will describe the details about our Hierarchy-aware Label Semantics Matching Network.

- Φ t , Φ l ∈ R dϕ represent text semantics and label semantics in joint embedding space, respectively.




# [DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling](https://aclanthology.org/2021.acl-long.6/)
- To our knowledge, DeepRapper is the first system that models rhythms for rap generation.

- Both objective and subjective evaluations verify the advantages of DeepRapper in generating rap lyrics with rhymes and rhythms.

- Our main contributions can be summarized as follows: • To model rhythms in rap generation, we develop a data mining pipeline to create rap datasets with aligned rhythmic beats.

- In this paper, we develop DeepRapper, a Transformer (Vaswani et al., 2017) based rap generation system which can model both rhymes and rhythms.




# [OTTers: One-turn Topic Transitions for Open-Domain Dialogue](https://aclanthology.org/2021.acl-long.194/)
- Our contributions are as follows: • We propose a new Natural Language Generation task based on one-turn topic transitions for open-domain dialogue based on a "bridging" strategy, which promotes grounding on KG entities.

- We have defined a new NLG task exploring one-turn topic transitions for mixedinitiative in open-domain systems.

- Baseline models based on state-of-the-art approaches to text generation illustrate possible approaches to the task and show that there is room for improvement.

- 3 One-turn Topic Transitions 3.1 Task Design and Data Collection Task Description.




# [Recursive Tree-Structured Self-Attention for Answer Sentence Selection](https://aclanthology.org/2021.acl-long.358/)
- We introduce the Tree Aggregation Transformer: a novel recursive and treestructured self-attention model for Answer Sentence Selection.

- We introduce the Tree Aggregation Transformer: a novel, recursive and tree-structured self-attention model for AS2.

- Our results in Table 4 establish a new state of the art in TrecQA and WikiQA, two widely used benchmark datasets in answer sentence selection.

- To the best of our knowledge, our method is the first to introduce tree self-attention to Answer Sentence Selection.




# [UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning](https://aclanthology.org/2021.acl-long.202/)
- We further make several ablation studies to validate that textual knowledge and visual knowledge can enhance each other in the unified semantic space.

- In this work, we propose UNIMO, a unified-modal pre-training architecture to leverage the large scale of non-paired text corpus and image collections for cross-modal learning.

- • The visual knowledge and textual knowledge can enhance each other to achieve better performance on several single-modal and multimodal tasks than previous methods.

- We further make several ablation studies to show that the unified-modal architecture can help textual knowledge and visual knowledge mutually enhance each other in the unified semantic space.




# [Rethinking Stealthiness of Backdoor Attack against NLP Models](https://aclanthology.org/2021.acl-long.431/)
- Moreover, in response to the shortcomings of existing backdoor attacking methods, we propose a novel word-based backdoor attacking method which considers both the stealthiness to system deployers and users, making an important step towards achieving stealthy backdoor attacks.

- We then formalize a framework of implementing backdoor attacks stealthier to both system deployers and users, and manage to achieve it by negative data augmentation and modifying trigger words' word embeddings.

- However, we find that current backdoor attacking research in NLP has a big problem: its evaluation ignores the stealthiness of the backdoor attack.

- Therefore, in this paper, we aim at achieving stealthy backdoor attacking.




# [Capturing Event Argument Interaction via A Bi-Directional Entity-Level Recurrent Decoder](https://aclanthology.org/2021.acl-long.18/)
- Different from traditional token-level Seq2Seq models, we use a bi-directional entity-level recurrent decoder (BERD) with a classifier to generate a sequence of argument roles entity by entity.

- We adopt BERT (Devlin et al., 2019) as encoder and the proposed bi-directional entity-level recurrent decoder as decoder for the experiment.

- To fully utilize both left-and rightside argument role information, inspired by the bidirectional decoder for machine translation (Zhang et al., 2018), we propose a neural architecture with a novel Bi-directional Entity-level Recurrent Decoder (BERD) to generate event argument roles entity by entity.

- from previous works is that we formalize EAE as a Seq2Seq-like learning problem instead of a classic classification or sequence labeling problem.




# [End-to-End Lexically Constrained Machine Translation for Morphologically Rich Languages](https://aclanthology.org/2021.acl-long.311/)
- We described the problem of word inflection in lexically constrained machine translation.

- We evaluate different methods of lexically constrained machine translation on the Czech language.

- We propose an approach to deal with word inflection in lexically constrained translation.

- To illustrate the problem, Figure 1 shows a sentence translation from English to Czech with outputs from three methods.




# [Document-level Event Extraction via Parallel Prediction Networks](https://aclanthology.org/2021.acl-long.492/)
- • We introduce a novel matching loss function to train the end-to-end model, which can bootstrap a global optimization.

- DE-PPN is based on an encoder-decoder framework that can extract structured events from a whole document in a parallel manner.

- In summary, our contributions are as follows: • We propose an encoder-decoder model, DE-PPN, that is based on a document-level encoder and a multi-granularity decoder to extract events in parallel with document-aware representations.

- In this paper, we propose an encoder-decoder model, DE-PPN, to extract events in parallel from a document.




# [Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation](https://aclanthology.org/2021.acl-long.10/)
- Here we explore the performance of monolingual and multilingual models on concept-to-text datasets.

- Results show that multilingual models outperform monolingual models, and that LAD outperforms previous work in improving the performance of multilingual models, especially in low resource conditions.

- In the concept-to-text NLG task, the language is the leader and the people are known as indian people.

- We propose Language Agnostic Delexicalisation (LAD), a novel delexicalisation method that aims to identify and delexicalise values in the text independently of the language.




# [Meta-Learning to Compositionally Generalize](https://aclanthology.org/2021.acl-long.258/)
- We summarise our contributions as follows: • We approach the problem of compositional generalization with a meta-learning objective that tries to explicitly reduce input memorization using similarity-driven virtual tasks.

- Central to this approach is the generation of tasks for meta-learning by sub-sampling training data.

- We introduce the meta-learning augmented approach to supervised learning from ; Wang et al. (2020a) that explicitly optimizes for outof-distribution generalization.

- For compositional generalization, Lake (2019) proposes a meta-learning procedure to train a memory-augmented neural model.




# [VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation](https://aclanthology.org/2021.acl-long.308/)
- Based on the above observation, we propose to plug a cross-attention module (query!=key/value) into the Transformer encoder and design a crossattention MLM task to explicitly capture the interdependence between languages.

- tial on a variety of cross-lingual understanding and generation tasks.

- This phenomenon reflects that our model can better build the interdependence between languages.

- We present VECO, a variable and flexible crosslingual pre-training model, targets at explicitly capturing the interdependence between languages via a plug-and-play cross-attention module.




# [MulDA: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual NER](https://aclanthology.org/2021.acl-long.453/)
- Our main contributions can be summarized as follows: • We propose a simple but effective labeled sequence translation method to translate the source training data to a desired language.

- These recent methods have demonstrated promising zero-shot cross-lingual NER performance.

- We have proposed a multilingual data augmentation framework for low resource cross-lingual NER.

- We first introduce a novel labeled sequence translation method to translate the training data to the target language as well as to other languages.




# [Employing Argumentation Knowledge Graphs for Neural Argument Generation](https://aclanthology.org/2021.acl-long.366/)
- This paper tackles argument generation through the use of argumentation knowledge graphs.

- In this section, we outline related studies on argument generation, argumentation knowledge graphs, and graph-to-text generation.

- Nevertheless, the experiment results show that our approach for controlling the generated arguments using argumentation knowledge graphs improves the quality.

- This emphasizes the impact of encoding the knowledge of the graphs into argumentative data for argument generation.




# [G-Transformer for Document-level Machine Translation](https://aclanthology.org/2021.acl-long.267/)
- Experiments show that G-Transformer converges faster and more stably than Transformer on different settings, obtaining the state-of-the-art results under both non-pretraining and pre-training settings.

- Experiments show that the resulting G-Transformer converges fast and stably on small and large data, giving the state-of-the-art results compared to existing models under both pre-training and random initialization settings.

- The above experiments show that training failure on Transformer can be caused by local minima.

- G-Transformer outperforms previous documentlevel MT models on News and Europarl with a significant margin.




# [Explaining Contextualization in Language Models using Visual Analytics](https://aclanthology.org/2021.acl-long.39/)
- Additionally, we show that contextualization is neither entirely driven by polysemy nor context variation.

- Thus, we decided to investigate function and content words in more detail, using the LMExplorer to explore contextualization in BERT with respect to the functionality continuum.

- This paper presented new insights on the contextualization of the functionality continuum, showing that BERT fails to capture the nature of semifunctional-semi-content words.

- This paper contributes to this line of work by examining the degree of contextualization of function vs. content words.




# [Bird's Eye: Probing for Linguistic Graph Structures with a Simple Information-Theoretic Approach](https://aclanthology.org/2021.acl-long.145/)
- Bird's Eye allows us to probe for entire linguisitic structures.

- In this work, we introduce a new probing approach, Bird's Eye, which can be used to detect if contextualized text representations encode entire linguistic graphs.

- We call this probe, Worm's Eye.

- We describe various stages of Bird's Eye below:




# [OntoED: Low-resource Event Detection with Ontology Embedding](https://aclanthology.org/2021.acl-long.220/)
- Our contributions can be summarized as follows: • We study the low-resource event detection problem and propose a novel ontology-based model, OntoED, that encodes intra and inter structures of events.

- This illustrates the effectiveness of OntoED handling new unseen event types without introducing outsourcing data.

- Given the event ontology with correlations among event types, we infer new event correlations based on existing ones.

- • We provide a novel ED framework based on ontology embedding with event correlations, which interoperates symbolic rules with popular deep neural networks.




# [Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation](https://aclanthology.org/2021.acl-long.22/)
- We conclude from this experiment that MBR is more robust to copy noise in the training data.

- Susceptibility to copy noise: Copied content in the training data disproportionately affects translation quality.

- We empirically study the properties of MBR decoding with common MT metrics as utility functions, and find it still exhibits a length bias and token frequency bias similar to beam search.

- To summarize, we find that MBR decoding has a higher domain robustness than beam search.




# [Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction](https://aclanthology.org/2021.acl-long.261/)
- We first define the Emotion Cause Extraction (ECE) task here.

- Moreover, we propose a graph-based model to enhance the semantic dependencies between a candidate clause and a given emotion clause by extracting relevant knowledge paths from ConceptNet.

- The experimental results show that our proposed method achieves comparative performance to the state-of-the-art methods, and is more robust against adversarial attacks.

- We propose a Knowledge-Aware Graph (KAG) model as shown in Figure 2, which incorporates knowledge paths extracted from ConceptNet for emotion cause extraction.




# [Adapting Unsupervised Syntactic Parsing Methodology for Discourse Dependency Parsing](https://aclanthology.org/2021.acl-long.449/)
- We adapt two current state-of-the-art models in unsupervised syntactic dependency parsing for discourse parsing.

- We apply the adaptations to two unsupervised syntactic dependency parsing methods.

- In this paper, we propose a method to adapt unsupervised syntactic parsing methods for discourse dependency parsing.

- To the best of our knowledge, we are the first to investigate unsupervised and semi-supervised discourse dependency parsing.




# [Comprehensive Study: How the Context Information of Different Granularity Affects Dialogue State Tracking?](https://aclanthology.org/2021.acl-long.193/)
- for dialogue state tracking

- • How to combine multiple granularities for dialogue state tracking?

- • Application of context information granularity in few-shot learning scenario.

- determine the effectiveness of small granularity in dialogue state tracking.




# [BERTGEN: Multi-task Generation through BERT](https://aclanthology.org/2021.acl-long.503/)
- In this paper, we presented BERTGEN, a novel generative, decoder-only model which extends BERT by combining multimodal and multilingual pretrained models.

- Our ablation studies show that BERTGEN is able to efficiently transfer relevant inductive biases from the pre-trained models and benefits from multi-task learning without suffering from catastrophic forgetting.

- The multi-task (and zero-shot) generation ability of BERTGEN is mostly inspired by Ha et al. (2016) and Johnson et al. (2017).

- Zero-shot performance.




# [Self-Guided Contrastive Learning for BERT Sentence Representations](https://aclanthology.org/2021.acl-long.197/)
- In this paper, we have proposed a contrastive learning method with self-guidance for improving BERT sentence embeddings.

- Contrastive Representation Learning.

- To conclude, we demonstrate that our self-guided contrastive learning is effective in improving the quality of BERT sentence embeddings when tested on STS tasks.

- We aim at developing a contrastive learning method that is free from external procedure such as data augmentation.




# [Guiding Teacher Forcing with Seer Forcing for Neural Machine Translation](https://aclanthology.org/2021.acl-long.223/)
- This means knowledge distillation does transfer future information from the seer decoder to the conventional decoder.

- The behaviors of the conventional decoder are guided by the seer decoder via knowledge distillation.

- To this end, we employ the method of knowledge distillation to transfer future information from the seer decoder to the conventional decoder.

- In this way, at test the conventional decoder can perform like the seer decoder as if it knew the future translation.




# [Societal Biases in Language Generation: Progress and Challenges](https://aclanthology.org/2021.acl-long.330/)
- Motivated by the importance of fairness in language generation, we present the first comprehensive survey on societal biases in language generation.

- Our contributions are a comprehensive survey on societal biases in language generation and an experimental study on biases from decoding techniques.

- In this work, we present a survey and commentary on the progress and challenges for studying societal biases in language generation.

- As a fairly nascent area of exploration, the study of biases in language generation still poses many challenges.




# [FEW-NERD: A Few-shot Named Entity Recognition Dataset](https://aclanthology.org/2021.acl-long.248/)
- We propose FEW-NERD, a large-scale few-shot NER dataset with fine-grained entity types.

- This is the first few-shot NER dataset and also one of the largest human-annotated NER dataset.

- In this paper, we present a human-annotated dataset, FEW-NERD, for few-shot learning in NER.

- To the best of our knowledge, FEW-NERD is the first dataset specially constructed for few-shot NER and also one of the largest human-annotated NER dataset (statistics in Section 5.1).




# [Matching Distributions between Model and Data: Cross-domain Knowledge Distillation for Unsupervised Domain Adaptation](https://aclanthology.org/2021.acl-long.421/)
- • For the first time, the gradient information of the source domain is exploited to boost the UDA performance.

- In this paper, we propose a generic framework named Cross-domain Knowledge Distillation (CdKD).

- CdKD learned the collective knowledge across different domains including domain-invariant and discriminative features by matching the joint distributions between a trained source model and a set of target data.

- The main contributions are outlined as, • We propose to investigate the problem of UDA without needing source data by exploring the distribution discrepancy between a source model and a set of target data.




# [HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation](https://aclanthology.org/2021.acl-long.423/)
- Besides, we propose a hierarchical user interest matching framework to match candidate news with different levels of interest representations to target user interests more accurately.

- Thus, we propose a hierarchical user interest modeling framework, which learns a hierarchical interest tree to capture diverse and multi-grained user interest.

- Thus, we propose a hierarchical user interest matching framework, which models user interests in candidate news from different interest granularities.

- Besides, we also propose a hierarchical user interest matching framework to match user interest with candidate news from different granularities, which can better target user interests.




# [TIMEDIAL: Temporal Commonsense Reasoning in Dialog](https://aclanthology.org/2021.acl-long.549/)
- Our experiments demonstrate that offthe-shelf, pre-trained language models cannot effectively reason about temporal aspects in a dialog, even with domain-specific finetuning.

- are designed to confuse models that rely on shallow pattern matching.

- Our findings indicate that large-scale pre-trained models even after fine-tuning may not be sufficient for robust temporal reasoning in dialogs, and motivate future research toward modeling temporal concepts over diverse everyday events, and contextual reasoning about them.

- Due to our design of challenging negatives, the full context can often confuse models that rely on shallow cues.




# [Changing the World by Changing the Data](https://aclanthology.org/2021.acl-long.170/)
- This position paper brings together the arguments for and against curating data 2 from linguistic and ethical perspectives ( §2).

- DL models learn spurious patterns present in the data.

- 2.2 Why Not to Change the Data? Since this is a position paper arguing that data curation is unavoidable, the arguments against it are presented together with the defense.

- Deciding what should not be remembered is clearly a data curation issue.




# [Multi-Task Retrieval for Knowledge-Intensive Tasks](https://aclanthology.org/2021.acl-long.89/)
- In order to evaluate the suitability of a multi-task trained retriever as a starting checkpoint for few-shot training, we take the various leave-one-out models and finetune them on our few-shot training sets.

- Large-scale pre-trained models have been shown to store knowledge directly into their parameters.

- Nevertheless, the multi-task model proves itself more robust, and achieves the top performance on it.

- In this recent work, the authors multi-task train a pre-trained model on around 50 datasets, before performing the final fine-tuning.




# [Concept-Based Label Embedding via Dynamic Routing for Hierarchical Text Classification](https://aclanthology.org/2021.acl-long.388/)
- We propose a novel concept-based label embedding model.

- • We propose a hierarchical network to extract the concepts and model the sharing process via a modified dynamic routing algorithm.

- To further exploit the information of concept for HTC, we propose a novel concept-based label embedding method which can explicitly represent the concepts and model the sharing mechanism among classes.

- Then a concept sharing module is designed for extracting concepts and modeling the sharing mechanism among classes.




# [Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models](https://aclanthology.org/2021.acl-long.523/)
- We propose Polyjuice, a general-purpose generator that produces fluent and diverse counterfactuals, allowing for control over the kinds and locations of perturbations.

- 2 General-Purpose Counterfactuals

- In another application, Polyjuice produces counterfactual explanations ( §4), providing significant insight on top of state-of-the-art explanation techniques.

-  A GPT-2 as Counterfactual Generator




# [Exploring Dynamic Selection of Branch Expansion Orders for Code Generation](https://aclanthology.org/2021.acl-long.394/)
- In this section, we extend the conventional Seq2Tree model with a context-based branch selector, which dynamically determines optimal expansion orders of branches for multi-branch AST nodes.

- Then we propose an extended Seq2Tree model equipped with a context-based branch selector, which is capable of dynamically determining optimal branch expansion orders for multi-branch nodes.

- Specifically, we propose to equip the conventional Seq2Tree model with a context-based Branch Selector, which dynamically quantifies the priorities of expanding different branches for multi-branch nodes during AST generations.

- In this way, the model is trained to determine optimal expansion orders of branches for multi-branch nodes, which will contribute to AST generations.




# [Dynamic Contextualized Word Embeddings](https://aclanthology.org/2021.acl-long.542/)
- We introduce dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context.

- Do dynamic contextualized word embeddings indeed capture interpretable dynamics in word meaning?

- We have introduced dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context.

- In this paper, we introduce dynamic contextualized word embeddings that combine the strengths of contextualized word embeddings with the flexibility of dynamic word embeddings.




# [Superbizarre Is Not Superb: Derivational Morphology Improves BERT's Interpretation of Complex Words](https://aclanthology.org/2021.acl-long.279/)
- Conceptually, PLMs can thus be interpreted as serial dual-route models.

- We present the first study examining how the input segmentation of PLMs, specifically BERT, affects their interpretations of derivationally complex English words.

- We have examined how the input segmentation of PLMs, specifically BERT, affects their interpretations of derivationally complex words.

- We show that PLMs can be interpreted as serial dualroute models, which implies that maximally meaningful input tokens should allow for the best generalization on new words.




# [ONE2SET: Generating Diverse Keyphrases as a Set](https://aclanthology.org/2021.acl-long.354/)
- To successfully train under ONE2SET paradigm, we propose a K-step target assignment mechanism and a separate set loss, which greatly increases the number and diversity of the generated keyphrases.

- Hence, we introduce a K-step target assignment mechanism, where we first auto-regressively generate K words for each code and then assign targets via bipartite matching based on the predicted words.

- An attempt is first made to remove the K-step target assignment mechanism, which means that we employ a fixed sequential matching strategy as in the ONE2SEQ paradigm.

- Hence, we introduce a K-step target assignment mechanism to assign the ground-truth keyphrase for each prediction, and a separate set loss to train the model in an end-to-end way.




# [Controllable Open-ended Question Generation with A New Question Type Ontology](https://aclanthology.org/2021.acl-long.502/)
- Distri4 Type-aware Open-ended Question Generation In this section, we present our type-aware question generation framework.

- Automatic metrics show that our type-aware question generation model outperforms competitive comparisons, highlighting the effectiveness of semantic graph-augmented representation and joint modeling of focus prediction and question generation.

- We present a new question type ontology which better captures the nuances of questions to support the study of open-ended question generation.

- Second, our question type ontology provides a new perspective for question diversity evaluation.




# [Comparing Test Sets with Item Response Theory](https://aclanthology.org/2021.acl-long.92/)
- We also find span selection as the most effective task format for discriminating between strong and weak models.

- • Span-based QA is an effective task format for discriminating between strong and weak models.

- Span Selection We observe that span selection datasets are the most discriminative.

- The dataset is formulated as a span selection QA task.




# [Multi-Label Few-Shot Learning for Aspect Category Detection](https://aclanthology.org/2021.acl-long.495/)
- • To alleviate the noise from the support set and query set, we design two effective attention mechanisms, i.e., support-set attention and query-set attention.

- In this paper, we formulate the aspect category detection (ACD) task in the few-shot learning (FSL) scenario.

- Specifically, we design two effective attention mechanisms for the support set and query set to alleviate the noise from both sets.

- Therefore, we propose a multi-label FSL method based on the prototypical network.




# [Robust Knowledge Graph Completion with Stacked Convolutions and a Student Re-Ranking Network](https://aclanthology.org/2021.acl-long.82/)
- • We develop a re-ranking procedure that distills knowledge from our ranking model into a student network that re-ranks promising candidate entities.

- We develop an expressive deep convolutional model that utilizes textual entity representations more effectively and improves sparse KGC.

- • We develop a deep convolutional architecture that utilizes textual embeddings more effectively than existing neural KGC models and significantly improves performance for sparse KGC.

- We develop an entity re-ranking procedure and demonstrate the effectiveness of the re-ranking paradigm for KGC.




# [Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open Domain Question Answering](https://aclanthology.org/2021.acl-long.315/)
- In this section, we describe our method for hybrid open-domain question answering.

- Our highlighted contributions are as follows: • We propose a multi-modal framework that incorporates hybrid knowledge sources with the Text2SQL ability for ODQA tasks.

- • We propose a simple but effective generative approach that takes both textual and tabular evidence and generates either direct answers or SQL queries, automatically determined by the context.

- Open-domain question answering (ODQA) is a task to answer factoid questions without a prespecified domain.




# [Transfer Learning for Sequence Generation: from Single-source to Multi-source](https://aclanthology.org/2021.acl-long.446/)
- The proposed framework aims to reduce the pretrain-finetune discrepancy and learn better multi-source representations.

- Therefore, we propose a two-stage finetuning method named gradual finetuning.

- In this paper, we propose a two-stage finetuning method named gradual finetuning.

- To alleviate the pretrain-finetune discrepancy, we adopt the gradual finetuning method to better transfer from single-source to multi-source.




# [Learning from Perturbations: Diverse and Informative Dialogue Generation with Inverse Adversarial Training](https://aclanthology.org/2021.acl-long.57/)
- (2) Do inverse adversarial training help neural dialogue models generate more diverse, engaging, and informative dialogue responses?

- To address the above issues, in this paper, we propose Inverse Adversarial Training (IAT) algorithm for training neural dialogue systems to avoid generic responses and model dialogue history better, thus generating diverse and informative responses.

- Experimental results show IAT helps neural dialogue systems model dialogue history better and generate more diverse and informative responses.

- In this work, we introduce inverse adversarial training (IAT) algorithm that is able to simultaneously reduce the dull response problem and help neural dialogue systems model dialogue history better.




# [Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation](https://aclanthology.org/2021.acl-long.562/)
- This paper aims to understand and mitigate the impact of fine-grained semantic divergences in NMT.

- Our experiments on EN↔FR tasks show that fine-grained semantic divergences hurt translation quality when they overwhelm the training data.

- 3 Analyzing the Impact of Divergences

- Following -who, in contrast, study the impact of noise on MT-we control for different types of fine-grained semantic divergences and different ratios of equivalent vs. divergent data.




# [Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation](https://aclanthology.org/2021.acl-long.468/)
- Consequently, we propose a novel embedding transfer strategy with a plug-and-play embedding generator.

- Figure 1: Illustration of our pretrain-finetune pipeline.

- Specifically, we revise the conventional pretrain-finetune pipeline as follows: Pretrain.

- As shown in Figure 1, we extend the prior pretrain-finetune paradigm with an embedding transfer stage.




# [More Identifiable yet Equally Performant Transformers for Text Classification](https://aclanthology.org/2021.acl-long.94/)
- This may lead to more identifiable attention weights.

- Thus, our contribution are as follows: • We provide a concrete theoretical analysis of identifiability of attention weights which was missing in the previous work by Brunner et al. (2019).

- It is important that the identifiability of attention weights should not come at the cost of reduced performance of the model.

- In this work, we probe the identifiability of attention weights in Transformer from a perspective that was ignored in Brunner et al. (2019).




# [PHMOSpell: Phonological and Morphological Knowledge Guided Chinese Spelling Check](https://aclanthology.org/2021.acl-long.464/)
- 2) We design a novel adaptive gating mechanism, which effectively incorporates the multi-modal information into a pre-trained language model in an end-to-end trainable way.

- PHMOSpell incorporates pinyin and glyph features into a pre-trained language model via an adaptive gating module for CSC.

- In this research, we propose a novel end-to-end trainable model called PHMOSpell for CSC, which incorporates both phonological and morphological knowledge from two feature extractors into a pretrained language model by an effective adaptive gating mechanism.

- The contributions of this paper are in three folds: 1) We derive both phonological and morphological knowledge of Chinese characters from multimodality and apply them to CSC.




# [Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval](https://aclanthology.org/2021.acl-long.392/)
- Our main contributions can be summarized as follows: • We propose a novel approach to represent the document with multiple pseudo query embeddings which are generated by a clustering process.

- In this paper, we propose a method to improve the performance of the first-stage retrieval model which is based on Bi-encoder and semi-interactive aggregator.

- Specifically, our method mimics the real queries by an iterative K-means clustering algorithm.

- Experimental results show that our approach achieves state-of-theart retrieval performance while still remaining efficient computation.




# [Data Augmentation for Text Generation Without Any Augmented Data](https://aclanthology.org/2021.acl-long.173/)
- In this section, we aim to formulate the problem of data augmentation for general text generation models without any use of augmented data mapping functions.

- To answer this question, we aim to formulate the problem of data augmentation for general text generation models without any use of augmented data mapping functions.

- The proposed approach provides a new paradigm and understanding of data augmentation for text generation.

- We have proposed an objective of formulating data augmentation without any use of any augmented data mapping function.




# [DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations](https://aclanthology.org/2021.acl-long.547/)
- • We design multi-turn reasoning modules to extract and integrate emotional clues by iteratively performing the intuitive retrieving process and conscious reasoning process, which imitates human unique cognitive thinking.

- Sec-ondly, in the cognitive phase, we design multi-turn reasoning modules to iteratively extract and integrate the emotional clues.

- Therefore, in the cognitive phase, we design multi-turn reasoning modules to iteratively extract and integrate the emotional clues.

- The main contributions of this work are summarized as follows: • We propose novel Contextual Reasoning Networks (DialogueCRN) to fully understand the conversational context from a cognitive perspective.




# [A Human-machine Collaborative Framework for Evaluating Malevolence in Dialogues](https://aclanthology.org/2021.acl-long.436/)
- We propose the human-machine collaborative evaluation (HMCEval) framework to solve this task.

- HMCEval achieves around 99% evaluation accuracy (compared to human evaluation) with as much as half of the human effort saved.

- In this paper, we propose the human-machine collaborative evaluation (HMCEval) framework for dialogue evaluation with the aim of balancing reliability and efficiency.

- In this work, we have introduced a human-machine collaborative evaluation framework (HMCEval) for reliable and efficient CDS evaluation.




# [On Finding the K-best Non-projective Dependency Trees](https://aclanthology.org/2021.acl-long.106/)
- All three papers utilize the K-best spanning tree algorithm of Camerini et al. (1980).

- Furthermore, we provided a novel extension to the algorithm that decodes the K-best dependency trees in O(KN 2 ).

- In this paper, we provided a simplification to Camerini et al. (1980)'s O(KN 2 ) K-best spanning trees algorithm.

- 5 Finding the K th Best Dependency Tree In this section, we present a novel extension to the algorithm presented thus far, that allows us to efficiently find the K-best dependency trees.




# [ILDC for CJPE: Indian Legal Documents Corpus for Court Judgment Prediction and Explanation](https://aclanthology.org/2021.acl-long.313/)
- Based on ILDC, we propose a new task: COURT JUDGMENT PREDICTION AND EXPLANATION (CJPE).

- We address the CJPE task via two sub-tasks in the following sequence: Prediction and Explanation.

- We release the ILDC and code for the prediction and explanation models via GitHub 1 .

- We perform detailed case studies on the corpus to understand differences in prediction and explanation annotations by legal experts, indicative of the computational challenges of modeling the data.




# [Towards Emotional Support Dialog Systems](https://aclanthology.org/2021.acl-long.269/)
- We then construct an Emotional Support Conversation dataset, ESConv.

- In this work, we define the task of Emotional Support Conversation and present an ESC Framework.

- In this paper, we define the task of Emotional Support Conversation (ESC), aiming to provide

- Training and Examination To teach crowdworkers how to provide effective emotional support




# [Distributed Representations of Emotion Categories in Emotion Space](https://aclanthology.org/2021.acl-long.184/)
- As far as we know, this is the first work to learn the distributed representations for emotion categories in emotion space rather than semantic space.

- Any model that outputs are soft labels can be employed to learn the distributed representations for emotion categories.

- The main contributions of this work are summarized as follows: • A general framework to learn distributed emotion representations from an emotion classification dataset is first proposed.

- Distributed representations of emotion categories in emotion space can also benefit NLP applications.




# [Online Learning Meets Machine Translation Evaluation: Finding the Best Systems with the Least Human Effort](https://aclanthology.org/2021.acl-long.242/)
- We contribute with an online MT ensemble that allows to reduce human effort by immediately incorporating human feedback in order to dynamically converge to the best systems 1 .

- We proposed an online learning approach to address the issue of finding the best MT systems among an ensemble, while making the most of existing human feedback.

- For fr-de (Table 4), our online approach often converges to the top 3 systems (or a subset of them) throughout the learning process (even at just 10 iterations), and it also converges to the best system when using EWAF with human-comet.

- In Machine Translation (MT), measuring the quality of a large amount of automatic translations can be a challenge.




# [GhostBERT: Generate More Features with Cheap Operations for BERT](https://aclanthology.org/2021.acl-long.509/)
- 1: Development set results of the baseline pre-trained language models and our proposed method on the GLUE benchmark.

- Empirical results on BERT, RoBERTa and ELECTRA demonstrate that adding the proposed ghost modules enhances the representation power and boosts the performance of the original model by supplying more features.

- In this paper, we propose GhostBERT to generate more features in pre-trained model with cheap operations.

- In this section, we show the efficacy of the proposed method with (pruned) BERT (Devlin et al., 2019), RoBERTa and ELEC-TRA (Clark et al., 2020) as backbone models.




# [Handling Extreme Class Imbalance in Technical Logbook Datasets](https://aclanthology.org/2021.acl-long.312/)
- The feedback loop approach for selecting training data is generic and could easily be applied to any learning problem with substantial class imbalances.

- In this work, we used a set of 7 logbook datasets from the aviation, automotive, and facility domains available at MaintNet (Akhbardeh et al., 2020a).

- The methodology presented in this paper could be applied to other maintenance corpora from a variety of technical domains.

- We acquired seven logbook datasets from three technical domains containing short instances with non-standard grammar and spelling, and many abbreviations.




# [When Do You Need Billions of Words of Pretraining Data?](https://aclanthology.org/2021.acl-long.90/)
- We find that ability in syntax and semantics largely saturates after only 10M to 100M words of pretraining data-on par with the data available to human learners-while learning factual knowledge requires much more data.

- The commonsense learning curve (for Winograd coref. only) rises far later, and is projected to continue to rise long after syntactic and semantic features stop improving.

- We find the greatest improvement in overall BLiMP performance between 1M and 100M words of pretraining data.

- Fourth, we probe the models' world knowledge and commonsense knowledge using unsupervised language model knowledge probing with the LAMA suite (Petroni et al., 2019).




# [Positional Artefacts Propagate Through Masked Language Model Embeddings](https://aclanthology.org/2021.acl-long.413/)
- Lastly, we find that "clipping" does not affect models' performance on three supervised tasks.

- Namely, we show that, across all layers, select neurons in BERT and RoBERTa consistently bear extremely large values.

- Neuron-level analysis In order to test the extent to which BERT and RoBERTa's outliers are related to positional information, we employ a probing technique inspired by Durrani et al. (2020).

- To verify our hypothesis, we clip BERT and RoBERTa's outliers by setting each neuron's value to zero.




# [A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies](https://aclanthology.org/2021.acl-long.131/)
- 3 Why do speakers code-switch?

- is to inform researchers in computational linguistics (CL) and language technologies about the linguistic and social aspects of code-switching (C-S) found in multilingual contexts (e.g. Europe and India) and how linguists describe and model them.

- Sitaram et al. (2019) provide a comprehensive survey of research in computational processing of C-S text and speech and Jose et al. ( 2020) present a list of datasets available for C-S research.

- We identify three main limitations of the current state of computational processing of C-S: data, evaluation and user-facing applications.




# [Conditional Generation of Temporally-ordered Event Sequences](https://aclanthology.org/2021.acl-long.555/)
- We propose a conditional generation model to tackle temporal event ordering and event infilling, and train it as a denoising autoencoder over outof-context temporal event sequences.

- This work presents a BART-based conditional generation model and a denoising autoencoder framework to learn temporal event knowledge, and addresses both temporal ordering and event infilling tasks by pretraining on automatically collected data.

- Our model can be formulated as a denoising autoencoder if x is created as a noised version of y.

- Our experiments demonstrate that our model is able to perform temporal ordering and infilling in a zeroshot manner, not fine-tuned on our target datasets, which suggests that it can also be applied to other settings requiring event schematic and temporal knowledge.




# [PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity](https://aclanthology.org/2021.acl-long.424/)
- Thus, incorporating popular news has the potential to alleviate the cold-start and diversity problems in personalized news recommendation.

- Extensive experiments on two real-world datasets show PP-Rec can effectively improve the performance of news recommendation in terms of both accuracy and diversity.

- Extensive experiments on two real-world datasets constructed by logs of commercial news websites and feeds in Microsoft validate that our method can effectively improve the accuracy and diversity of news recommendation.

- We propose a unified model to predict time-aware news popularity based on news content, recency, and near real-time CTR.




# [Multi-Head Highly Parallelized LSTM Decoder for Neural Machine Translation](https://aclanthology.org/2021.acl-long.23/)
- In practice, however, LSTM is slower than the self-attention network in training.

- To constrain the number of LSTM parameters, we compute several small HPLSTMs in parallel like multi-head self-attention.

- The Transformer can be trained efficiently due to the highly parallelized self-attention network.

- It enables sequence-level parallelization in context modelling, as all token representations can be computed in parallel, and linear transformations are only required to compute the sequence once.




# [Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities](https://aclanthology.org/2021.acl-long.203/)
- In order to learn robust joint multimodal representations, we propose a unified model, Missing Modality Imagination Network (MMIN), which can deal with different uncertain missing-modality conditions in real application scenarios.

- In summary, the main contributions of this work are: 1) We propose a unified model, Missing Modality Imagination Network (MMIN), to improve the robustness of emotion recognition systems under uncertain missing-modality testing con-ditions.

- In this work, we propose a novel unified model, Missing Modality Imagination Network (MMIN), to address the above issues.

- The proposed MMIN model as a unified multimodal emotion recognition model can learn robust joint multimodal representations and outperforms the standard multimodal fusion models on both benchmark datasets under both the uncertain missing-modality and the fullmodality conditions.




# [Rational LAMOL: A Rationale-Based Lifelong Learning Framework](https://aclanthology.org/2021.acl-long.229/)
- Then we introduce the core lifelong learning framework of Rational LAMOL in Section 3.2.

- Moreover, using unsupervised rationale generation instead of human rationales also yielded competitive performance, achieving average improvements of 2.67% from original LAMOL.

- We introduce Rational LAMOL and its detailed implementation in this section.

- Furthermore, our framework can be applied to any NLP datasets by leveraging unsupervised rationale generation, eliminating the need for human rationales while maintaining comparable improvements.




# [A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering](https://aclanthology.org/2021.acl-long.318/)
- To alleviate the spurious solution problem in weakly supervised QA, we propose to explicitly exploit the semantic correlations between a question and its solution via mutual information maximization.

- Our contributions are as follows: (1) We propose a mutual information maximization approach for the spurious solution problem in weakly supervised QA, which exploits the semantic correlations between a question and its solution; (2) We conducted extensive experiments on four QA datasets.

- To exploit the semantic correlations between a question and its solution, we propose to maximize the mutual information between question-answer pairs and model-predicted solutions.

- They do not explicitly exploit the semantic correlations between a question and its solution.




# [TAN-NTM: Topic Attention Networks for Neural Topic Modeling](https://aclanthology.org/2021.acl-long.299/)
- Our contributions can be summarised as: • We propose a document encoding framework for topic modeling which leverages the topicword distribution to perform attention effectively in a topic aware manner.

- • We show that our topic model encoder can be adapted to improve the topic guided supervised keyphrase generation achieving improved performance on this task.

- Our model encodes better latent document-topic features as validated through better performance on document classification and supervised keyphrase generation tasks.

- A key application of topic models is supervised keyphrase generation.




# [Intrinsic Bias Metrics Do Not Correlate with Application Bias](https://aclanthology.org/2021.acl-long.150/)
- These plots show no relationship at all between intrinsic and extrinsic metrics.

- To answer this question, we analyse the relationship between intrinsic and extrinsic bias.

- We need to measure the relationship between intrinsic and extrinsic metrics as bias changes, we must generate many datapoints for each experiment.

- If intrinsic and extrinsic measures do not correlate with simple embeddings, this result is unlikely to be changed by adding more architectural layers and configurable hyperparameters.




# [Generating SOAP Notes from Doctor-Patient Conversations Using Modular Summarization Techniques](https://aclanthology.org/2021.acl-long.384/)
- • A new collection of extractive-abstractive approaches for generating long sectionsegmented summaries of conversations, including new methods that leverage annotations attributing summary sentences to conversation utterances.

- In this paper, we introduce the first end-to-end methods for generating whole SOAP notes based on clinical conversations.

- Our first methodological contribution is to propose a spectrum of methods, for decomposing summarizaton tasks into extractive and abstractive subtasks.

- We proposed a spectrum of extractive-abstractive summarization methods that leverage: (i) section-structured form of the SOAP notes and (ii) linked conversation utterances associated with every SOAP note sentence.




# [Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models](https://aclanthology.org/2021.acl-long.114/)
- CGMH and REFLECTIVE DECODING both return multiple sampled, ranked paraphrases.

- REFLECTIVE DECODING Out-of-the-Box A major advantage to applying REFLECTIVE DECOD-ING

- REFLECTIVE DECODING requires no supervision, only two complementary off-the-shelf LMs-one forward ( −→ LM) and one backward ( ←− LM).

- We present REFLECTIVE DECODING, a novel unsupervised text generation method for tasks that do not fit the text continuation paradigm.




# [HiddenCut: Simple Data Augmentation for Natural Language Understanding with Better Generalization](https://aclanthology.org/2021.acl-long.338/)
- To sum up, our contributions are: • We propose a simple data augmentation method, HiddenCut, to regularize PLMs during fine-tuning by cutting contiguous spans of representations in the hidden space.

- Through extensive experiments on indistribution datasets (GLUE benchmarks) and outof-distribution datasets (challenging counterexamples), HiddenCut consistently and significantly outperformed state-of-the-art baselines, and demonstrated superior generalization performances.

- • We demonstrate the effectiveness of Hidden-Cut through extensive experiments on both indistribution and out-of-distribution datasets.

- Results show that our method consistently outperforms baselines, especially on out-of-distribution and challenging counterexamples.




# [COINS: Dynamically Generating COntextualized Inference Rules for Narrative Story Completion](https://aclanthology.org/2021.acl-long.395/)
- Narrative Story Completion.

- Specifically, we focus on Narrative Story Completion (NSC), a new task setting for story generation.

- We addressed a Narrative Story Completion task that allows us to probe the coherence capabilities of a neural generation model.

- Our main contributions are as follows: 1) We propose a new setting for a Narrative Story Completion task, which asks a system to complete a narrative story given its beginning and ending, with the aim of examining the reasoning capacities of a model that solves the task.




# [Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data](https://aclanthology.org/2021.acl-long.322/)
- In this paper, we investigated the learning of language and multimodal representations of typed text collected from mobile data.

- As a step towards privacy-preserving learning, we also propose approaches that obfuscate user identity while remaining predictive of daily mood.

- learning to predict daily mood labels y.

- We then study our proposed method of learning privacy-preserving features to determine whether it can obfuscate user identity while remaining predictive of daily mood.




# [Optimizing Deeper Transformers on Small Datasets](https://aclanthology.org/2021.acl-long.163/)
- We call our method the Data-dependent Transformer Fixed-update initialization scheme, DT-Fixup.

- Based on the derivation, we propose a data-dependent initialization strategy for the mixed setup of the new transformers on pre-trained encodings.

- We first apply DT-Fixup on the task of crossdomain Text-to-SQL semantic parsing.

- On two important tasks, Text-to-SQL semantic parsing and logical reading comprehension that require reasoning and structural understanding, applying DT-Fixup achieves SOTA or near-SOTA results by simplying using extra transformer layers on top of the pre-trained models.




# [Do Context-Aware Translation Models Pay the Right Attention?](https://aclanthology.org/2021.acl-long.65/)
- Neural Machine Translation.

- Next, we study NMT models and quantify the degree to which the model's attention is aligned with the supporting context from professional translators.

- We apply attention regularization to guide model attention to increase alignment with the supporting context from SCAT.

- We examined how baseline context-aware translation models use context, and demonstrated how context annotations can improve context-aware translation accuracy.




# [Revisiting the Negative Data of Distantly Supervised Relation Extraction](https://aclanthology.org/2021.acl-long.277/)
- • Our empirical evaluations show that the proposed method consistently outperforms existing approaches, and achieves excellent perfor-mance even learned with a large quantity of false positive samples.

- In this paper, we revisit the negative data in relation extraction task.

- In this paper, we address these challenges caused by negative data.

- and propose a multi-label collective loss function.




# [Competence-based Multimodal Curriculum Learning for Medical Report Generation](https://aclanthology.org/2021.acl-long.234/)
- Then we propose the multiple difficulty-based curriculum learning for medical report generation.

- • We assess the difficulty of each training instance from multiple perspectives and propose a competence-based multimodal curriculum learning framework (CMCL) to consider multiple difficulties simultaneously.

- To this end, we propose a novel Competencebased Multimodal Curriculum Learning framework (CMCL) which progressively learns medical reports following an easy-to-hard fashion.

- In this section, we briefly describe typical medical report generation approaches and introduce the proposed Competence-based Multimodal Curriculum Learning (CMCL).




# [Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data](https://aclanthology.org/2021.acl-long.140/)
- • Noise-Aware Loss Function.

- • Weak Label Completion. As the weakly labeled data suffer from severe missing entity issue, we propose a weak label completion procedure.

- • NAL: Noise-aware loss function, i.e., Eq.( 4).

- Size of Weakly Labeled Data.




# [Taming Pre-trained Language Models with N-gram Representations for Low-Resource Domain Adaptation](https://aclanthology.org/2021.acl-long.259/)
- 4 Motivated by this find-3337 ing, we propose a light-weight Transformer-based Domain-aware N-gram Adaptor (T-DNA) by incorporating n-gram representations to bridge the domain gap between source and target vocabulary.

- To better represent and incorporate unseen and domain-specific n-grams, we first need to find and extract them.

- In the previous section, we show that T-DNA is helpful in fine-tuning.

- Experimental results demonstrate that T-DNA significantly improves domain adaptation performance based on a generic pre-trained model and outperforms all baselines on eight classification tasks (on eight datasets).




# [The Art of Abstention: Selective Prediction and Error Regularization for Natural Language Processing](https://aclanthology.org/2021.acl-long.84/)
- In this paper, we introduce the problem of selective prediction for NLP.

- In this part, we show that our simple regularization trick improves selective prediction performance.

- We provide theoretical background and evaluation metrics for the problem, and also propose a simple error regularization method that improves selective prediction performance for NLP models.

- Previously, selective prediction and confidence estimation have been studied in limited NLP scenarios.




# [Learning Prototypical Functions for Physical Artifacts](https://aclanthology.org/2021.acl-long.540/)
- Our work uses a subset of frames from FrameNet to represent the prototypical functions for human-made physical artifacts.

- We explored several approaches for learning the prototypical functions of human-made physical artifacts.

- First, we define a new NLP task to associate physical objects with frames from FrameNet as a canonical representation for their prototypical function.

- This paper focuses on the specific task of learning the prototypical functions for human-made physical artifacts using a subset of FrameNet frames as the set of function types.




# [Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger](https://aclanthology.org/2021.acl-long.37/)
- In this paper, we propose to use the syntactic structure as the trigger of textual backdoor attacks for the first time.

- Textual backdoor attacks are much less investigated.

- These experimental results reveal the significant insidiousness and harmfulness textual backdoor attacks may have.

- However, there are few studies on textual backdoor attacks.




# [Dialogue Response Selection with Hierarchical Curriculum Learning](https://aclanthology.org/2021.acl-long.137/)
- Dialogue Response Selection.

- We propose a hierarchical curriculum learning (HCL) framework for training neural matching models.

- Our learning framework jointly employs the corpus-level and instance-level curriculum.

- In this work, we propose a novel hierarchical curriculum learning framework for training response selection models for multi-turn conversations.




# [Evaluating Evaluation Measures for Ordinal Classification and Ordinal Quantification](https://aclanthology.org/2021.acl-long.214/)
- Surprisingly, however, there are only a small number of known evaluation measures that meet this requirement.

- This paper concerns evaluation measures for Ordinal Classification (OC) and Ordinal Quantification (OQ) tasks.

- Table 4 summarises the properties of the nine measures we examined in the context of OC tasks.

- Indeed, in the context of evaluating information retrieval evaluation measures, 




# [Generating Landmark Navigation Instructions from Maps as a Graph-to-Text Problem](https://aclanthology.org/2021.acl-long.41/)
- The main contributions of this paper are: • We collect and publish a large scale dataset of natural language landmark navigation instructions that are validated by human navigation runs in Street View.

- • We present a method to represent geospatial routes as a graph and propose an appropriate graph-to-text architecture that learns to generate navigation instructions from real-world data.

- We present a neural model that takes a real-world map repre-sentation from OpenStreetMap 1 as input and generates navigation instructions that contain salient landmarks, learned directly from human natural language instructions.

- The data collection resulted in 7,672 navigation instructions that were manually validated in Street View.




# [The Limitations of Limited Context for Constituency Parsing](https://aclanthology.org/2021.acl-long.208/)
- In this section, we formalize the results characterizing the representational power of the ON-LSTM architecture.

- To formalize our results, we con-sider the well-established sandbox of probabilistic context-free grammars (PCFGs).

- On the flipside, we show that restricting the context even mildly can considerably decrease the representational power.

- We quantify the representational power of two major frameworks in neural approaches to syntax: learning a syntactic distance (Shen et al., 2018a(Shen et al., ,b, 2019 and learning to parse through sequential transitions (Dyer et al., 2016;Chelba, 1997).




# [ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer](https://aclanthology.org/2021.acl-long.393/)
- We name our approach ConSERT, a Contrastive Framework for SEntence Representation Transfer.

- In this section, we present ConSERT for sentence representation transfer.

- In this paper, we propose ConSERT, a selfsupervised contrastive learning framework for transferring sentence representations to downstream tasks.

- It mitigates the collapse of BERT-derived representations and transfers them to downstream tasks.




# [Modeling Language Usage and Listener Engagement in Podcasts](https://aclanthology.org/2021.acl-long.52/)
- This paper presents the first quantitative analysis of how linguistic style and textual attributes in podcasts relate to listener engagement using automatically computed features.

- The BERT classifiers achieve nearly 81% accuracy, indicating that podcast content is highly predictive of engagement.

- We also show that the overall textual information in podcasts is highly predictive of engagement in this experiment, with an accuracy as high as 81%.

- Prediction accuracies (Table 4) are over 70% with linguistic features only, indicating that the features that we have identified are relatively strong predictors of engagement.




# [Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation](https://aclanthology.org/2021.acl-long.221/)
- • We propose an uncertainty-based sampling strategy for self-training, which selects more complementary sentences for the authentic parallel data.

- In this section, we introduced the uncertainty-based sampling strategy for self-training and the overall framework.

- In this work, we demonstrate the necessity of distinguishing monolingual sentences for self-training in NMT, and propose an uncertainty-based sampling strategy to sample monolingual data.

- Inspired by the above finding, we propose an uncertainty-based sampling strategy for selftraining, in which monolingual sentences with higher uncertainty would be selected with higher probability ( §3.1).




# [Bootstrapped Unsupervised Sentence Representation Learning](https://aclanthology.org/2021.acl-long.402/)
- It achieves state-of-the-art results on multiple semantic textual similarity (STS) tasks.

- We further extend our method for learning multilingual sentence representations and demonstrate that it is able to outperform strong multilingual baselines on cross-lingual STS tasks under both unsupervised and supervised settings.

- To demonstrate the flexibility of the proposed method, we further extend it for learning multilingual sentence representations and evaluate it on cross-lingual STS tasks.

- The experimental results demonstrate that our method could significantly outperform the state-of-the-art unsupervised methods and it can be further extended for learning multilingual sentence representations.




# [Learning Event Graph Knowledge for Abductive Reasoning](https://aclanthology.org/2021.acl-long.403/)
- (c) A latent variable z is employed to learn the commonsense knowledge from event graph.

- To facilitate this, proposed a text based abductive reasoning task αNLI.

- In this paper, we propose a variational autoencoder based framework ege-RoBERTa with a twostage training procedure for the abductive reasoning task.

- To this end, we propose a variational autoencoder based framework ege-RoBERTa, which employs a latent variable z to implicitly capture the necessary event graph knowledge and enhance the pretrained language model RoBERTa.




# [PhotoChat: A Human-Human Dialogue Dataset with Photo Sharing Behavior for Joint Image-Text Modeling](https://aclanthology.org/2021.acl-long.479/)
- In summary, our main contributions are: • We create the first human-human dialogue with photo sharing acts via crowd-sourcing.

- The maximum sequence length of BERT, ALBERT, and T5 for the photo-sharing intent prediction task is 512.

- The best photo-sharing intent prediction baseline model achieves 58.1% F1 score with 58.2% precision and 57.9% recall.

- • We propose two new tasks to promote building an intelligent photo suggest system.




# [Examining the Inductive Bias of Neural Language Models with Artificial Languages](https://aclanthology.org/2021.acl-long.38/)
- Thus, we offer a study investigating the inductive biases of language models through the construction of artificial languages.

- We suggest that properly investigating the inductive biases of language models will likely require artificial languages.

- In order to compare inductive biases across architectures, two neural architectures were tested: transformers and LSTMs.

- We set out to construct a set of PCFGs to expose the inductive bias of neural language models.




# [Verb Knowledge Injection for Multilingual Event Processing](https://aclanthology.org/2021.acl-long.541/)
- Cleanliness of Verb Knowledge.

- Event Extraction.

- Crucially, we showed that the benefits of the knowledge from resourcerich languages can be extended to other, resourceleaner languages through translation-based transfer of verb class/frame membership information.

- Figure 1 illustrates our framework for injecting verb knowledge from VerbNet or FrameNet and leveraging it in downstream event processing tasks.




# [HATECHECK: Functional Tests for Hate Speech Detection Models](https://aclanthology.org/2021.acl-long.4/)
- In this article, we introduced HATECHECK, a suite of functional tests for hate speech detection models.

- To enable more targeted diagnostic insights, we introduce HATECHECK, a suite of functional tests for hate speech detection models.

- 1 HATECHECK is broadly applicable across English-language hate speech detection models.

- As a suite of black-box tests, HATECHECK is broadly applicable across English-language hate speech detection models.




# [Towards Robustness of Text-to-SQL Models against Synonym Substitution](https://aclanthology.org/2021.acl-long.195/)
- In short, we make the following contributions: • We conduct a comprehensive study to evaluate the robustness of text-to-SQL models against synonym substitution.

- We present two categories of approaches for improving model robustness to synonym substitution.

- cluding text-to-SQL translation.

- Text-to-SQL translation.




# [Assessing the Representations of Idiomaticity in Vector Models with a Noun Compound Dataset Labeled at Type and Token Levels](https://aclanthology.org/2021.acl-long.212/)
- This section displays some of the comparative analyses for the relevance of type and token annotation for idiomaticity detection.

- Our contributions can be summarised as: (1) building the NCTTI dataset with information about type and token idiomaticity for NCs in two languages, (2) evaluating to what extent models are able to detect idiomaticity at type and token level, analysing different levels of contextualisation and (3) proposing two new measures of idiomaticity.

- This paper presented the NCTTI, a dataset of NCs in English and Portuguese annotated at type and token level with human judgments about idiomaticity, and with suggestions of paraphrases.

- These results indicate at best weak (NC out Comp ) to moderate (NC out) correlations between models' predictions and human judgments, both at type and token levels.




# [UNIRE: A Unified Label Space for Entity Relation Extraction](https://aclanthology.org/2021.acl-long.19/)
- We propose a novel table that presents entities and relations as squares and rectangles.

- The joint decoder is set to find the best squares and rectangles.

- For example, entities and relations correspond to squares and rectangles in the table.

- Unified Label Space This family of methods aims to unify two sub-tasks tackle this task in a unified label space.




# [Deep Differential Amplifier for Extractive Summarization](https://aclanthology.org/2021.acl-long.31/)
- Our contributions in this work are concluded as follows: • We propose a novel conceptualization of extractive summarization as rebalance problem.

- • We introduce a heuristic approach, calculating and amplifying the semantic representation of pivotal information by integrating both the differential amplifier and residual learning.

- Besides, we employ another weighted cross-entropy function to compensate for the imbalance.

- Besides, another heuristic method is to make our model pay more attention to 1-class: a weighted cross-entropy function.




# [Measure and Evaluation of Semantic Divergence across Two Languages](https://aclanthology.org/2021.acl-long.100/)
- We analyse the semantic divergence of wordtranslation pairs in a bilingual corpus of news articles.

- BERT embeddings coupled with a clustering step lead to the best performance on synthetic corpora.

- Semantic change across languages.

- In this paper, we define an experimental framework to measure and classify the semantic divergence of a word and its translation in a bilingual corpus.




# [Mention Flags (MF): Constraining Transformer-based Text Generators](https://aclanthology.org/2021.acl-long.9/)
- These two signals allow the model to achieve high constraint satisfaction and help to maintain high text quality

- As shown in Table 6, the MF models consistently generate higher-quality text (higher METEOR or CIDEr Score) and achieve higher constraint satisfaction than the baseline models.

- This section shows that Mention Flags are still useful for improving the constraint satisfaction and generated text quality when trained with many fewer instances.

- We have shown that our proposed MF model can achieve higher constraint satisfaction ratio and automatic metrics.




# [Learning to Perturb Word Embeddings for Out-of-distribution QA](https://aclanthology.org/2021.acl-long.434/)
- • We propose a simple yet effective data augmentation method to improve the generalization performance of pretrained language models for QA tasks.

- We proposed a simple yet effective data augmentation method based on a stochastic word embedding perturbation for out-of-distribution QA tasks.

- Specifically, our stochastic noise generator learns to generate the adaptive noise depending on the contextualized embedding of each word.

- • We extensively validate our method for domain generalization tasks on diverse datasets, on which it largely outperforms strong baselines, including a QA-pair generation method.




# [Every Bite Is an Experience: Key Point Analysis of Business Reviews](https://aclanthology.org/2021.acl-long.262/)
- Aspect-based sentiment summarization.

- Overall, this work makes a dual contribution: first, it proposes a new framework for review summarization.

- Previous work on review summarization was dominated by two paradigms: aspect-based sentiment summarization and multi-document opinion summarization.

- As discussed in the previous section, textual summaries provide more detail than aspect-based sentiment summaries, but lack a quantitative dimension.




# [Language Model Augmented Relevance Score](https://aclanthology.org/2021.acl-long.521/)
- In this paper, we present MARS (Language Model Augmented Relevance Score), a new NLG evaluation metric that requires neither supervision from human ratings nor additional training on specific domains.

- off-the-shelf

- We have proposed MARS, a context-aware and easy-to-deploy NLG metric built upon an off-theshelf language model (GPT-2).

- Second, MARS is context-aware.




# [EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets](https://aclanthology.org/2021.acl-long.171/)
- Our work makes the first attempt of introducing LTH to both efficient pre-training and efficient fine-tuning of BERT.

- Inspired by this, we set out to explore whether there are structured winning tickets in the early stage of BERT training that can significantly accelerate language model pre-training and fine-tuning.

- By instead using network slimming (Liu et al., 2017) on the self-attention and fully-connected sub-layers inside a transformer, we are the first to introduce an effective approach that can identify structured winning tickets in the early stage of BERT training, that are successfully applied for efficient language modeling pre-training and fine-tuning.

- Experimental results demonstrate that the proposed method is able to achieve comparable performance to standard BERT with much less training time.




# [Risk Minimization for Zero-shot Sequence Labeling](https://aclanthology.org/2021.acl-long.380/)
- In this paper, we propose two approaches to the zero-shot sequence labeling problem.

- We design a new decomposable risk function parameterized by a fixed matrix that models the relations between the noisy predictions from the source models and the true labels.

- It suggests that our LVM approach learns the relations between predicted labels from the source models and true labels better than MRT.

- Meanwhile, introducing uncertainties for the relations between the predicted labels from the source models and the true labels in both training and prediction processes significantly benefit our approaches.




# [Discovering Dialogue Slots with Weak Supervision](https://aclanthology.org/2021.acl-long.189/)
- We achieve state-of-the-art results for slot tagging without manual supervision in four different domains, with a 6-16% absolute F1 score increase over the previous benchmark.

- To verify the usefulness of the labels discovered by our method, we use them to train and evaluate an end-to-end task-oriented dialogue system.

- We train an end-to-end neural dialogue system using our automatically discovered slots in the restaurant domain and demonstrate that our approach improves performance over an unsupervised model, finding the correct venue in 5% more cases (35% more when no restaurant ontology is provided).

- Finally, we use the resulting slot labels in the corpus to train a neural slot tagger (Section 3.3).




# [Surprisal Estimators for Human Reading Times Need Character Models](https://aclanthology.org/2021.acl-long.290/)
- This paper presents a character model that can be used to estimate word generation probabilities in a structural parser-based processing model.

- To answer this question, this paper presents a character model that can be used to estimate word generation probabilities in a structural parser-based processing model.

- Experiments demonstrate that surprisal estimates calculated from this processing model generally contribute to substantially better fits to human response data than those calculated from large-scale pretrained language models or other incremental parsers.

- Regression analyses on self-paced reading, eye-tracking, and fMRI data demonstrate that surprisal estimates calculated from this character-based structural processing model contribute to substantially better fits compared to those calculated from large-scale language models, despite the fact that these other models are trained on much more data and show lower perplexities on test data.




# [AdaTag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding](https://aclanthology.org/2021.acl-long.362/)
- But we utilize an adaptive decoding approach, where the decoding network is parameterized with the attribute embedding.

- In this paper we address the limitations of the existing contribution lines, through adaptive decoder parameterization.

- We propose to generate a decoder on the fly for each attribute based on its embedding.

- To this end, we use conditional random fields (CRF) (Lafferty et al., 2001) as the decoders, and parameterize the decoding layers with the attribute embedding through a hypernetwork (Ha et al., 2017) and a Mixture-of-Experts (MoE) module (Jacobs et al., 1991).




# [Exploiting Document Structures and Cluster Consistencies for Event Coreference Resolution](https://aclanthology.org/2021.acl-long.374/)
- We present a novel end-to-end coreference resolution framework for event mentions based on deep learning.

- The experiments demonstrate the benefits of the proposed methods and lead to state-of-the-art performance for ECR.

- Second, several regularization techniques are proposed to exploit the consistencies between human-provided and machine-generated clusters of event mentions in documents.

- Table 3 presents the performance of the models.




# [How to Adapt Your Pretrained Multilingual Model to 1600 Languages](https://aclanthology.org/2021.acl-long.351/)
- Pretrained multilingual models (PMMs) are a straightforward way to enable zero-shot learning via cross-lingual transfer, thus eliminating the need for labeled data for the target task and language.

- Prior to the introduction of PMMs, cross-lingual transfer was often based on word embeddings (Mikolov et al., 2013).

- Bharadwaj et al. (2016) use phoneme conversion to aid cross-lingual NER in a zero-shot setting.

- Similarly, translation-based approaches, as well as few-shot learning may offer additional benefits over a zero-shot setting.




# [Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence](https://aclanthology.org/2021.acl-long.499/)
- We summarize our contributions in two folds: I. We propose a generation model named HINT for long text generation.

- In this paper, we propose HINT, a generation model equipped with HIgh-level representations for loNg Text generation.

- Results show that HINT can learn meaningful high-level representations and generate more coherent long texts than baselines.

- We adopt two pretraining objectives called similarity prediction and order discrimination to learn the representations at sentence level and discourse level, respectively.




# [Energy-Based Reranking: Improving Neural Machine Translation Using Energy-Based Models](https://aclanthology.org/2021.acl-long.349/)
- We introduce energy-based re-ranking (EBR) to improve the performance of autoregressive neural machine translation.

- The former results in the effective maximum likelihood estimation (MLE) for training the parameters of NMT models.

- We believe that pre-trained BERT helps low-resource tasks more than large-scale translation tasks.

- Figure 2: The EBM is trained such that its energy landscape is consistent with the BLEU score.




# [On Sample Based Explanation Methods for NLP: Efficiency, Faithfulness, and Semantic Evaluation](https://aclanthology.org/2021.acl-long.419/)
- A benchmark of evaluating sample-based explanation methods has not been agreed upon.

- Our contributions are: 1. We propose a new explanation framework that can use arbitrary explanation units as explanations and be Hessian-free and faithful at the same time; 2. A new metric to measure the semantic relatedness between a test instance and its explanation for BERT-based deep models.

- The new method allows for arbitrary text spans as the explanation unit and is Hessian-free while being faithful to the final model.

- 2) Can we understand the connection between IF and TracIn which may spark discoveries on sample-based explanation methods?




# [Alignment Rationale for Natural Language Inference](https://aclanthology.org/2021.acl-long.417/)
- To resolve above problems, this paper proposes AREC, a post-hoc local approach to generate Alignment Rationale Explanation for Co-attention based models.

- Our contributions are summarized as follows: 1) We come up with AREC, a post-hoc local explanation method to extract the alignment rationale for co-attention based models.

- Experimental results show that our method could generate more faithful and readable explanations.

- Experimental results show that our explanation is faithful and readable.




# [DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue](https://aclanthology.org/2021.acl-long.439/)
- We have introduced DVD, a diagnostic dataset signed to analyze video-grounded dialogue systems.

- VIT is a new reasoning requirement in video-grounded dialogue tasks.

- To address the limitations of existing benchmarks and analyze dialogue systems more efficiently, we propose DVD, a Diagnostic Dataset for Video-grounded Dialogues.

- We designed our experiments to systematically analyze model capabilities and shortcomings through unique challenges in video-grounded dialogue systems.




# [A Knowledge-Guided Framework for Frame Identification](https://aclanthology.org/2021.acl-long.407/)
- Thus, in this paper, we propose a Knowledge Guided Frame Identification framework (KGFI) which consists of a Bert-based context encoder and a frame encoder based on a specialized graph convolutional network (FrameGC-N).

- In particular, the frame encoder incorporates multiple types of frame knowledge into frame representation which guides the KGFI to jointly map target words and frames into the same embedding space.

- (3) Three types of frame-to-frames relations, including Inherits, Using and Subframe, are used in this study.

- In summary, our contribution is threefold: • To the best of our knowledge, we are the † See the details in https://FN.icsi.berkeley.edu/fndrupal/ first to propose a unified FI method which leverages heterogeneous frame knowledge for building rich frame representations.




# [Pre-training Universal Language Representation](https://aclanthology.org/2021.acl-long.398/)
- Furthermore, we conduct experiments on a wide range of downstream tasks from the GLUE benchmark and a question answering task.

- ULR-BERT Our universal language representation model trained on Wikipedia with MLM and MiSAD.

- A universal language representation model encodes linguistic units such as words, phrases or sentences into fixed-sized vectors and handles multiple layers of linguistic objects in a unified way.

- This work formally introduces universal language representation learning to enable unified vector operations among different language hierarchies.




# [Contributions of Transformer Attention Heads in Multi-and Cross-lingual Tasks](https://aclanthology.org/2021.acl-long.152/)
- In this paper, we explore the roles of attention heads in cross-lingual and multi-lingual tasks for two reasons.

- This paper studied the contributions of attention heads in Transformer-based models.

- As Table 3 shows, pruning attention heads generally has positive effects on our cross-lingual and multi-lingual NER models.

- The biggest challenge we face when studying the roles of attention heads in cross-lingual and multi-lingual tasks is locating the heads to prune.




# [Transition-based Bubble Parsing: Improvements on Coordination Structure Prediction](https://aclanthology.org/2021.acl-long.557/)
- Empirically, our bubble parsers achieve state-of-the-art results on the task of coordination structure prediction on two English datasets.

- Our method achieves state-of-the-art performance on both datasets and improves accuracy on the subset of sentences exhibiting complex coordination structures.

- Task and Evaluation We validate the utility of our transition-based parser using the task of coordination structure prediction.

- Experiments on the English Penn Treebank (PTB; extended with coordination annotation and the English GENIA treebank demonstrate the effectiveness of our proposed transition-based bubble parsing on the task of coordination structure prediction.




# [Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models](https://aclanthology.org/2021.acl-long.333/)
- Lightweight convolutions consistently improved performance.

- Our empirical results provide evidence for future research integrating convolutions and self-attention for NLP.

- In this work, we formalized the relationship between self-attention and convolution.

- Our findings provide a solid foundation from which to study convolutions and self-attention in language tasks.




# [SENT: Sentence-level Distant Relation Extraction via Negative Training](https://aclanthology.org/2021.acl-long.484/)
- To summarize the contribution of this work: • We propose the use of negative training for sentence-level distant RE, which greatly protects the model from noisy information.

- • We present a sentence-level framework, SENT, which includes a noise-filtering and a re-labeling strategy for re-fining distant data.

- Based on NT, we propose SENT, a sentencelevel framework for distant RE.

- In this work, we propose the use of negative training (NT) (Kim et al., 2019) for distant RE.




# [Anonymisation Models for Text Data: State of the Art, Challenges and Future Directions](https://aclanthology.org/2021.acl-long.323/)
- The case study illustrates a number of issues facing current methods for text anonymisation.

- This position paper discussed a number of unresolved challenges in text anonymisation.

- In this paper, we review the core concepts underlying text anonymisation, and survey the approaches put forward to solve this task.

- The five annotators were researchers without previous experience in text anonymisation.




# [Structural Knowledge Distillation: Tractably Distilling Information for Structured Predictor](https://aclanthology.org/2021.acl-long.46/)
- 3. The teacher factorization produces more finegrained substructures than the student factorization.

- 2. The student factorization produces more finegrained substructures than the teacher factorization.

- We derive a factorized form of the structural KD objective and make it tractable to compute and optimize for many typical choices of teacher and student models.

- In this paper, we propose structural knowledge distillation, which transfers knowledge between structured prediction models.




# [A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters](https://aclanthology.org/2021.acl-long.447/)
- Zero-/Few-Shot Crosslingual Transfer.

- Variance of Few-Shot Transfer.

- In this work, however, we demonstrate that the gains from few-shot transfer exhibit a high degree of sensitivity to the selection of few shots.

- We have presented an extensive study of few-shot crosslingual transfer.




# [Shortformer: Better Language Modeling Using Shorter Inputs](https://aclanthology.org/2021.acl-long.427/)
- as in the WikiText-103 models

- We then introduce new methods based on shorter input subsequences that improve runtime, memory efficiency, and perplexity.

- Finally, we show additive gains from combining staged training and position-infused attention (Shortformer, §6), resulting in a model that trains much quicker and achieves better perplexity on WikiText-103.

- We propose a two-stage training routine that initially uses short input subsequences followed by long subsequences.




# [Breaking Down the Invisible Wall of Informal Fallacies in Online Discussions](https://aclanthology.org/2021.acl-long.53/)
- In this appendix, we review the most frequent fallacies on Reddit.

- In our work, we align frequent fallacies on Reddit with these rules, with the goal of formalizing their definitions.

- We find frequent fallacy mentions on Reddit and the subreddits in which they are the most prevalent.

- The remaining fallacies are selected for the creation of an annotated dataset of fallacies.




# [De-Confounded Variational Encoder-Decoder for Logical Table-to-Text Generation](https://aclanthology.org/2021.acl-long.430/)
- Logical Table-to-Text Generation.

- In this paper, we propose a de-confounded variational encoder-decoder for the logical table-to-text generation.

- And we apply the causal intervention method to reduce the spurious correlations.

- The experiments have shown that our model achieves new state-of-the-art performance on two logical table-to-text datasets with or without pretrained LMs.




# [Glancing Transformer for Non-Autoregressive Neural Machine Translation](https://aclanthology.org/2021.acl-long.155/)
- Based on GLM, we develop the glancing Transformer (GLAT) for neural machine translation.

- In this paper, we propose Glancing Transformer with a glancing language model to improve the performance of single-pass parallel generation models.

- The glancing Transformer (GLAT) formulates a glancing language model (GLM) during training.

- Transformer has been the most widely used architecture for machine translation (Vaswani et al., 2017).




# [A Large-Scale Chinese Multimodal NER Dataset with Speech Clues](https://aclanthology.org/2021.acl-long.218/)
- • We further propose a multimodal multitask method by introducing a speech-to-text alignment auxiliary task.

- • We establish a family of baselines to leverage textual features or multimodal features.

- Furthermore, we propose a simple multimodal multitask method by introducing a speechto-text alignment auxiliary task.

- In this paper, we explore Chinese multimodal NER with both textual and acoustic contents.




# [Unsupervised Out-of-Domain Detection via Pre-trained Transformers](https://aclanthology.org/2021.acl-long.85/)
- To the best of our knowledge, our method is the first to incorporate transformers and pre-training techniques to improve out-of-domain detection.

- We explore two domain-specific fine-tuning approaches.

- • We propose two different techniques for finetuning a pre-trained transformer to further improve its capability of detecting OOD data.

- We study the problem of detecting out-of-domain samples with unsupervised in-domain data, which is a more general setting for out-of-domain detection.




# [Enhancing the generalization for Intent Classification and Out-of-Domain Detection in SLU](https://aclanthology.org/2021.acl-long.190/)
- The evaluation on four datasets shows that DRM can consistently improve upon previous state-of-the-art methods.

- Thus, we mainly focus on comparing our method with strong baselines with BERT and RoBERTa models.

- Among all OOD detection approaches, our proposed L-Mahalanobis OOD detection approach achieves the best performance for both linear and DRM combined BERT and RoBERTa models.

- We call the structure a Domain-Regularized Module (DRM).




# [Claim Matching Beyond English to Scale Global Fact-Checking](https://aclanthology.org/2021.acl-long.347/)
- To construct a dataset for claim matching, we design a two-step sampling and annotation process.

- Human fact-checking is high-quality but timeconsuming.

- We also compare our system with other state-of-the-art multilingual embedding models used for reranking, namely LASER and LaBSE.

- Claim Matching Classifier.




# [Improving Encoder by Auxiliary Supervision Tasks for Table-to-Text Generation](https://aclanthology.org/2021.acl-long.466/)
- And then, we introduce a reasoning module to perform reasoning on the graph.

- Furthermore, we investigate the impact of the two auxiliary tasks on table-to-text generation.

- Moreover, to endow the model with the reasoning ability, we first build an entity graph on the row level according to the relations between players and teams.

- Table -to-text generation is an essential task for text generation from structured data.




# [Enhancing Content Preservation in Text Style Transfer Using Reverse Attention and Conditional Layer Normalization](https://aclanthology.org/2021.acl-long.8/)
- The contributions are as follows: • We introduce reverse attention as a way to suppress style information while preserving content information when building a content representation of an input.

- In this paper, we propose a novel method for text style transfer.

- This paper further enhances content preservation by fusing content information in creating target style representation.

- Thus, unlike previous attempts in text style transfer, the style representation is dynamic respect to the content, being content-dependent embedding.




# [Integrated Directional Gradients: Feature Interaction Attribution for Neural NLP Models](https://aclanthology.org/2021.acl-long.71/)
- • Second, we state a set of axioms that a well behaved feature group attribution method should satisfy (Section 2.2).

- • Third, we present the method of Integrated Directional Gradients or IDG as a solution to the feature group attribution problem that satisfies the stated axioms (Section 2.3).

- In this section we present a solution to the "feature group attribution problem" that we call the Integrated Directional Gradients method or IDG.

- The major contributions of the current work are as follows: • First, we formally define the feature group attribution problem as an extension to the feature attribution problem (Section 2.1).




# [Exploring Distantly-Labeled Rationales in Neural Network Models](https://aclanthology.org/2021.acl-long.433/)
- Distantly-labeled rationale words may vary dramatically in quality.

- In practice, distantly-labeled rationales serve as a plausible alternative.

- Although distantly-labeled rationale words are often universally helpful, given a specific context, different rationale words may exhibit varied importance.

- We refer to the distantly-labeled rationale words that are not helpful in a specific instance as Non-Important Rationales, or NoIRs for short.




# [Selective Knowledge Distillation for Neural Machine Translation](https://aclanthology.org/2021.acl-long.504/)
- In summary, our contributions are as follows: • We propose a novel protocol for analyzing the property for the suitable medium samples for transferring teacher's knowledge.

- • We propose two selective strategies: batchlevel selection and global-level selection.

- Experimental results show that our approach yields an improvement of +1.28 and + 0.89 BLEU points over the Transformer baseline.

- To address this problem, we propose two simple yet effective strategies, namely the batch-level selection and global-level selection.




# [Verb Metaphor Detection via Contextual Relation Learning](https://aclanthology.org/2021.acl-long.327/)
- We propose the Metaphor-relation BERT (Mr-BERT) model to realize verb metaphor detection as a relation classification task.

- This paper presented the Metaphor-relation BERT (MrBERT) model for verb metaphor detection.

- Second, MrBERT enables modeling the metaphorical relation between a verb and its context components, and uses the relation representation for determining the metaphoricity of the verb.

- Table 8 shows the results on the MOH-X and TroFi datasets.




# [Accelerating Text Communication via Abbreviated Sentence Input](https://aclanthology.org/2021.acl-long.514/)
- We investigated abbreviation by omitting midword vowels.

- Abbreviated input.

- Here we investigate accelerating input by allowing users to skip typing spaces and mid-word vowels.

- We found that dropping spaces and mid-word vowels can provide compression of sentences from 28% to 38%.




# [CitationIE: Leveraging the Citation Graph for Scientific Information Extraction](https://aclanthology.org/2021.acl-long.59/)
- In this paper, we show citation graph embeddings can improve scientific information extraction.

- This leads to a sizable increase in the performance of the end-to-end CitationIE system relative to the current state-of-the-art, Jain et al. (2020).

- We break down the end-to-end information extraction process as a sequence of these four related tasks, with each task taking the output of the preceding tasks as input.

- We used early fusion for saliency classification in the end-to-end model due to strong empirical performance there.




# [CHASE: A Large-Scale and Pragmatic Chinese Dataset for Cross-Database Context-Dependent Text-to-SQL](https://aclanthology.org/2021.acl-long.180/)
- • CHASE, to the best of our knowledge, is the first large-scale and pragmatic Chinese dataset for XDTS.

- Given the limitations of existing datasets, we present CHASE, a large-scale and pragmatic Chinese dataset for XDTS.

- This work presents CHASE, a free and open dataset for the research community to study the crossdatabase context-dependent Text-to-SQL problem (XDTS).

- Upon identifying the limitations of existing datasets, we present CHASE, a large-scale and pragmatic Chinese dataset for XDTS.




# [Measuring Conversational Uptake: A Case Study on Student-Teacher Interactions](https://aclanthology.org/2021.acl-long.130/)
- We introduce a framework for computationally measuring uptake.

- We propose a framework for measuring uptake, a core conversational phenomenon with particularly high relevance in teaching contexts.

- Now we introduce our main uptake measure, used to capture a broader range of uptake phenomena beyond repetition including, e.g., acknowledgment and question answering (Section 2).

- There are multiple linguistic strategies for uptake, such as acknowledgment, collaborative completion, repetition, and question answering -see Figure 1 for a non-exhaustive list.




# [StereoSet: Measuring stereotypical bias in pretrained language models](https://aclanthology.org/2021.acl-long.416/)
- We show that current pretrained language models exhibit strong stereotypical biases.

- In this work, we assess the stereotypical biases of popular pretrained language models.

- StereoSet measures stereotypical biases in gender, profession, race, and religion.

- In this work, we propose methods to evaluate stereotypical bias of pretrained language models.




# [Factorising Meaning and Form for Intent-Preserving Paraphrasing](https://aclanthology.org/2021.acl-long.112/)
- We present SEPARATOR, a method for generating paraphrases that balances high variation in surface form with strong intent preservation.

- In this paper, we propose SEPARATOR, a method for generating paraphrases that exhibit high variation in surface form while still retaining the original intent.

- Kumar et al. (2019) use submodular function maximisation to improve the diversity of paraphrases generated by an encoder-decoder model.

- SEPARATOR is able to generate question paraphrases with a better balance of diversity and intent preservation compared to prior work.




# [Changes in European Solidarity Before and During COVID-19: Evidence from a Large Crowd-and Expert-Annotated Twitter Dataset](https://aclanthology.org/2021.acl-long.129/)
- (iii) We present novel empirical evidence regarding changes in European solidarity debates before and after the outbreak of the COVID-19 pandemic.

- In this paper, we contributed the first large-scale human and automatically annotated dataset labeled for solidarity and its contestation, anti-solidarity.

- Figure 4: Scatter plot between infection rates and number of anti-solidarity tweets.

- Our contributions are: (i) We provide a novel Twitter corpus annotated for expressions of social solidarity and antisolidarity.




# [MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation](https://aclanthology.org/2021.acl-long.440/)
- In this paper, we propose an multimodal fused graph convolutional network (MMGCN) for multimodal emotion recognition in conversation (ERC).

- It constructs a graph that captures not only intra-speaker context dependency but also inter-modality dependency.

- In order to effectively explore the multimodal information and at the same time capture longdistance contextual information, we propose a new multimodal fused graph convolutional network (MMGCN) model in this work.

- In order to capture the utterance-level contextual dependencies across multiple modalities, we propose a Multimodal fused Graph Convolutional Network (MMGCN).




# [Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis](https://aclanthology.org/2021.acl-long.26/)
- We proposed a new framework to automatically generate counterfactual augmented data (CAD) for enhancing the robustness of sentiment analysis models.

- It suggests that our method have its absolute advantage for data augmentation in sentiment analysis when compared to the state-of-theart style-transfer models.

- First, we conduct a systematic comparison of several different state-of-the-art models (Wang and Culotta, 2021).

- We further show that our methods can achieve better performance even when compared to models trained with humangenerated counterfactuals.




# [Automated Concatenation of Embeddings for Structured Prediction](https://aclanthology.org/2021.acl-long.206/)
- Together with fine-tuned embeddings, ACE achieves state-of-the-art performance in 6 tasks over 21 datasets.

- In this paper, we propose Automated Concatenation of Embeddings, which automatically searches for better embedding concatenation for structured prediction tasks.

- Results show that ACE with fine-tuned embeddings achieves state-of-the-art performance in all test sets, which shows that finding a good embedding concatenation helps structured prediction tasks.

- In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks.




# [ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining](https://aclanthology.org/2021.acl-long.535/)
- Benchmarking Other Conversation Summarization Datasets We benchmark our models on widely used meeting summarization datasets.

- We believe that such benchmarking will facilitate a more straightforward comparison of conversation summarization models across domains.

- In addition to introducing manually-curated datasets for conversation summarization, we also aim to unify previous work in conversation summarization.

- As we propose novel conversation summarization datasets and modeling components, this section is divided into the following two parts.




# [Evaluation Examples Are Not Equally Informative: How Should That Change NLP Leaderboards?](https://aclanthology.org/2021.acl-long.346/)
- This paper advocates incorporating decades of research in crafting education tests to improve how we evaluate the capabilities of NLP models.

- Then we show that-like in educational testing-IRT identifies good and bad items.

- In addition to better understanding datasets, it is also helpful for efficiently selecting evaluation items to annotate.

- We propose and validate an alternate IRT ranking method for leaderboard evaluations, show it can guide annotation, detect annotation error, and naturally partition evaluation data.




# [Can vectors read minds better than experts? Comparing data augmentation strategies for the automated scoring of children's mindreading ability](https://aclanthology.org/2021.acl-long.96/)
- We also showed that data augmentation improves the performance on unseen data.

- First, data augmentation can successfully be used to improve the performance of the systems on the MIND-CA corpus.

- Second, data augmentation also improves the performance of the automated systems on the unseen examples from UK-MIND-20.

- We demonstrated that data augmentation can improve the performance of automated systems including on novel, unseen data.




# [Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference](https://aclanthology.org/2021.acl-long.488/)
- In this paper, we introduce KECI (Knowledge-Enhanced Collective Inference), a novel end-to-end framework that utilizes external domain knowledge for joint entity and relation extraction.

- In this work, we propose a novel span-based framework named KECI that utilizes external domain knowledge for joint entity and relation extraction from biomedical text.

- Also, joint entity and relation extraction can be naturally formulated as the task of extracting a span graph from an input document .

- Table 6 shows some examples from the ADE dataset that illustrate how incorporating external knowledge can improve the performance of joint biomedical entity and relation extraction.




# [Attention Calibration for Transformer in Neural Machine Translation](https://aclanthology.org/2021.acl-long.103/)
- • Detailed analyses show that calibrated attention weights are more uniform at lower layers while more focused at the higher layers.

- Highentropy attention weights are found to have great needs for calibration at all layers.

- We further find a greater need for calibration in the original attention weights with high entropy.

- In contrast, high-layer attention weights become more focused on specific essential inputs.




# [All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text](https://aclanthology.org/2021.acl-long.565/)
- While the accuracy of classifying GPT2-vs. human-authored text is significantly 7 different from chance, evaluators' accuracy distinguishing GPT3-and humanauthored text is not.

- We found that evaluators were unable to distinguish between GPT3and human-authored text across story, news, and recipe domains.

- We found that untrained evaluators were unable to distinguish between human-and GPT3-generated text from three domains.

- 3 Can we train evaluators to better identify machine-generated text?




# [On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study](https://aclanthology.org/2021.acl-long.517/)
- Results on an extensive collection of out-of-domain evaluation sets suggest that ADC training data does not offer clear benefits vis-à-vis robustness under distribution shift.

- In this paper, we demonstrated that across a variety of models and datasets, training on adversarial data leads to better performance on evaluation sets created in a similar fashion, but tends to yield worse performance on out-of-domain evaluation sets not created adversarially.

- For all models, we find that while fine-tuning on adversarial data usually leads to better performance on (previously collected) adversarial data, it typically leads to worse performance on a large, diverse collection of out-of-domain datasets (compared to fine-tuning on standard data).

- Overall, we find that models fine-tuned on ADC data typically generalize better to out-ofdomain adversarial test sets than models fine-tuned on SDC data, confirming the findings by Dinan et al. (2019).




# [Prosodic segmentation for parsing spoken dialogue](https://aclanthology.org/2021.acl-long.79/)
- Our primary contributions are: • We show that a parser that has access to prosody can perform both SU segmentation and parsing as well as a model that only has to parse.

- However, pitch and intensity features are extracted from the speech signal at the frame level.

- Further analysis indicates that adding pitch and intensity features can help the model to disambiguate the two, while pause and duration features do not.

- We report the model's performance with and without prosodic features during the segmentation and parsing steps.




# [W-RST: Towards a Weighted RST-style Discourse Framework](https://aclanthology.org/2021.acl-long.302/)
- In particular, we find that automatically generated, weighted discourse trees can benefit key NLP downstream tasks.

- Further, we show that W-RST trees interestingly align with the uncertainty of human annotations.

- We further show that real-valued importance scores (at least partially) align with human annotations and can interestingly also capture uncertainty in human annotators, implying some alignment of the importance distributions with linguistic ambiguity.

- Based on this observation, we investigate the potential of replacing the binary nuclearity assessment postulated by RST with automatically generated, real-valued importance scores in a new, Weighted-RST framework.




# [Modeling Transitions of Focal Entities for Conversational Knowledge Base Question Answering](https://aclanthology.org/2021.acl-long.255/)
- (2) We propose a graphbased neural network model to capture the transitions of focal entities and derive a focal entity distribution that can be plugged into a standard single-turn KBQA system.

- In this paper, we present a method to model the transitions of focal entities in a conversation in order to improve conversational KBQA.

- Our work also intends to capture the flow of the conversation but we specifically model the transitions of focal entities.

- We summarize our contributions of this paper as follows: (1) We propose to explicitly model the focal entities of a conversation in order to improve conversational KBQA.




# [EnsLM: Ensemble Language Model for Data Diversity by Semantic Clustering](https://aclanthology.org/2021.acl-long.230/)
- Inspired by them, in order to jointly consider topic learning and sample clustering, we propose the autoencoding topic model with mixture prior (mATM).

- To jointly consider the clustering and topic modeling for better clustering (as shown in Fig. 1b) and for joint training with the following LM, we firstly introduce an autoencoding topic model with mixture priors (mATM).

- Guided by clustering assignments that describe the data diversity, EnsLM learns both shared and cluster-specific knowledge by weight modulations.

- For the topic model, we additionally exclude stopwords.




# [Learning Latent Structures for Cross Action Phrase Relations in Wet Lab Protocols](https://aclanthology.org/2021.acl-long.525/)
- This paper is organized around two main contributions: (i): the WLP-MSTG Corpus that extends the WLP Corpus (Kulkarni et al., 2018) by including intra-and cross-sentence temporal and causal relationships and (ii): a novel model that builds upon latent structures to resolve implicit arguments and long-range relations spanning multiple sentences.

- We also report significant improvements in understanding implicit arguments and identifying long range relationships across multiple sentences.

- This analysis provides valuable insight about the challenges in the form of long-range relations and implicit arguments that are present in extracting MSTGs from WLPs.

- Furthermore, neither successfully establish complex relations involving implicit arguments.




# [Semi-Supervised Text Classification with Balanced Deep Representation Distributions](https://aclanthology.org/2021.acl-long.391/)
- To alleviate the aforementioned problem, we propose a novel SSTC method built on BERT with AM loss, namely Semi-Supervised Text Classification with Balanced Deep representation Distributions (S 2 TC-BDD).

- 3 The Proposed S 2 TC-BDD Method In this section, we describe the proposed deep selftraining SSTC method, namely Semi-Supervised Text Classification with Balanced Deep representation Distributions (S 2 TC-BDD).

- In particular, we develop a Balanced Deep representation Distribution (BDD) loss, aiming at more accurate pseudo-labels for unlabeled texts.

- We can estimate each label angle variance over both labeled and pseudo-labeled texts during the self-training loops.




# [CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding](https://aclanthology.org/2021.acl-long.181/)
- To train a robust semantic-aware PLM, we propose Contrastive Learning with semantIc Negative Examples (CLINE).

- Thus, it is crucial to explore the appropriate methods to learn changed semantics from semantic negative examples.

- 3) By constructing positive and negative examples for contrastive learning in pre-training stage, our method CLINE-B and CLINE-R learn better sentence representation and detect small semantic changes.

- In this paper, we propose a negative example construction strategy with opposite semantics to improve the sentence representation learning and the robustness of the pre-trained language model.




# [An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models](https://aclanthology.org/2021.acl-long.178/)
- First, can automated HPO methods outperform grid search?

- Results of the initial study show that HPO often fails to match grid search's performance.

- Time Budget.

- To bridge this gap, in this paper, we propose an experimental study for fine-tuning pre-trained language models using the HuggingFace library.




# [Neural Machine Translation with Monolingual Translation Memory *](https://aclanthology.org/2021.acl-long.567/)
- (3) Our model gains a strong cross-domain transferability by hot-swapping domain-specific monolingual memory.

- (2) Our model can substantially boost translation quality in low-resource scenarios by utilizing extra monolingual TM that is not present in training pairs.

- We show that a task-specific cross-lingual memory retriever can be learned by end-to-end MT training.

- Then in §3.2, we describe the model design for the cross-lingual memory retrieval model.




# [Directed Acyclic Graph Network for Conversational Emotion Recognition](https://aclanthology.org/2021.acl-long.123/)
- We design a directed acyclic graph (DAG) to model the information propagation in a conversation.

- Secondly, inspired by DAGNN (Thost and Chen, 2021), we propose a directed acyclic graph neural network for ERC, namely DAG-ERC.

- In this paper, we presented a new idea of modeling conversation context with a directed acyclic graph (DAG) and proposed a directed acyclic graph neural network, namely DAG-ERC, for emotion recognition in conversation (ERC).

- In this section, we introduce the proposed Directed Acyclic Graph Neural Network for ERC (DAG-ERC).




# [Beyond Sentence-Level End-to-End Speech Translation: Context Helps](https://aclanthology.org/2021.acl-long.200/)
- Our experiments confirm the effectiveness of context-aware modeling for end-to-end speech translation.

- • Contextual modeling improves translation quality and reduces latency and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a).

- Context-aware ST improves general translation quality in BLEU, and also helps pronoun and homophone translation.

- Compared to sentence-level ST, context-aware ST is less sensitive to those errors.




# [Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model](https://aclanthology.org/2021.acl-long.50/)
- Thus, our contributions are as follows: • To develop a computational method for automatically mapping textual information to the warmthcompetence plane as proposed in the Stereotype Content Model.

- In this exploratory study, we present a computational implementation of the Stereotype Content Model to better understand and counter stereotypes in text.

- • To analyze human-generated anti-stereotypes as a first step towards automatically generating antistereotypes, as a method of countering stereotypes in text with constructive, alternative perspectives.

- We then discuss how we might use information about warmth and competence to generate anti-stereotypes with the specific goal of reducing biased thinking.




# [DEXPERTS: Decoding-Time Controlled Text Generation with Experts and Anti-Experts](https://aclanthology.org/2021.acl-long.522/)
- We propose DEXPERTS, 1 a decoding-time method for controlled text generation based on a product of experts (Hinton, 2002).

- Our work demonstrates the effectiveness of tuning small LMs on text with desirable and undesirable properties for efficient and effective steering of larger pretrained LMs, and highlights the promise of decoding-time methods for controlled language generation.

- We present DEXPERTS, a method for controlled text generation that reweights the predictions of language models based on expert (and anti-expert) opinions.

- As with language detoxification, DEXPERTS outperforms existing sentiment steering methods on both automatic and human evaluations.




# [Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation](https://aclanthology.org/2021.acl-long.473/)
- • We contribute two public large-scale related work generation datasets that are beneficial for the community.

- Then, we propose a relationaware multi-document encoder that relates multiple input documents in a relation graph.

- Related Work Generation.

- Most of the previous related work section generation methods are extractive.




# [Early Detection of Sexual Predators in Chats](https://aclanthology.org/2021.acl-long.386/)
- We introduce the task of early sexual predator detection (eSPD) in chats.

- We defined the problem of early sexual predator detection (eSPD) in online chats and proposed an evaluation setup for this task.

- New state of the art on SPD.

- We hope that making our task setup accessible to the research community will encourage more research into the highly important topic of early sexual predator detection.




# [Evaluating morphological typology in zero-shot cross-lingual transfer](https://aclanthology.org/2021.acl-long.244/)
- We have compared performance of two state-of-the-art zero-shot cross-lingual models on two tasks (part-of-speech tagging and sentiment analysis) for 19 languages across four morphological typologies.

- Our results show that POS tagging is more sensitive to morphological typology than sentiment analysis and that the models perform much better on fusional languages, such as German, than on the other typologies.

- Namely, to what degree do other variables contribute to effects on cross-lingual transfer.

- We fine-tune the models for part-of-speech tagging and sentiment analysis on 19 languages from four morphologically diverse typologies.




# [MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation](https://aclanthology.org/2021.acl-long.86/)
- Specifically, our work makes the following contributions: • We present a text-based adversarial algorithm, MATE-KD, which increases the accuracy of the student model using KD.

- They show improvements on multiple language models on the GLUE benchmark.

- Our model is able to achieve stateof-the-art results for a 6 layer transformer model on the GLUE leaderboard.

- We have presented MATE-KD, a novel text-based adversarial training algorithm which improves the student model in KD by generating adversarial examples while accessing the logits of the teacher only.




# [Multilingual Speech Translation from Efficient Finetuning of Pretrained Models](https://aclanthology.org/2021.acl-long.68/)
- Our contributions are as follows: • We propose a simple and effective approach to combine pretrained single-modality modules to perform speech-to-text translation.

- • We present an efficient transfer learning strategy by only finetuning the LayerNorm and Attention (LNA) parameters of pretrained models.

- • Our approach is also effective for zero-shot multilingual translation (train on A → B and B → C, test on A → C), which provides an efficient approach for many-to-many speechto-text translation without dependency for parallel data for every direction.

- We provide an efficient finetuning strategy which is not only data-and parameter-efficient, but also demonstrates crosslingual transfer ability by only finetuning 10 ∼ 50% of the parameters of large pretrained models.




# [Obtaining Better Static Word Embeddings Using Contextual Embedding Models](https://aclanthology.org/2021.acl-long.408/)
- Our resulting embeddings outperform the current static embedding methods, as well as the current state-of-the-art static embedding distillation method on both unsupervised lexical similarity tasks as well as on downstream supervised tasks, by a significant margin.

- The resulting embeddings can also be used as a task-agnostic tool to measure the lexical information conveyed by contextual embedding models and allow a fair comparison with their static analogues.

- A few methods for distilling static embeddings have already been proposed.

- To ensure a fair comparison, we also evaluate SENT2VEC, CBOW and SKIPGRAM models that were trained on the same corpus.




# [An End-to-End Progressive Multi-Task Learning Framework for Medical Named Entity Recognition and Normalization](https://aclanthology.org/2021.acl-long.485/)
- Based on this idea, we propose an end-to-end progressive multi-task learning framework for medical named entity recognition and normalization (E2EMERN 1 ).

- In this paper, we reconsider the process of NER and NEN and propose the end-to-end progressive multitask learning framework for medical named entity recognition and normalization.

- To capture the semantic features of two tasks, proposed a multi-task learning framework with an explicit feedback strategy for medical NER and NEN.

- Besides the supervised learning, our framework exploits the standard entity information in the NER task and is potential in a zero-shot scenario compared with BioBERT.




# [SPANNER: Named Entity Re-/Recognition as Span Prediction](https://aclanthology.org/2021.acl-long.558/)
- Setup To explore how different mechanisms influence the performance of span prediction models

- We first investigate what strengths and weaknesses are when NER is conceptualized as a span prediction task.

- Figure 2: The framework of span prediction system (SPANNER) as system combiner.

- In other words, the span prediction model play two roles showing in Fig. 1:




# [Supporting Cognitive and Emotional Empathic Writing of Students](https://aclanthology.org/2021.acl-long.314/)
- We introduce a novel empathy annotation scheme and an annotated corpus of student-written peer reviews extracted from a real-world learning scenario.

- We gathered a corpus of 500 student-generated peer reviews written in German.

- Our corpus consisted of 500 student-written peer reviews that were annotated for review components and their emotional and cognitive empathy levels.

- Hence, we propose a new annotation scheme to model peer review components and their emotional and cognitive empathy levels that reflect the feedback discourse in peer review texts.




# [Element Intervention for Open Relation Extraction](https://aclanthology.org/2021.acl-long.361/)
- In this paper, we revisit the procedure of OpenRE from a causal view.

- By formulating OpenRE using a structural causal model, we identify the cause of the above-mentioned problems, and alleviate the problems by Element Intervention.

- We also provide two specific implementations of the interventions based on entity ranking and context contrasting.

- Specifically, we formulate the process of OpenRE using a structural causal model (SCM) (Pearl, 2009), as shown in Figure 1.




# [Neural semi-Markov CRF for Monolingual Word Alignment](https://aclanthology.org/2021.acl-long.531/)
- In this paper, we present a novel neural semi-Markov CRF alignment model, which unifies both word and phrase alignments though variablelength spans, calculates span-based semantic similarities, and takes alignment label transitions into consideration.

- In this work, we present the first neural semi-CRF word alignment model which achieves competitive performance on both in-domain and outof-domain evaluations.

- Our model exceeds 90% F1 in the in-domain evaluation and also has very good generalizability on three out-of-domain datasets.

- Finally, we demonstrate the utility of monolingual word alignment in two downstream applications, namely automatic text simplification and sentence pair classification.




# [Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets](https://aclanthology.org/2021.acl-long.81/)
- Across the four benchmarks, we found several pitfalls affecting individual tests.

- We identified pitfalls affecting individual tests that call into question the validity of the benchmarks' operationalizations of stereotyping.

- Rather, by applying a measurement modeling lens, our goal is to provide a constructive scaffolding for reasoning through and articulating the challenges of constructing and using such benchmarks.

- We inventory a range of pitfalls ( §4)including unstated assumptions, ambiguities, and inconsistencies-surrounding the conceptualization and operationalization of stereotyping implied by both the individual tests (pairs of contrastive sentences) and their construction.




# [PASS: Perturb-and-Select Summarizer for Product Reviews](https://aclanthology.org/2021.acl-long.30/)
- Finally, we show that the resulting PASS system, outperforms SOTA models in the domain of product reviews in terms of informativeness, CP-Diversity and coherence.

- (2) Propose a method that leverages strong pre-trained models that improve the CP-Diversity while significantly reducing the risk of self-inconsistencies.

- In summary, the main contributions of this work are: (1) highlight two shortcomings of existing product reviews summarizers, namely low CP-Diversity and self-inconsistency, and propose a dedicated metric for the former.

- We compare the PASS system to four baselines: COPYCAT (Brazinskas et al., 2020b) is an unsupervised reviews summarizer that is trained to generate a review given other reviews for the same product.




# [OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics](https://aclanthology.org/2021.acl-long.500/)
- Therefore, we propose OpenMEVA, a benchmark for Open-ended story generation Metrics Evaluation.

- We present OpenMEVA, a benchmark to comprehensively assess capabilities of metrics for evaluating open-ended story generation.

- We assess the ability of the unreferenced metrics 8 to judge story coherence based on the discrimination test set of AUTOS.

- We conduct extensive experiments to assess the capabilities of existing automatic metrics on Open-MEVA.




# [Making Pre-trained Language Models Better Few-shot Learners](https://aclanthology.org/2021.acl-long.295/)
- Few-shot learning.

- We refer to our approach as LM-BFF, better few-shot fine-tuning of language models: a strong, taskagnostic method for few-shot learning.

- Fine-tuning of language models.

- We present a systematic evaluation for analyzing few-shot performance on 8 single-sentence and 7 sentence-pair NLP tasks.




# [AUGNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation](https://aclanthology.org/2021.acl-long.95/)
- In this paper, we proposed AUGNLG, a novel data augmentation approach that combines a self-trained retrieval model with a few-shot learned NLU, to automatically create MR-to-Text data from opendomain texts.

- Our approach, in contrast, generates MR-to-Text data by jointly employing a self-trained neural retrieval model with a few-shot learned NLU model.

- In order to go beyond this restriction, this paper proposes AUGNLG, a novel data augmentation approach, that automatically creates MR-to-Text data from open-domain texts by combining a self-trained neural retrieval model with a few-shot learned NLU model.

- FEWSHOTWOZ In sum, the results (1) verify the effectiveness of complementing existing transfer learning methods with our novel data augmentation approach; (2) reveal that automatically augmented MR-to-Text data alone can lead to a competitive performance, previously only achieved with existing MR-to-Text data.




# [CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes](https://aclanthology.org/2021.acl-long.109/)
- Our results show that the common regime of finetuning a large pre-trained model is a useful method for our task of extracting clinical action items.

- Using TTP, we select pre-training datasets of sizes ∼250K, ∼500K, ∼1M, and ∼2M sentences from the set of MIMIC-III discharge notes.

- Similar to prior work on multi-aspect extractive summarization, we employ sentencelevel multi-label classification techniques (Hayashi et al., 2020).

- We evaluated BERT-based models that incorporate multi-sentence context, and introduced a novel task-targeted pre-training approach that can reduce pre-training time while maintaining similar performance to models pre-trained on much larger in-domain datasets.




# [Automatic ICD Coding via Interactive Shared Representation Networks with Self-distillation Mechanism](https://aclanthology.org/2021.acl-long.463/)
- In this paper, we propose an Interactive Shared Representation Network with Self-Distillation Mechanism (ISD) to address the above issues.

- This paper proposes an interactive shared representation network and a self-distillation mechanism for the automatic ICD coding task, to address the long-tail and noisy text issues.

- (1) Effectiveness of Self-distillation.

- 2) To relieve the long-tail issue, we propose an interactive shared representation network, which can capture the internal connections among codes with different frequencies.




# [A Systematic Investigation of KB-Text Embedding Alignment at Scale](https://aclanthology.org/2021.acl-long.139/)
- We compare the performance of different alignment methods using two evaluation tasks -few-shot link prediction and analogical reasoning.

- We define two tasks, few-shot link prediction and analogical reasoning, to evaluate the effectiveness of injecting text information into KB embeddings and injecting KB information into text embeddings, respectively, based on which we evaluate and compare an array of embedding alignment methods.

- 2. We set up a novel evaluation framework with two evaluation tasks, few-shot link prediction and analogical reasoning, to facilitate future research on this important problem.

- Finally, using COVID-19 as a case study, we also demonstrate that such alignment can effectively inject text information into KB embeddings to complete KBs on emerging entities and events.




# [Modeling Bilingual Conversational Characteristics for Neural Chat Translation](https://aclanthology.org/2021.acl-long.444/)
- Our contributions are summarized as follows: • To the best of our knowledge, we are the first to incorporate the role preference, dialogue coherence, and translation consistency into neural chat translation.

- CPCC contains three specific latent variational modules to learn the distributions of role preference, dialogue coherence, and translation consistency, respectively.

- We propose to model bilingual conversational characteristics through tailored latent variables for neural chat translation.

- We design three tailored latent variational modules to learn the distributions of inherent bilingual conversational characteristics, i.e., role preference, dialogue coherence, and translation consistency.




# [A unified approach to sentence segmentation of punctuated text in many languages](https://aclanthology.org/2021.acl-long.309/)
- We propose a simple window-based model and semi-supervised training paradigm for the segmentation of punctuated text ( §3).

- We release these data splits along with our tool, ERSATZ, as open source. 1

- We show here that a simple context-based model can produce state-of-the-art results with a modest hyperparameter search, trained on noisy annotations from imperfectly-segmented data.

- NLTK additionally provides the framework to train a new model.




# [How Did This Get Funded?! Automatically Identifying Quirky Scientific Achievements](https://aclanthology.org/2021.acl-long.2/)
- • We construct a dataset containing thousands of funny scientific papers.

- In this work, we presented a novel task in humor recognition -detecting funny and unusual scientific papers, which represents a subtle and sophisticated humor type.

- Our contributions are: • We formulate a novel humor recognition task in the scientific domain.

- • We develop multiple classifiers, combining findings from psychology and linguistics with recent NLP advances.




# [Database Reasoning Over Text](https://aclanthology.org/2021.acl-long.241/)
- We also introduce a modular architecture to support database reasoning over text and characterize its behavior on our reference dataset.

- Our architecture is capable of overcoming the limitations of transformer models because it runs multiple transformers in parallel, each taking a small set of facts.

- We compare our proposed architecture to transformer-based models that explore the effect of three attention mechanisms representative of the state-of-the-art.

- In our proposed architecture, we only fine-tune transformer models on small support sets.




# [QASR: QCRI Aljazeera Speech Resource A Large Scale Annotated Arabic Speech Corpus](https://aclanthology.org/2021.acl-long.177/)
- In this paper, we introduce a 2, 000 hours transcribed Arabic speech corpus, QASR.

- The QASR is publicly available for the research community.

- We report for the first time named entity recognition in Arabic news transcription.

- We adopt the End-to-End Transformer (E2E-T) architecture from as our baseline for QASR dataset.




# [Improving Dialog Systems for Negotiation with Personality Modeling](https://aclanthology.org/2021.acl-long.56/)
- Our analysis reveals that the agent demonstrates diverse negotiation behavior and adapts well to different types of opponents.

- Our approach provides the ability to model and infer personality types of opponents, predict changes in their mental state, and use this information to adapt the agent's high-level strategy in negotiation tasks.

- In this work, we proposed a novel framework to integrate the concept of Theory of Mind (ToM) into generating task-oriented dialogs.

- We in-troduced a probabilistic formulation for first-order ToM and introduce two ways to incorporate it into a dialog agent, by 1) explicitly and 2) implicitly modeling the personality of the opponent.




# [Ruddit: Norms of Offensiveness for English Reddit Comments](https://aclanthology.org/2021.acl-long.210/)
- Finally, we benchmark several widely-used neural models in their ability to predict offensiveness scores on this new dataset.

- HateBERT was trained on RAL-E, a large dataset of English language Reddit comments from communities banned for being offensive or hateful.

- In this paper, we present the first dataset of 6000 English language Reddit comments that has finegrained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive) -normative offensiveness ratings for the comments.

- For the first time, we use comparative annotations to detect offensive language.




# [Interpretable and Low-Resource Entity Matching via Decoupling Feature Learning from Decision Making](https://aclanthology.org/2021.acl-long.215/)
- HIF and KAT Induction are trained separately.

- We design a self-supervised training method for HIF to learn from unlabeled data.

- Entity Matching.

- We present a decoupled framework for interpretable entity matching.




# [Style is NOT a single variable: Case Studies for Cross-Style Language Understanding](https://aclanthology.org/2021.acl-long.185/)
- We introduce a benchmark XSLUE of mostly existing datasets for studying cross-style language understanding and evaluation.

- Our work has following contributions: • Aggregate 15 different styles and 23 sentencelevel classification tasks ( §3).

- Using XSLUE, we found interesting cross-style observations in classification, correlation, and generation case studies.

- • Study cross-style variations in classification ( §4), correlation ( §5), and generation ( §6): our jointly trained classifier on multiple styles shows better performance than individuallytrained classifiers.




# [Out-of-Scope Intent Detection with Self-Supervision and Discriminative Training](https://aclanthology.org/2021.acl-long.273/)
- Particularly, we construct a set of pseudo out-of-scope examples to aid the training process.

- • We propose a novel out-of-scope intent detection approach by matching training and test tasks to bridge the gap between fitting to training data and generalizing to test data.

- We have proposed a simple, effective, and efficient approach for out-of-scope intent detection by overcoming the limitation of previous methods via matching train-test conditions.

- • We propose to efficiently construct two types of pseudo outliers by using a simple selfsupervised method and leveraging publicly available auxiliary datasets.




# [Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction](https://aclanthology.org/2021.acl-long.360/)
- In this work, we introduce a Frame-aware Event Argument Extraction (FEAE) learning framework for IEAE.

- In summary, our contributions in this work are as follows: 1) We introduce a Frame-aware Event Argument Extraction framework to train models for implicit event argument extraction.

- Specifically, we introduce a curriculum knowledge distillation strategy, FEAE, to train an MRC model that could focus on frame-aware information to identify implicit arguments.

- We achieve new state-of-the-art performance on the RAMS dataset.




# [Article Reranking by Memory-Enhanced Key Sentence Matching for Detecting Previously Fact-Checked Claims](https://aclanthology.org/2021.acl-long.425/)
- Our main contributions are as follows: • We propose a novel reranker MTM for factchecked claim detection, which can better identify key sentences in fact-checking articles by exploiting their characteristics.

- Select key sentences K.

- Experiments on the public Twitter dataset and the private Weibo dataset show that MTM outperforms the state of the art.

- Table 1 shows the statistics of the two datasets.




# [Multi-TimeLine Summarization (MTLS): Improving Timeline Summarization by Generating Multiple Summaries](https://aclanthology.org/2021.acl-long.32/)
- MTLS improves the performance of timeline summarization by generating multiple summaries.

- To address this task, we further propose a Two-Stage Affinity Propagation Summarization framework (2SAPS).

- We propose here the Multiple TimeLine Summarization (MTLS) task that enhances and further generalizes TLS.

- We introduced MTLS task to generalize the timeline summarization problem.




# [VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation](https://aclanthology.org/2021.acl-long.80/)
- In this paper, we introduce VoxPopuli, a largescale multilingual speech corpus for representation learning, semi-supervised learning and interpretation.

- In this paper, we introduce a large-scale multilingual speech corpus, VoxPopuli, for representation learning, semi-supervised learning and interpretation.

- VoxPopuli is also the first corpus for large amounts of open speech-to-speech interpretation data.

- As we can see from Table 1, VoxPopuli has a total of 400K hours of unlabeled data well-distributed across 23 EU languages, resulting in 8K-24K hours of data for each language.




# [Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning](https://aclanthology.org/2021.acl-long.102/)
- The work also evaluates a few multilingual language models (ML-LMs) for cross-lingual commonsense reasoning (XCSR), and introduced a new model which outperforms them.

- Experiments show that the proposed MCP objective indeed significantly improves the performance of state-ofthe-art ML-LMs in cross-lingual commonsense reasoning.

- We propose multilingual contrastive pretraining, a simple and effective sentence-level pretext task for enhancing ML-LMs in cross-lingual commonsense reasoning, which significantly improves the state-of-the-art ML-LMs in crosslingual commonsense reasoning.

- We evaluate and improve popular multilingual language models (ML-LMs) for advancing commonsense reasoning beyond English.




# [Consistency Regularization for Cross-Lingual Fine-Tuning](https://aclanthology.org/2021.acl-long.264/)
- We summarize our contributions as follows: • We propose XTUNE, a cross-lingual finetuning method to better utilize data augmentations based on consistency regularization.

- We propose to improve cross-lingual fine-tuning with two consistency regularization methods, so that we can effectively leverage cross-lingual data augmentations.

- Second, we introduce model consistency to regularize the models trained with various augmentation strategies.

- In contrast, we propose to utilize consistency regularization to better leverage data augmentation for cross-lingual fine-tuning.




# [Few-Shot Question Answering by Pretraining Span Selection](https://aclanthology.org/2021.acl-long.239/)
- We introduce Splinter (span-level pointer), a pretrained model for few-shot question answering.

- We formulate a new task for pretraining question answering from unlabeled text: recurring span selection.

- The recurring span selection objective was designed to emulate extractive question answering using unlabeled text.

- We investigate the task of few-shot question answering by sampling small training sets from existing question answering benchmarks.




# [Text-Free Image-to-Speech Synthesis Using Learned Segmental Units](https://aclanthology.org/2021.acl-long.411/)
- The main contributions of our paper are as follows: 1. The first methodology for fluent image-tospeech synthesis that does not rely on text.

- Specifically, we introduce a model capable of directly generating fluent spoken audio captions of images without the need for natural language text, either as an intermediate representation or a form of supervision during training (Figure 1).

- Instead, we leverage sub-word speech units discovered using a self-supervised learning objective as a drop-in replacement for the text.

- Far less work has focused on generating spoken audio captions from natural images.




# [A Conditional Splitting Framework for Efficient Constituency Parsing](https://aclanthology.org/2021.acl-long.450/)
- Our main contributions are: • We cast the constituency parsing task into a series of conditional splitting decisions and use a seq2seq architecture to model the splitting decision at each decoding step.

- Discourse Parsing For measuring discourse parsing speed

- Our method supports an efficient top-down decoding algorithm that uses a pointing function for scoring possible splitting points.

- We have presented a novel, generic parsing method for constituency parsing based on a Seq2Seq framework.




# [LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification](https://aclanthology.org/2021.acl-long.276/)
- • Our framework is knowledge guided and learnable.

- Moreover, our framework is knowledge guided and learnable.

- We analyze the effect of the learnable dual augmentation for event causality identification.

- In summary, the contributions as follows: • We propose a new learnable data augmentation framework to solve the data lacking problem of ECI.




# [Space Efficient Context Encoding for Non-Task-Oriented Dialogue Generation with Graph Attention Transformer](https://aclanthology.org/2021.acl-long.546/)
- We propose an approach to integrate a concise encoding of knowledge graphs into a Transformer-based decoder architecture for knowledge-grounded dialogue generation.

- The evaluation results prove that our encoding reduces space requirements without negative effects on the precision of reproduction of knowledge and perceived consistency.

- Space Efficient Context Encoding For our proposed encoding, we generate dialogue-specific local knowledge graphs (subgraphs of a background knowledge graph) that capture the information relevant to the dialogue (similar to (Chaudhuri et al., 2021)).

- With this work, we integrate arbitrary knowledge graphs into open-domain knowledgegrounded dialogues, preserving the information encoded in their structure.




# [Unleash GPT-2 Power for Event Detection](https://aclanthology.org/2021.acl-long.490/)
- We introduce a mechanism for knowledge consistency enforcement to mitigate noises from generated data based on optimal transport.

- To avoid noises in the generated data, we propose a novel teacher-student architecture in a multi-task learning framework.

- Hence, we propose a teacher-student network, in which the teacher is first trained on O to learn the anchor knowledge.

- This table reveals that the teacher-student architecture GPTEDOT significantly improves the performance over previous state-of-the-art models for ED in cybersecurity domain.




# [Weakly Supervised Named Entity Tagging with Learnable Logical Rules](https://aclanthology.org/2021.acl-long.352/)
- We defined five types of simple logical rules and introduced compound logical rules that are composed from simple rules to detect entity boundaries and classify their types simultaneously.

- In summary, our main contributions are: • We define five types of logical rules and introduce compound logical rules that can accurately detect entity boundaries and classify their types.

- Therefore, we propose to learn compound logical rules, which are composed from multiple simple rules and logical connectives (e.g. "and").

- We also design a dynamic label selection method to select accurate pseudo labels generated from learned rules for training a discriminative tagging model.




# [Cross-replication Reliability -An Empirical Approach to Interpreting Inter-rater Reliability](https://aclanthology.org/2021.acl-long.548/)
- We extend it to cross-kappa (κ x ) to measure cross-replication reliability.

- We opensource a large-scale replication dataset of facial expression judgements analyzed with the proposed framework.

- We call it cross-replication reliability (xRR).

- We present in Appendix A the International Replication (IRep) dataset, 1 a large-scale crowdsourced dataset of four million judgements of human facial expressions in videos.




# [Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering](https://aclanthology.org/2021.acl-long.478/)
- We propose a consistency training framework for conversational question answering, which enhances QA models' abilities to understand conversational context.

- • Our framework encourages QA models to learn how to resolve the conversational dependency via consistency regularization.

- Our framework leverages both the original and self-contained questions for explicit guidance on how to resolve conversational dependency.

- 3 On the other hand, our framework enhances QA models' reasoning abilities for CQA by jointly utilizing original and self-contained questions.




# [BACO: A Background Knowledge-and Content-Based Framework for Citing Sentence Generation](https://aclanthology.org/2021.acl-long.116/)
- We summarize our contributions as follows: • We propose a BAckground knowledge-and COntent-based framework, named BACO, for citing sentence generation.

- In this paper, we propose a BAckground knowledge-and COntent-based framework, named BACO.

- We integrated them into BACO, a BAckground knowledge-and COntent-based framework for citing sentence generation, which learns and uses information that relate to (1) background knowledge; and (2) content.

- It integrates both background knowledge and content information as context for text generation.




# [Conversations Are Not Flat: Modeling the Dynamic Information Flow across Dialogue Utterances](https://aclanthology.org/2021.acl-long.11/)
- Our contributions are summarized as follows: • We propose the DialoFlow, a new paradigm to construct the dynamic information flow in the dialogue history by addressing the semantic influence brought about by each utterance.

- To capture the dynamic information flow across the dialogue utterances, we design a Flow module to model the context changing scheme.

- For interactive dialogue quality evaluation, our proposed Flow score obtains an impressively high chatbot-level correlation (r = 0.9) with human ratings on 2200 human-bot dialogues from 11 chatbots.

- Besides, we design an automatic referencefree evaluation metric Flow score based on the pre-trained DialoFlow for interactive dialogue quality evaluation.




# [Can Generative Pre-trained Language Models Serve as Knowledge Bases for Closed-book QA?](https://aclanthology.org/2021.acl-long.251/)
- 2 Using SQuAD for Closed-book QA In the closed-book QA task , a model needs to answer questions without external resources.

- firstly use closed-book QA to detect how much knowledge is in pre-trained language models' parameters.

- Existing research leaves many open questions on the potential of generative pre-trained LMs on closed-book QA.

- While the other three datasets are used by following previous work , we make a novel adaptation of the SQuAD dataset for closed-book QA.




# [A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations](https://aclanthology.org/2021.acl-long.511/)
- Our main contributions are summarized below: • A novel objective to train disentangled representations from attributes.

- Applications to fair classification and sentence generation.

- We develop new tools to build disentangled textual representations and evaluate them on fair classifi-cation and two sentence generation tasks, namely, style transfer and conditional sentence generation.

- However, the Renyi's surrogate achieves slightly better-disentangled representations.




# [Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction](https://aclanthology.org/2021.acl-long.385/)
- In summary, our contributions are as follows: • A new framework named tail-to-tail nonautoregressive sequence prediction (TtT) is proposed to tackle the problem of CGEC.

- We propose a new framework named tail-to-tail non-autoregressive sequence prediction, which abbreviated as TtT, for the problem of CGEC.

- A BERT based sequence encoder is introduced to conduct bidirectional representation learning.

- • Focal loss penalty strategy is adopted to alleviate the class imbalance problem considering that most of the tokens in a sentence are not changed.




# [A Targeted Assessment of Incremental Processing in Neural Language Models and Humans](https://aclanthology.org/2021.acl-long.76/)
- We combine these two approaches with a targeted assessment of incremental processing in neural language models and humans.

- For human incremental processing data, we use by-word reaction times (RTs).

- We find that models systematically under-predict the observed human data.

- Do differences in surprisal accurately predict the slowdowns observed in human reaction time data?




# [Generating Query Focused Summaries from Query-Free Resources](https://aclanthology.org/2021.acl-long.475/)
- In this work we proposed an abstractive framework for query focused summarization.

- Participants assessed summaries created by PQSUM-WSL, the state-of-the-art abstractive system, QUERYSUM, a state-of-the-art extractive system, UNILM-CD, and MARGESUM-CD.

- In this work, we propose to decompose QFS into two sub-tasks, namely query modeling and conditional language modeling.

- Experimental results across datasets show that the proposed system yields state-of-the-art performance despite the weakly supervised setting, and produces more relevant and coherent summaries compared to existing approaches.




# [The statistical advantage of automatic NLG metrics at the system level](https://aclanthology.org/2021.acl-long.533/)
- With the decomposition, we can adjust metric errors estimates to a noise-free and infinite test set setting by taking only their bias component.

- Note that this term is also the true error of a metric estimator in a noise-free, infinite test set setting.

- Through rigorous comparison between metrics, humans, and the perfect segment-level annotator, we identify the settings where metrics outperform humans due to a statistical advantage in variance.

- With the bias-variance-noise decomposition, we can adjust our observed error estimates to the noise-free, infinite test set setting of the true error.




# [AdvPicker: Effectively Leveraging Unlabeled Data via Adversarial Discriminator for Cross-Lingual NER](https://aclanthology.org/2021.acl-long.61/)
- Specifically, we first train an encoder and a NER classifier on labeled source-language data to learn entity domain knowledge.

- Translation-based methods generally use pseudo target-language data translated from labeled sourcelanguage data.

- Language-Independent Data To leverage such less language-dependent data

- Our experimental results show that the proposed method benefits strongly from this data selection process and outperforms existing SOTA methods; without requiring any additional external resources (e.g., gazetteers or machine translation).




# [Syntax-augmented Multilingual BERT for Cross-lingual Transfer](https://aclanthology.org/2021.acl-long.350/)
- 1. Does augmenting mBERT with syntax improve (generalized) cross-lingual transfer?

- Our main idea is to augment mBERT with language syntax for zero-shot cross-lingual transfer.

- The results endorse the effectiveness of our proposed approach in the cross-lingual transfer.

- We highlight the cross-lingual transfer gap for mBERT and syntax-augmented mBERT on the evaluation tasks in Table 7.




# [Dual Graph Convolutional Networks for Aspect-based Sentiment Analysis](https://aclanthology.org/2021.acl-long.494/)
- • We propose orthogonal and differential regularizers.

- Our contributions are highlighted as follows: • We propose a DualGCN model for the ABSA task.

- To improve the semantic representation, we propose two regularizers for the SemGCN module, i.e., orthogonal and differential regularizers.

- Therefore, the dependency probability matrix is used to alleviate dependency parsing errors.




# [Answering Ambiguous Questions through Generative Evidence Fusion and Round-Trip Prediction](https://aclanthology.org/2021.acl-long.253/)
- The proposed round-trip prediction is a general approach for answering ambiguous open-domain questions, which improves our REFUEL as well as several baseline models.

- REFUEL achieves a new state-of-the-art on AM-BIGQA, and shows competitive performance on NQ-OPEN and TriviaQA.

- To address these issues, we propose REFUEL, Round-trip Evidence FUsion via gEneration with retrievaL, a new framework for answering ambiguous open-domain questions.

- ( 3) We propose a model-agnostic round-trip prediction approach to find more interpretations missed in the first prediction pass, which we further refine using a conditional-probability-based filtering approach.




# [Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning](https://aclanthology.org/2021.acl-long.528/)
- Therefore, we build a new large-scale geometry problem benchmark, called Geometry3K.

- Datasets for Geometry Problem Solving.

- Approaches for Geometry Problem Solving.

- To overcome these challenges, we first construct a new large-scale benchmark, called Geometry3K, to assess algorithms' performance of geometry problem solving.




